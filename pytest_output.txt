============================= test session starts ==============================
platform darwin -- Python 3.10.17, pytest-8.3.5, pluggy-1.5.0 -- /Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/setup/venv/bin/python3.10
cachedir: .pytest_cache
rootdir: /Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents
configfile: pytest.ini
plugins: anyio-4.9.0, langsmith-0.3.42, asyncio-0.26.0, mock-3.14.0, cov-6.1.1
asyncio: mode=auto, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collecting ... collected 139 items / 1 skipped

ADK/agent_data/test_pass_53/test_load_metadata_from_faiss.py::TestLoadMetadataFromFaiss::test_load_completed PASSED [  0%]
ADK/agent_data/test_pass_53/test_load_metadata_from_faiss.py::TestLoadMetadataFromFaiss::test_load_pending PASSED [  1%]
ADK/agent_data/test_pass_53/test_load_metadata_from_faiss.py::TestLoadMetadataFromFaiss::test_load_not_exists PASSED [  2%]
ADK/agent_data/test_pass_53/test_load_metadata_from_faiss.py::TestLoadMetadataFromFaiss::test_load_gcs_meta_download_fails PASSED [  2%]
ADK/agent_data/test_pass_53/test_load_metadata_from_faiss.py::TestLoadMetadataFromFaiss::test_load_gcs_meta_download_timeout PASSED [  3%]
ADK/agent_data/test_pass_53/test_load_metadata_from_faiss.py::TestLoadMetadataFromFaiss::test_load_gcs_meta_download_permission_denied PASSED [  4%]
ADK/agent_data/test_pass_53/test_mcp_agent_all_tools.py::TestMCPStdioAllTools::test_core_tools_and_errors FAILED [  5%]
ADK/agent_data/test_pass_53/test_mcp_agent_batch_large.py::TestMCPAgentLargeBatchDirect::test_large_batch_processing_direct FAILED [  5%]
ADK/agent_data/test_pass_53/test_mcp_agent_batch_tools.py::TestMCPBatchToolsStdio::test_batch_mixed_success_failure FAILED [  6%]
ADK/agent_data/test_pass_53/test_mcp_agent_batch_tools.py::TestMCPBatchToolsStdio::test_batch_successful FAILED [  7%]
ADK/agent_data/test_pass_53/test_mcp_agent_batch_tools.py::TestMCPBatchToolsStdio::test_query_metadata FAILED [  7%]
ADK/agent_data/test_pass_53/test_mcp_agent_batch_tools.py::TestMCPBatchToolsStdio::test_save_document FAILED [  8%]
ADK/agent_data/test_pass_53/test_mcp_agent_batch_tools.py::TestMCPBatchToolsStdio::test_semantic_search_local FAILED [  9%]
ADK/agent_data/test_pass_53/test_mcp_agent_batch_tools.py::TestMCPBatchToolsStdio::test_update_metadata FAILED [ 10%]
ADK/agent_data/test_pass_53/test_mcp_agent_batch_tools.py::TestMCPBatchToolsStdio::test_update_metadata_fail FAILED [ 10%]
ADK/agent_data/test_pass_53/test_mcp_agent_batch_tools.py::TestMCPBatchToolsStdio::test_vectorize_document FAILED [ 11%]
ADK/agent_data/test_pass_53/test_mcp_agent_loop.py::TestMCPStdioServer::test_add_numbers_tool_via_mcp_stdio PASSED [ 12%]
ADK/agent_data/test_pass_53/test_mcp_agent_loop.py::TestMCPStdioServer::test_echo_tool_via_mcp_stdio PASSED [ 12%]
ADK/agent_data/test_pass_53/test_mcp_agent_loop.py::TestMCPStdioServer::test_invalid_json_input PASSED [ 13%]
ADK/agent_data/test_pass_53/test_mcp_agent_loop.py::TestMCPStdioServer::test_unknown_tool PASSED [ 14%]
ADK/agent_data/test_pass_53/test_mcp_registered_tools.py::TestMCPStdioGetRegisteredTools::test_get_registered_tools_mcp_stdio FAILED [ 15%]
ADK/agent_data/test_pass_53/test_mcp_timeout.py::test_mcp_timeout_handling FAILED [ 15%]
ADK/agent_data/test_pass_53/test_mcp_timeout.py::test_log_output_verification FAILED [ 16%]
ADK/agent_data/test_pass_53/test_query_metadata_from_faiss.py::TestQueryMetadataFaiss::test_query_success PASSED [ 17%]
ADK/agent_data/test_pass_53/test_query_metadata_from_faiss.py::TestQueryMetadataFaiss::test_query_firestore_error PASSED [ 17%]
ADK/agent_data/test_pass_53/test_query_metadata_from_faiss.py::TestQueryMetadataFaiss::test_query_gcs_download_error PASSED [ 18%]
ADK/agent_data/test_pass_53/test_query_metadata_from_faiss.py::TestQueryMetadataFaiss::test_query_faiss_search_error PASSED [ 19%]
ADK/agent_data/test_pass_53/test_query_metadata_from_faiss.py::TestQueryMetadataFaiss::test_query_with_local_files_exist PASSED [ 20%]
ADK/agent_data/test_pass_53/test_query_metadata_from_faiss.py::TestQueryMetadataFaiss::test_query_firestore_missing_gcs_faiss_path FAILED [ 20%]
ADK/agent_data/test_pass_53/test_query_metadata_from_faiss.py::TestQueryMetadataFaiss::test_query_pending_status PASSED [ 21%]
ADK/agent_data/test_pass_53/test_query_metadata_from_faiss.py::TestQueryMetadataFaiss::test_query_index_not_found PASSED [ 22%]
ADK/agent_data/test_pass_53/test_query_metadata_from_faiss.py::TestQueryMetadataFaiss::test_query_embedding_error ERROR [ 23%]
ADK/agent_data/test_pass_55/test_load_metadata_from_faiss.py::TestLoadMetadataFromFaiss::test_load_completed PASSED [ 23%]
ADK/agent_data/test_pass_55/test_load_metadata_from_faiss.py::TestLoadMetadataFromFaiss::test_load_pending PASSED [ 24%]
ADK/agent_data/test_pass_55/test_load_metadata_from_faiss.py::TestLoadMetadataFromFaiss::test_load_not_exists PASSED [ 25%]
ADK/agent_data/test_pass_55/test_load_metadata_from_faiss.py::TestLoadMetadataFromFaiss::test_load_gcs_meta_download_fails PASSED [ 25%]
ADK/agent_data/test_pass_55/test_load_metadata_from_faiss.py::TestLoadMetadataFromFaiss::test_load_gcs_meta_download_timeout PASSED [ 26%]
ADK/agent_data/test_pass_55/test_load_metadata_from_faiss.py::TestLoadMetadataFromFaiss::test_load_gcs_meta_download_permission_denied PASSED [ 27%]
ADK/agent_data/test_pass_55/test_query_metadata_from_faiss.py::TestQueryMetadataFaiss::test_query_success PASSED [ 28%]
ADK/agent_data/test_pass_55/test_query_metadata_from_faiss.py::TestQueryMetadataFaiss::test_query_firestore_error PASSED [ 28%]
ADK/agent_data/test_pass_55/test_query_metadata_from_faiss.py::TestQueryMetadataFaiss::test_query_gcs_download_error PASSED [ 29%]
ADK/agent_data/test_pass_55/test_query_metadata_from_faiss.py::TestQueryMetadataFaiss::test_query_faiss_search_error PASSED [ 30%]
ADK/agent_data/test_pass_55/test_query_metadata_from_faiss.py::TestQueryMetadataFaiss::test_query_with_local_files_exist PASSED [ 30%]
ADK/agent_data/test_pass_55/test_query_metadata_from_faiss.py::TestQueryMetadataFaiss::test_query_firestore_missing_gcs_faiss_path PASSED [ 31%]
ADK/agent_data/test_pass_55/test_query_metadata_from_faiss.py::TestQueryMetadataFaiss::test_query_pending_status PASSED [ 32%]
ADK/agent_data/test_pass_55/test_query_metadata_from_faiss.py::TestQueryMetadataFaiss::test_query_index_not_found PASSED [ 33%]
ADK/agent_data/test_pass_55/test_query_metadata_from_faiss.py::TestQueryMetadataFaiss::test_query_embedding_error PASSED [ 33%]
ADK/agent_data/tests/test_add_numbers_tool.py::test_add_numbers_positive PASSED [ 34%]
ADK/agent_data/tests/test_add_numbers_tool.py::test_add_numbers_negative PASSED [ 35%]
ADK/agent_data/tests/test_add_numbers_tool.py::test_add_numbers_mixed PASSED [ 35%]
ADK/agent_data/tests/test_add_numbers_tool.py::test_add_numbers_zero PASSED [ 36%]
ADK/agent_data/tests/test_add_numbers_tool.py::test_add_numbers_float PASSED [ 37%]
ADK/agent_data/tests/test_add_numbers_tool.py::test_add_numbers_invalid_input PASSED [ 38%]
ADK/agent_data/tests/test_mcp_agent_all_tools.py::TestMCPStdioAllTools::test_core_tools_and_errors FAILED [ 38%]
ADK/agent_data/tests/test_mcp_agent_batch_large.py::TestMCPAgentLargeBatchDirect::test_large_batch_processing_direct FAILED [ 39%]
ADK/agent_data/tests/test_mcp_agent_batch_tools.py::TestMCPBatchToolsStdio::test_batch_mixed_success_failure FAILED [ 40%]
ADK/agent_data/tests/test_mcp_agent_batch_tools.py::TestMCPBatchToolsStdio::test_batch_successful FAILED [ 41%]
ADK/agent_data/tests/test_mcp_agent_batch_tools.py::TestMCPBatchToolsStdio::test_query_metadata FAILED [ 41%]
ADK/agent_data/tests/test_mcp_agent_batch_tools.py::TestMCPBatchToolsStdio::test_save_document FAILED [ 42%]
ADK/agent_data/tests/test_mcp_agent_batch_tools.py::TestMCPBatchToolsStdio::test_semantic_search_local FAILED [ 43%]
ADK/agent_data/tests/test_mcp_agent_batch_tools.py::TestMCPBatchToolsStdio::test_update_metadata FAILED [ 43%]
ADK/agent_data/tests/test_mcp_agent_batch_tools.py::TestMCPBatchToolsStdio::test_update_metadata_fail FAILED [ 44%]
ADK/agent_data/tests/test_mcp_agent_batch_tools.py::TestMCPBatchToolsStdio::test_vectorize_document FAILED [ 45%]
ADK/agent_data/tests/test_mcp_agent_loop.py::TestMCPStdioServer::test_add_numbers_tool_via_mcp_stdio PASSED [ 46%]
ADK/agent_data/tests/test_mcp_agent_loop.py::TestMCPStdioServer::test_echo_tool_via_mcp_stdio PASSED [ 46%]
ADK/agent_data/tests/test_mcp_agent_loop.py::TestMCPStdioServer::test_invalid_json_input PASSED [ 47%]
ADK/agent_data/tests/test_mcp_agent_loop.py::TestMCPStdioServer::test_unknown_tool PASSED [ 48%]
ADK/agent_data/tests/test_mcp_registered_tools.py::TestMCPStdioGetRegisteredTools::test_get_registered_tools_mcp_stdio FAILED [ 48%]
ADK/agent_data/tests/test_mcp_timeout.py::test_mcp_timeout_handling FAILED [ 49%]
ADK/agent_data/tests/test_mcp_timeout.py::test_log_output_verification FAILED [ 50%]
ADK/agent_data/tests/test_multiply_numbers_tool.py::test_multiply_numbers_positive PASSED [ 51%]
ADK/agent_data/tests/test_multiply_numbers_tool.py::test_multiply_numbers_negative PASSED [ 51%]
ADK/agent_data/tests/test_multiply_numbers_tool.py::test_multiply_numbers_mixed PASSED [ 52%]
ADK/agent_data/tests/test_multiply_numbers_tool.py::test_multiply_numbers_zero PASSED [ 53%]
ADK/agent_data/tests/test_multiply_numbers_tool.py::test_multiply_numbers_float PASSED [ 53%]
ADK/agent_data/tests/test_multiply_numbers_tool.py::test_multiply_numbers_invalid_keys PASSED [ 54%]
ADK/agent_data/tests/test_multiply_numbers_tool.py::test_multiply_numbers_invalid_type PASSED [ 55%]
ADK/agent_data/tests/test_multiply_numbers_tool.py::test_multiply_numbers_type_error PASSED [ 56%]
ADK/agent_data/tests/test_query_metadata_tool.py::test_query_metadata_basic PASSED [ 56%]
ADK/agent_data/tests/test_query_metadata_tool.py::test_query_metadata_empty_query PASSED [ 57%]
ADK/agent_data/tests/test_query_metadata_tool.py::test_query_metadata_complex_query PASSED [ 58%]
ADK/agent_data/tests/test_save_document_tool.py::test_save_document_success PASSED [ 58%]
ADK/agent_data/tests/test_save_document_tool.py::test_save_document_overwrite PASSED [ 59%]
ADK/agent_data/tests/test_save_text_tool.py::test_save_text_basic PASSED [ 60%]
ADK/agent_data/tests/test_save_text_tool.py::test_save_text_empty PASSED [ 61%]
ADK/agent_data/tests/test_save_text_tool.py::test_save_text_numbers PASSED [ 61%]
ADK/agent_data/tests/test_save_text_tool.py::test_save_text_invalid_filename PASSED [ 62%]
ADK/agent_data/tests/test_semantic_search_local_tool.py::test_semantic_search_local_basic PASSED [ 63%]
ADK/agent_data/tests/test_semantic_search_local_tool.py::test_semantic_search_local_empty_query PASSED [ 64%]
ADK/agent_data/tests/test_semantic_search_local_tool.py::test_semantic_search_local_short_query PASSED [ 64%]
ADK/agent_data/tests/test_update_metadata_tool.py::test_update_metadata_basic PASSED [ 65%]
ADK/agent_data/tests/test_update_metadata_tool.py::test_update_metadata_empty PASSED [ 66%]
ADK/agent_data/tests/test_update_metadata_tool.py::test_update_metadata_complex PASSED [ 66%]
ADK/agent_data/tests/test_vectorize_document_tool.py::test_vectorize_document_basic PASSED [ 67%]
ADK/agent_data/tests/test_vectorize_document_tool.py::test_vectorize_document_empty_content PASSED [ 68%]
ADK/agent_data/tests/tools/test_load_metadata_from_faiss.py::TestLoadMetadataFromFaiss::test_load_completed PASSED [ 69%]
ADK/agent_data/tests/tools/test_load_metadata_from_faiss.py::TestLoadMetadataFromFaiss::test_load_pending PASSED [ 69%]
ADK/agent_data/tests/tools/test_load_metadata_from_faiss.py::TestLoadMetadataFromFaiss::test_load_not_exists PASSED [ 70%]
ADK/agent_data/tests/tools/test_load_metadata_from_faiss.py::TestLoadMetadataFromFaiss::test_load_gcs_meta_download_fails PASSED [ 71%]
ADK/agent_data/tests/tools/test_load_metadata_from_faiss.py::TestLoadMetadataFromFaiss::test_load_gcs_meta_download_timeout PASSED [ 71%]
ADK/agent_data/tests/tools/test_load_metadata_from_faiss.py::TestLoadMetadataFromFaiss::test_load_gcs_meta_download_permission_denied PASSED [ 72%]
ADK/agent_data/tests/tools/test_query_metadata_from_faiss.py::TestQueryMetadataFaiss::test_query_success PASSED [ 73%]
ADK/agent_data/tests/tools/test_query_metadata_from_faiss.py::TestQueryMetadataFaiss::test_query_firestore_error PASSED [ 74%]
ADK/agent_data/tests/tools/test_query_metadata_from_faiss.py::TestQueryMetadataFaiss::test_query_gcs_download_error PASSED [ 74%]
ADK/agent_data/tests/tools/test_query_metadata_from_faiss.py::TestQueryMetadataFaiss::test_query_faiss_search_error PASSED [ 75%]
ADK/agent_data/tests/tools/test_query_metadata_from_faiss.py::TestQueryMetadataFaiss::test_query_with_local_files_exist PASSED [ 76%]
ADK/agent_data/tests/tools/test_query_metadata_from_faiss.py::TestQueryMetadataFaiss::test_query_firestore_missing_gcs_faiss_path PASSED [ 76%]
ADK/agent_data/tests/tools/test_query_metadata_from_faiss.py::TestQueryMetadataFaiss::test_query_pending_status PASSED [ 77%]
ADK/agent_data/tests/tools/test_query_metadata_from_faiss.py::TestQueryMetadataFaiss::test_query_index_not_found PASSED [ 78%]
ADK/agent_data/tests/tools/test_query_metadata_from_faiss.py::TestQueryMetadataFaiss::test_query_embedding_error PASSED [ 79%]
ADK/agent_data/tests/tools/test_save_metadata_to_faiss.py::TestSaveMetadataToFaiss::test_save_pickle_dump_fails PASSED [ 79%]
ADK/agent_data/tests/tools/test_save_metadata_to_faiss.py::TestSaveMetadataToFaiss::test_save_auto_embed PASSED [ 80%]
ADK/agent_data/tests/tools/test_save_metadata_to_faiss.py::TestSaveMetadataToFaiss::test_save_with_vector_data PASSED [ 81%]
ADK/agent_data/tests/tools/test_save_metadata_to_faiss.py::TestSaveMetadataToFaiss::test_save_openai_embedding_error FAILED [ 82%]
ADK/agent_data/tests/tools/test_save_metadata_to_faiss.py::TestSaveMetadataToFaiss::test_save_gcs_upload_faiss_fails FAILED [ 82%]
ADK/agent_data/tests/tools/test_save_metadata_to_faiss.py::TestSaveMetadataToFaiss::test_save_gcs_upload_meta_fails FAILED [ 83%]
ADK/agent_data/tests/tools/test_save_metadata_to_faiss.py::TestSaveMetadataToFaiss::test_save_firestore_update_fails FAILED [ 84%]
ADK/agent_data/tests/tools/test_save_metadata_to_faiss.py::TestSaveMetadataToFaiss::test_save_malformed_embedding_result ERROR [ 84%]
ADK/agent_data/tests/tools/test_save_metadata_to_faiss.py::TestSaveMetadataToFaiss::test_save_firestore_client_init_fails ERROR [ 85%]
ADK/agent_data/tests/tools/test_save_metadata_to_faiss.py::TestSaveMetadataToFaiss::test_save_temp_file_deletion_fails FAILED [ 86%]
test_pass_56/test_save_metadata_to_faiss_prompt20_backup.py::TestSaveMetadataToFaiss::test_save_pickle_dump_fails PASSED [ 87%]
test_pass_56/test_save_metadata_to_faiss_prompt20_backup.py::TestSaveMetadataToFaiss::test_save_auto_embed PASSED [ 87%]
test_pass_56/test_save_metadata_to_faiss_prompt20_backup.py::TestSaveMetadataToFaiss::test_save_with_vector_data PASSED [ 88%]
test_pass_56/test_save_metadata_to_faiss_prompt20_backup.py::TestSaveMetadataToFaiss::test_save_openai_embedding_error FAILED [ 89%]
test_pass_56/test_save_metadata_to_faiss_prompt20_backup.py::TestSaveMetadataToFaiss::test_save_gcs_upload_faiss_fails PASSED [ 89%]
test_pass_56/test_save_metadata_to_faiss_prompt20_backup.py::TestSaveMetadataToFaiss::test_save_gcs_upload_meta_fails PASSED [ 90%]
test_pass_56/test_save_metadata_to_faiss_prompt20_backup.py::TestSaveMetadataToFaiss::test_save_firestore_update_fails ERROR [ 91%]
test_pass_56/test_save_metadata_to_faiss_prompt20_backup.py::TestSaveMetadataToFaiss::test_save_empty_metadata_dict ERROR [ 92%]
test_pass_56/test_save_metadata_to_faiss_prompt20_backup.py::TestSaveMetadataToFaiss::test_save_text_field_missing_in_metadata ERROR [ 92%]
test_pass_56/test_save_metadata_to_faiss_prompt20_backup.py::TestSaveMetadataToFaiss::test_save_dimension_mismatch_with_vector_data PASSED [ 93%]
test_pass_56/test_save_metadata_to_faiss_prompt20_backup.py::TestSaveMetadataToFaiss::test_save_malformed_embedding_result FAILED [ 94%]
test_pass_56/test_save_metadata_to_faiss_prompt20_backup.py::TestSaveMetadataToFaiss::test_save_firestore_client_init_fails ERROR [ 94%]
test_pass_56/test_save_metadata_to_faiss_prompt20_backup.py::TestSaveMetadataToFaiss::test_save_vector_data_zero_dimension_vector FAILED [ 95%]
test_pass_56/test_save_metadata_to_faiss_prompt20_backup.py::TestSaveMetadataToFaiss::test_save_temp_file_deletion_fails ERROR [ 96%]
tests/test_validate_batch_tools.py::TestValidateBatchTools::test_batch_with_invalid_tools PASSED [ 97%]
tests/test_validate_batch_tools.py::TestValidateBatchTools::test_batch_with_mixed_tools PASSED [ 97%]
tests/test_validate_batch_tools.py::TestValidateBatchTools::test_batch_with_prohibited_tools PASSED [ 98%]
tests/test_validate_batch_tools.py::TestValidateBatchTools::test_empty_batch PASSED [ 99%]
tests/test_validate_batch_tools.py::TestValidateBatchTools::test_valid_batch PASSED [100%]

==================================== ERRORS ====================================
_____ ERROR at setup of TestQueryMetadataFaiss.test_query_embedding_error ______
file /Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/test_pass_53/test_query_metadata_from_faiss.py, line 474
      @patch(STORAGE_CLIENT_QUERY_PATH)
      async def test_query_embedding_error(self, MockStorageClient, MockFirestoreClient, mock_get_embedding):
          from ADK.agent_data.tools.query_metadata_faiss_tool import query_metadata_faiss
          mock_fs_instance = MockFirestoreClient.return_value
          mock_collection_ref = mock_fs_instance.collection.return_value
          mock_doc_ref = mock_collection_ref.document.return_value

          mock_doc_snapshot = MagicMock()
          mock_doc_snapshot.exists = True
          index_name = "embed_error_index"
          gcs_faiss_path = f"gs://{MOCKED_ENV_VARS['GCS_BUCKET_NAME']}/{index_name}.faiss"
          gcs_meta_path = f"gs://{MOCKED_ENV_VARS['GCS_BUCKET_NAME']}/{index_name}.meta"
          mock_doc_snapshot.to_dict.return_value = {
              "vectorStatus": "completed",
              "gcs_faiss_path": gcs_faiss_path,
              "gcs_meta_path": gcs_meta_path,
              "dimension": 5
          }
          mock_doc_ref.get.return_value = mock_doc_snapshot

          mock_get_embedding.return_value = {"status": "error", "error": "Simulated embedding API error", "total_tokens": 0}

          result = await query_metadata_faiss(agent_context=None, index_name=index_name, key="test query", top_k=1)

          assert result.get("meta", {}).get("status") == "error"
          assert "Simulated embedding API error" in result["error"]
          MockFirestoreClient.assert_called_once_with(project="chatgpt-db-project", database="test-default")
          mock_fs_instance.collection.assert_called_once_with(MOCKED_ENV_VARS["FAISS_INDEXES_COLLECTION"])
          mock_collection_ref.document.assert_called_once_with(index_name)
          mock_doc_ref.get.assert_called_once()
          # Assert embedding call with positional arguments
          mock_get_embedding.assert_called_once_with(None, "test query")
          MockStorageClient.assert_not_called()
E       fixture 'MockFirestoreClient' not found
>       available fixtures: ADK/agent_data/test_pass_53/test_query_metadata_from_faiss.py::<event_loop>, ADK/agent_data/test_pass_53/test_query_metadata_from_faiss.py::TestQueryMetadataFaiss::<event_loop>, _session_event_loop, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_env_vars, mock_firestore_client, mock_logging, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/test_pass_53/test_query_metadata_from_faiss.py:474
_ ERROR at setup of TestSaveMetadataToFaiss.test_save_malformed_embedding_result _
file /Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/tests/tools/test_save_metadata_to_faiss.py, line 639
      @patch(f"{SAVE_TOOL_MODULE_PATH}.OPENAI_AVAILABLE", True)
      @patch(f"{SAVE_TOOL_MODULE_PATH}.openai_client", MagicMock()) # Ensure OpenAI client is mocked as available
      @patch(f"{SAVE_TOOL_MODULE_PATH}.faiss.IndexFlatL2")
      @patch(f"{SAVE_TOOL_MODULE_PATH}.faiss.write_index")
      @patch(f"{SAVE_TOOL_MODULE_PATH}.pickle.dump")
      @patch(FIRESTORE_CLIENT_PATH)
      @patch(UPLOAD_WITH_RETRY_PATH)
      @patch(STORAGE_CLIENT_SAVE_PATH)
      async def test_save_malformed_embedding_result(self,
                                                   mock_get_embedding,
                                                   MockFaissIndexFlatL2,
                                                   mock_faiss_write_index,
                                                   mock_pickle_dump,
                                                   mock_firestore_constructor,
                                                   mock_upload_with_retry,
                                                   mock_storage_client_constructor,
                                                   mocker, request):
          """Test handling of malformed (but not exception-raising) embedding results."""
          from ADK.agent_data.tools.save_metadata_to_faiss_tool import save_metadata_to_faiss

          mock_fs_instance = mock_firestore_constructor.return_value
          mock_collection_ref = mock_fs_instance.collection.return_value # Added for clarity
          mock_doc_ref = mock_collection_ref.document.return_value

          mock_storage_client_instance = mock_storage_client_constructor.return_value
          mock_bucket_instance = mock_storage_client_instance.bucket.return_value
          # mock_blob_instance will be returned by mock_bucket_instance.blob calls
          # For example, by configuring mock_bucket_instance.blob.return_value = MagicMock(spec=storage.Blob)
          # However, the original test structure implies that the specific blob instance isn't critical for these mocks.

          mock_upload_with_retry.return_value = MagicMock()

          mock_index_instance = MockFaissIndexFlatL2.return_value

          mock_get_embedding.side_effect = [
              {"embedding": np.array([0.1]*10, dtype=np.float32), "total_tokens": 5, "status": "success"}, # doc1
              {"status": "success", "total_tokens": 0}, # doc2 - malformed (missing 'embedding')
              "This is not a dictionary" # doc3 - malformed (not a dict)
          ]

          input_metadata = {
              "doc1": {"text": "Valid text for doc1"},
              "doc2": {"text": "Text for doc2 (malformed: missing embedding key)"},
              "doc3": {"text": "Text for doc3 (malformed: not a dict)"}
          }
          input_data = {
              "index_name": "test_malformed_embed_index",
              "metadata_dict": input_metadata,
              "text_field_to_embed": "text",
              "dimension": 10
          }

          result = await save_metadata_to_faiss(**input_data)

          print(f"\nResult dictionary for {request.node.name}: {result}\n")
          assert result is not None
          assert result.get("status") == "success"
          assert result.get("index_name") == "test_malformed_embed_index"
          assert result.get("vector_count") == 1
          assert result["dimension"] == 10
          assert result.get("gcs_upload_status") == "success"
          assert result.get("firestore_update_status") == "success"

          assert "meta" in result
          assert result["meta"].get("embedded_docs_count") == 1
          failed_doc_ids = result["meta"].get("failed_doc_ids", [])
          assert "doc2" in failed_doc_ids
          assert "doc3" in failed_doc_ids
          assert len(failed_doc_ids) == 2

          embedding_errors = result["meta"].get("embedding_generation_errors", {})
          assert "doc2" in embedding_errors
          assert "Malformed embedding result or non-success status for doc_id doc2" in embedding_errors["doc2"]
          assert "doc3" in embedding_errors
          assert "Malformed embedding result or non-success status for doc_id doc3" in embedding_errors["doc3"]

          mock_get_embedding.assert_has_calls([
              call(agent_context=None, text_to_embed="Valid text for doc1"),
              call(agent_context=None, text_to_embed="Text for doc2 (malformed: missing embedding key)"),
              call(agent_context=None, text_to_embed="Text for doc3 (malformed: not a dict)")
          ], any_order=True)
          assert mock_get_embedding.call_count == 3

          MockFaissIndexFlatL2.assert_called_once_with(10)
          mock_index_instance.add.assert_called_once()
          added_vectors = mock_index_instance.add.call_args[0][0]
          assert np.array_equal(added_vectors, np.array([[0.1]*10], dtype=np.float32))

          mock_faiss_write_index.assert_called_once()
          expected_pickle_data = {
              'ids': ['doc1'],
              'metadata': {'doc1': input_metadata['doc1']}
          }
          mock_pickle_dump.assert_called_once_with(expected_pickle_data, mocker.ANY)
          assert mock_upload_with_retry.call_count == 2 # .faiss and .meta

          mock_storage_client_constructor.assert_called_once_with(project="chatgpt-db-project")
          mock_storage_client_instance.bucket.assert_called_once_with(MOCKED_ENV_VARS["GCS_BUCKET_NAME"])

          mock_bucket_instance.blob.assert_has_calls([
              call(f"{input_data['index_name']}.faiss"),
              call(f"{input_data['index_name']}.meta")
          ], any_order=True)
          assert mock_bucket_instance.blob.call_count == 2

          mock_firestore_constructor.assert_called_once()
          mock_fs_instance.collection.assert_called_once_with(MOCKED_ENV_VARS["FAISS_INDEXES_COLLECTION"]) # Added assertion
          mock_collection_ref.document.assert_called_once_with(input_data["index_name"]) # Added assertion
          mock_doc_ref.set.assert_called_once()
          actual_set_call_args = mock_doc_ref.set.call_args[0][0]
          assert actual_set_call_args.get("vectorCount") == 1
E       fixture 'mock_storage_client_constructor' not found
>       available fixtures: ADK/agent_data/tests/tools/test_save_metadata_to_faiss.py::<event_loop>, ADK/agent_data/tests/tools/test_save_metadata_to_faiss.py::TestSaveMetadataToFaiss::<event_loop>, _session_event_loop, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_env_vars, mock_logging, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/tests/tools/test_save_metadata_to_faiss.py:639
_ ERROR at setup of TestSaveMetadataToFaiss.test_save_firestore_client_init_fails _
file /Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/tests/tools/test_save_metadata_to_faiss.py, line 751
      @patch(f"{SAVE_TOOL_MODULE_PATH}.OPENAI_AVAILABLE", True)
      @patch(f"{SAVE_TOOL_MODULE_PATH}.openai_client", MagicMock()) # Ensure OpenAI client is mocked as available
      @patch(f"{SAVE_TOOL_MODULE_PATH}.faiss.IndexFlatL2")
      @patch(f"{SAVE_TOOL_MODULE_PATH}.faiss.write_index")
      @patch(f"{SAVE_TOOL_MODULE_PATH}.pickle.dump")
      @patch(FIRESTORE_CLIENT_PATH)
      @patch(UPLOAD_WITH_RETRY_PATH)
      @patch(STORAGE_CLIENT_SAVE_PATH)
      async def test_save_firestore_client_init_fails(self,
                                                   mock_get_embedding,
                                                   MockFaissIndexFlatL2,
                                                   mock_faiss_write_index,
                                                   mock_pickle_dump,
                                                   mock_firestore_constructor,
                                                   mock_upload_with_retry,
                                                   mock_storage_client_constructor,
                                                   mocker, request):
          """Test failure during Firestore client initialization."""
          from ADK.agent_data.tools.save_metadata_to_faiss_tool import save_metadata_to_faiss

          mock_firestore_constructor = mocker.patch(FIRESTORE_CLIENT_PATH)
          mock_firestore_constructor.side_effect = google_exceptions.GoogleCloudError("Mocked Firestore client init failed")

          input_data = {
              "index_name": "test_fs_init_fail_index",
              "metadata_dict": {"doc1": {"text": "Some text"}},
              "text_field_to_embed": "text",
              "dimension": 10
          }

          result = await save_metadata_to_faiss(**input_data)

          print(f"\nResult dictionary for {request.node.name}: {result}\n")
          assert result is not None
          assert result.get("status") == "error"
          assert "Failed to initialize Firestore client" in result.get("message", "")
          assert result.get("meta", {}).get("error_type") in ["GoogleCloudError", "ConfigurationError", "FirestoreError", "FirestoreInitializationError"]
          assert result.get("meta", {}).get("index_name") == "test_fs_init_fail_index"
          # As per user's note on tool modification, check for these fields in meta for errors
          assert "vector_count" not in result.get("meta", {}) # Should fail before vector processing
          assert "dimension" not in result.get("meta", {})    # Should fail before vector processing

          mock_firestore_constructor.assert_called_once()
          # Ensure FAISS/GCS operations are not attempted
          mocker.patch(f"{SAVE_TOOL_MODULE_PATH}.faiss.IndexFlatL2").assert_not_called()
          mocker.patch(UPLOAD_WITH_RETRY_PATH).assert_not_called()
E       fixture 'mock_storage_client_constructor' not found
>       available fixtures: ADK/agent_data/tests/tools/test_save_metadata_to_faiss.py::<event_loop>, ADK/agent_data/tests/tools/test_save_metadata_to_faiss.py::TestSaveMetadataToFaiss::<event_loop>, _session_event_loop, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_env_vars, mock_logging, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/tests/tools/test_save_metadata_to_faiss.py:751
__ ERROR at setup of TestSaveMetadataToFaiss.test_save_firestore_update_fails __
file /Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/test_pass_56/test_save_metadata_to_faiss_prompt20_backup.py, line 556
      @patch(f"{SAVE_TOOL_MODULE_PATH}.openai_client", MagicMock())
      @patch(f"{SAVE_TOOL_MODULE_PATH}.pickle.dump")
      @patch(f"{SAVE_TOOL_MODULE_PATH}.faiss.write_index")
      @patch(f"{SAVE_TOOL_MODULE_PATH}.get_openai_embedding", new_callable=AsyncMock)
      async def test_save_firestore_update_fails(self,
                                                 mock_get_embedding,
                                                 mock_faiss_write_index,
                                                 MockFaissIndexFlatL2,
                                                 mock_pickle_dump,
                                                 mocker, request):
          # Mock dependencies
          mock_firestore_constructor = mocker.patch(FIRESTORE_CLIENT_PATH)
          mock_upload_with_retry_local = mocker.patch(UPLOAD_WITH_RETRY_PATH, return_value=MagicMock()) # GCS succeeds
          mock_storage_client = mocker.patch(STORAGE_CLIENT_SAVE_PATH)

          from ADK.agent_data.tools.save_metadata_to_faiss_tool import save_metadata_to_faiss

          # Setup mock instances
          mock_fs_instance = mock_firestore_constructor.return_value
          mock_collection_ref = mock_fs_instance.collection.return_value
          mock_doc_ref = mock_collection_ref.document.return_value
          # Simulate Firestore .set() failure
          mock_doc_ref.set.side_effect = api_core_exceptions.Aborted("Mocked Firestore update failed")

          mock_actual_storage_client_instance = MagicMock(spec=storage.Client)
          mock_storage_client.return_value = mock_actual_storage_client_instance
          mock_bucket = MagicMock()
          mock_blob = MagicMock()
          mock_actual_storage_client_instance.bucket = MagicMock()
          mock_actual_storage_client_instance.bucket.return_value = mock_bucket
          mock_bucket.blob.return_value = mock_blob

          mock_index_instance = MockFaissIndexFlatL2.return_value
          mock_index_instance.ntotal = 1
          mock_get_embedding.return_value = {"embedding": np.array([0.1]*10, dtype=np.float32), "total_tokens": 5, "status": "success"}

          input_data = {
              "index_name": "test_firestore_fail_index",
              "metadata_dict": {"doc1": {"text": "Firestore test text"}},
              "text_field_to_embed": "text",
              "dimension": 10
          }

          result = await save_metadata_to_faiss(**input_data)

          print(f"\nResult dictionary for {request.node.name}: {result}\n")
          assert result is not None
          assert result.get("status") == "partial_success" # Corrected status
          assert "message" in result
          assert "FAISS/GCS successful, but Firestore update failed" in result["message"] # Corrected message
          assert result.get("meta", {}).get("error_type") == "FirestoreRegistryError"
          assert result.get("firestore_update_status") == "failed"
          assert result.get("gcs_upload_status") == "success" # GCS part should succeed

          # Verify mocks
          MockFaissIndexFlatL2.assert_called_once_with(10)
          mock_index_instance.add.assert_called_once()
          mock_faiss_write_index.assert_called_once()
          mock_pickle_dump.assert_called_once()

          assert mock_upload_with_retry_local.call_count == 2 # Both .faiss and .meta attempted and succeeded
          mock_storage_client.assert_called_once_with(project=MOCKED_ENV_VARS["GOOGLE_CLOUD_PROJECT"])
          mock_actual_storage_client_instance.bucket.assert_called_once_with(MOCKED_ENV_VARS["GCS_BUCKET_NAME"])
          assert mock_bucket.blob.call_count == 2

          mock_firestore_constructor.assert_called_once()
          mock_fs_instance.collection.assert_called_once_with(MOCKED_ENV_VARS["FAISS_INDEXES_COLLECTION"])
          mock_collection_ref.document.assert_called_once_with("test_firestore_fail_index")
          mock_doc_ref.set.assert_called_once() # Attempt to set was made
E       fixture 'mock_pickle_dump' not found
>       available fixtures: ADK/agent_data/utils::<event_loop>, _session_event_loop, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_env_vars, mock_logging, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_pass_56/test_save_metadata_to_faiss_prompt20_backup.py::<event_loop>, test_pass_56/test_save_metadata_to_faiss_prompt20_backup.py::TestSaveMetadataToFaiss::<event_loop>, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/test_pass_56/test_save_metadata_to_faiss_prompt20_backup.py:556
___ ERROR at setup of TestSaveMetadataToFaiss.test_save_empty_metadata_dict ____
file /Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/test_pass_56/test_save_metadata_to_faiss_prompt20_backup.py, line 626
      @patch(f"{SAVE_TOOL_MODULE_PATH}.openai_client", MagicMock()) # Not used, but keep pattern
      @patch(f"{SAVE_TOOL_MODULE_PATH}.pickle.dump")
      @patch(f"{SAVE_TOOL_MODULE_PATH}.faiss.write_index")
      @patch(f"{SAVE_TOOL_MODULE_PATH}.get_openai_embedding", new_callable=AsyncMock)
      async def test_save_empty_metadata_dict(self,
                                              mock_get_embedding,
                                              mock_faiss_write_index,
                                              MockFaissIndexFlatL2,
                                              mock_pickle_dump,
                                              mocker, request):
          # Mock dependencies that might be called even with empty input
          mock_firestore_constructor = mocker.patch(FIRESTORE_CLIENT_PATH)
          mock_upload_with_retry_local = mocker.patch(UPLOAD_WITH_RETRY_PATH)
          mock_storage_client = mocker.patch(STORAGE_CLIENT_SAVE_PATH)

          from ADK.agent_data.tools.save_metadata_to_faiss_tool import save_metadata_to_faiss

          input_data = {
              "index_name": "test_empty_meta_index",
              "metadata_dict": {},
              "text_field_to_embed": "text", # Still need this if auto-embedding path is taken
              "dimension": 10
          }

          result = await save_metadata_to_faiss(**input_data)

          print(f"\nResult dictionary for {request.node.name}: {result}\n")
          assert result is not None
          assert result.get("status") == "error" # Changed from "success"
          assert "No valid texts found for embedding" in result.get("message") # More specific check
          assert result.get("meta", {}).get("embedded_docs_count") == 0 # Check in meta
          assert result.get("index_name") is None # index_name is also in meta for this error
          assert result.get("meta", {}).get("index_name") == "test_empty_meta_index"
          assert result.get("gcs_faiss_path") is None
          assert result.get("gcs_meta_path") is None
          # For early ValueError, gcs_upload_status and firestore_update_status are not set
          # assert result.get("gcs_upload_status") == "skipped"
          # assert result.get("firestore_update_status") == "skipped"
          assert result.get("meta", {}).get("error_type") == "ValueError"

          # Ensure no core processing or I/O operations were performed
          mock_get_embedding.assert_not_called()
          MockFaissIndexFlatL2.assert_not_called()
          mock_faiss_write_index.assert_not_called()
          mock_pickle_dump.assert_not_called()
          mock_upload_with_retry_local.assert_not_called()

          # Firestore client might be initialized but no document set/update should occur for an empty index
          mock_firestore_constructor.assert_called_once()
          mock_fs_instance = mock_firestore_constructor.return_value
          mock_fs_instance.collection.assert_not_called() # collection should not be called for empty metadata
          mock_doc_ref = mock_fs_instance.collection.return_value.document.return_value
          mock_doc_ref.set.assert_not_called()

          mock_storage_client.assert_not_called() # If no files, no client needed for bucket/blob ops
E       fixture 'mock_pickle_dump' not found
>       available fixtures: ADK/agent_data/utils::<event_loop>, _session_event_loop, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_env_vars, mock_logging, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_pass_56/test_save_metadata_to_faiss_prompt20_backup.py::<event_loop>, test_pass_56/test_save_metadata_to_faiss_prompt20_backup.py::TestSaveMetadataToFaiss::<event_loop>, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/test_pass_56/test_save_metadata_to_faiss_prompt20_backup.py:626
_ ERROR at setup of TestSaveMetadataToFaiss.test_save_text_field_missing_in_metadata _
file /Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/test_pass_56/test_save_metadata_to_faiss_prompt20_backup.py, line 682
      @patch(f"{SAVE_TOOL_MODULE_PATH}.openai_client", MagicMock())
      @patch(f"{SAVE_TOOL_MODULE_PATH}.pickle.dump")
      @patch(f"{SAVE_TOOL_MODULE_PATH}.faiss.write_index")
      @patch(f"{SAVE_TOOL_MODULE_PATH}.get_openai_embedding", new_callable=AsyncMock)
      async def test_save_text_field_missing_in_metadata(self,
                                                         mock_get_embedding,
                                                         mock_faiss_write_index,
                                                         MockFaissIndexFlatL2,
                                                         mock_pickle_dump,
                                                         mocker, request):
          mock_firestore_constructor = mocker.patch(FIRESTORE_CLIENT_PATH)
          mock_upload_with_retry_local = mocker.patch(UPLOAD_WITH_RETRY_PATH, return_value=MagicMock())
          mock_storage_client = mocker.patch(STORAGE_CLIENT_SAVE_PATH)

          from ADK.agent_data.tools.save_metadata_to_faiss_tool import save_metadata_to_faiss

          mock_fs_instance = mock_firestore_constructor.return_value
          mock_collection_ref = mock_fs_instance.collection.return_value
          mock_doc_ref = mock_collection_ref.document.return_value

          # Corrected storage client and bucket mocking
          mock_storage_client_constructor = mocker.patch(STORAGE_CLIENT_SAVE_PATH)
          mock_actual_storage_client_instance = mock_storage_client_constructor.return_value

          mock_bucket_instance = MagicMock(spec=storage.Bucket)
          mock_actual_storage_client_instance.bucket.return_value = mock_bucket_instance

          mock_blob_instance = MagicMock(spec=storage.Blob)
          mock_bucket_instance.blob.return_value = mock_blob_instance

          mock_index_instance = MockFaissIndexFlatL2.return_value
          mock_index_instance.ntotal = 2 # Expecting two docs to be embedded (doc1, doc3)
          # doc1 will be embedded, doc2 will be skipped (missing text_field), doc3 will be embedded
          mock_get_embedding.side_effect = [
              {"embedding": np.array([0.1]*10, dtype=np.float32), "total_tokens": 5, "status": "success"}, # for doc1
              {"embedding": np.array([0.3]*10, dtype=np.float32), "total_tokens": 5, "status": "success"}  # for doc3
          ]

          input_metadata = {
              "doc1": {"text": "This is doc1"},
              "doc2": {"other_field": "This is doc2, missing text field"},
              "doc3": {"text": "This is doc3"}
          }
          input_data = {
              "index_name": "test_text_field_missing_index",
              "metadata_dict": input_metadata,
              "text_field_to_embed": "text",
              "dimension": 10
          }

          expected_doc_ids_for_pickle = ['doc1', 'doc3']
          expected_metadata_for_pickle = {
              "doc1": input_metadata["doc1"],
              "doc3": input_metadata["doc3"]
          }
          expected_data_for_pickle = {
              'ids': expected_doc_ids_for_pickle,
              'metadata': expected_metadata_for_pickle
          }

          result = await save_metadata_to_faiss(**input_data)

          print(f"\nResult dictionary for {request.node.name}: {result}\n")
          assert result is not None
          assert result.get("status") == "success"
          assert result.get("vector_count") == 2
          assert result["dimension"] == 10
          assert result.get("index_name") == "test_text_field_missing_index"
          assert result.get("gcs_upload_status") == "success"
          assert result.get("firestore_update_status") == "success"

          mock_get_embedding.assert_has_calls([
              call(agent_context=None, text_to_embed="This is doc1"),
              call(agent_context=None, text_to_embed="This is doc3")
          ], any_order=True)
          assert mock_get_embedding.call_count == 2

          MockFaissIndexFlatL2.assert_called_once_with(10)
          assert mock_index_instance.add.call_count == 1 # Batch add for 2 vectors
          added_vectors = mock_index_instance.add.call_args[0][0]
          assert added_vectors.shape[0] == 2 # Two vectors added

          mock_faiss_write_index.assert_called_once()
          mock_pickle_dump.assert_called_once_with(expected_data_for_pickle, mocker.ANY)
          assert mock_upload_with_retry_local.call_count == 2

          # Verify storage client calls
          mock_storage_client_constructor.assert_called_once_with(project=MOCKED_ENV_VARS["GOOGLE_CLOUD_PROJECT"])
          mock_actual_storage_client_instance.bucket.assert_called_once_with(MOCKED_ENV_VARS["GCS_BUCKET_NAME"])
          assert mock_bucket_instance.blob.call_count == 2
          mock_bucket_instance.blob.assert_has_calls([
              call(f"{input_data['index_name']}.faiss"),
              call(f"{input_data['index_name']}.meta")
          ], any_order=True)

          mock_doc_ref.set.assert_called_once()
E       fixture 'mock_pickle_dump' not found
>       available fixtures: ADK/agent_data/utils::<event_loop>, _session_event_loop, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_env_vars, mock_logging, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_pass_56/test_save_metadata_to_faiss_prompt20_backup.py::<event_loop>, test_pass_56/test_save_metadata_to_faiss_prompt20_backup.py::TestSaveMetadataToFaiss::<event_loop>, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/test_pass_56/test_save_metadata_to_faiss_prompt20_backup.py:682
_ ERROR at setup of TestSaveMetadataToFaiss.test_save_firestore_client_init_fails _
file /Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/test_pass_56/test_save_metadata_to_faiss_prompt20_backup.py, line 923
      @patch(f"{SAVE_TOOL_MODULE_PATH}.openai_client", MagicMock()) # For OPENAI_AVAILABLE check
      @patch(f"{SAVE_TOOL_MODULE_PATH}.OPENAI_AVAILABLE", True)
      @patch(FIRESTORE_CLIENT_PATH)
      async def test_save_firestore_client_init_fails(self, mock_firestore_constructor, mock_openai_available, mock_openai_client_setup, mocker, request):
          """Test failure during Firestore client initialization."""
          from ADK.agent_data.tools.save_metadata_to_faiss_tool import save_metadata_to_faiss

          mock_firestore_constructor.side_effect = google_exceptions.GoogleCloudError("Mocked Firestore client init failed")

          input_data = {
              "index_name": "test_fs_init_fail_index",
              "metadata_dict": {"doc1": {"text": "Some text"}},
              "text_field_to_embed": "text",
              "dimension": 10
          }

          result = await save_metadata_to_faiss(**input_data)

          print(f"\nResult dictionary for {request.node.name}: {result}\n")
          assert result is not None
          assert result.get("status") == "error"
          assert "Failed to initialize Firestore client" in result.get("message", "")
          assert result.get("meta", {}).get("error_type") in ["GoogleCloudError", "ConfigurationError", "FirestoreError"]
          assert result.get("meta", {}).get("index_name") == "test_fs_init_fail_index"
          # As per user's note on tool modification, check for these fields in meta for errors
          assert "vector_count" not in result.get("meta", {}) # Should fail before vector processing
          assert "dimension" not in result.get("meta", {})    # Should fail before vector processing

          mock_firestore_constructor.assert_called_once()
          # Ensure FAISS/GCS operations are not attempted
          mocker.patch(f"{SAVE_TOOL_MODULE_PATH}.faiss.IndexFlatL2").assert_not_called()
          mocker.patch(UPLOAD_WITH_RETRY_PATH).assert_not_called()
E       fixture 'mock_openai_available' not found
>       available fixtures: ADK/agent_data/utils::<event_loop>, _session_event_loop, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_env_vars, mock_logging, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_pass_56/test_save_metadata_to_faiss_prompt20_backup.py::<event_loop>, test_pass_56/test_save_metadata_to_faiss_prompt20_backup.py::TestSaveMetadataToFaiss::<event_loop>, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/test_pass_56/test_save_metadata_to_faiss_prompt20_backup.py:923
_ ERROR at setup of TestSaveMetadataToFaiss.test_save_temp_file_deletion_fails _
file /Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/test_pass_56/test_save_metadata_to_faiss_prompt20_backup.py, line 985
      @patch(f"{SAVE_TOOL_MODULE_PATH}.os.remove")
      @patch(f"{SAVE_TOOL_MODULE_PATH}.os.path.exists", return_value=True)
      @patch(f"{SAVE_TOOL_MODULE_PATH}.openai_client", MagicMock())
      @patch(f"{SAVE_TOOL_MODULE_PATH}.OPENAI_AVAILABLE", True)
      @patch(f"{SAVE_TOOL_MODULE_PATH}.pickle.dump")
      @patch(f"{SAVE_TOOL_MODULE_PATH}.faiss.IndexFlatL2")
      @patch(f"{SAVE_TOOL_MODULE_PATH}.faiss.write_index")
      @patch(f"{SAVE_TOOL_MODULE_PATH}.get_openai_embedding", new_callable=AsyncMock)
      @patch(FIRESTORE_CLIENT_PATH)
      @patch(UPLOAD_WITH_RETRY_PATH)
      @patch(STORAGE_CLIENT_SAVE_PATH)
      async def test_save_temp_file_deletion_fails(self,
                                                   mock_storage_client_constructor,
                                                   mock_upload_with_retry,
                                                   mock_firestore_constructor,
                                                   mock_get_embedding,
                                                   mock_faiss_write_index,
                                                   MockFaissIndexFlatL2,
                                                   mock_pickle_dump,
                                                   mock_os_path_exists, # Renamed from mock_os_exists
                                                   mock_os_remove,
                                                   mock_openai_available, # Renamed for clarity
                                                   mock_openai_client,    # Renamed for clarity
                                                   mocker, request):
          """Test that the tool logs a warning but otherwise succeeds if temp file deletion fails."""
          from ADK.agent_data.tools.save_metadata_to_faiss_tool import save_metadata_to_faiss

          mock_fs_instance = mock_firestore_constructor.return_value
          mock_doc_ref = mock_fs_instance.collection.return_value.document.return_value
          mock_storage_client_instance = mock_storage_client_constructor.return_value
          mock_bucket_instance = mock_storage_client_instance.bucket.return_value
          mock_blob_instance = mock_bucket_instance.blob.return_value
          mock_upload_with_retry.return_value = MagicMock()

          mock_index_instance = MockFaissIndexFlatL2.return_value
          mock_index_instance.ntotal = 1
          mock_get_embedding.return_value = {"embedding": np.array([0.1]*10, dtype=np.float32), "total_tokens": 5, "status": "success"}

          mock_os_remove.side_effect = OSError("Mocked os.remove failed")
          mock_log_warning = mocker.patch(f"{SAVE_TOOL_MODULE_PATH}.logging.warning")

          input_data = {
              "index_name": "test_temp_delete_fail_index",
              "metadata_dict": {"doc1": {"text": "Successful run text"}},
              "text_field_to_embed": "text",
              "dimension": 10
          }

          result = await save_metadata_to_faiss(**input_data)

          print(f"\nResult dictionary for {request.node.name}: {result}\n")
          assert result is not None
          assert result.get("status") == "success"

          temp_faiss_path = f"/tmp/{input_data['index_name']}.faiss"
          temp_meta_path = f"/tmp/{input_data['index_name']}.meta"

          remove_calls = [call(temp_faiss_path), call(temp_meta_path)]
          mock_os_remove.assert_has_calls(remove_calls, any_order=True)
          assert mock_os_remove.call_count == 2

          warning_calls = [
              call(f"Failed to remove temporary file {temp_faiss_path}: Mocked os.remove failed"),
              call(f"Failed to remove temporary file {temp_meta_path}: Mocked os.remove failed")
          ]
          mock_log_warning.assert_has_calls(warning_calls, any_order=True)
          assert mock_log_warning.call_count >= 2 # Can be more if other warnings are logged
E       fixture 'mock_openai_available' not found
>       available fixtures: ADK/agent_data/utils::<event_loop>, _session_event_loop, anyio_backend, anyio_backend_name, anyio_backend_options, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, class_mocker, cov, doctest_namespace, event_loop, event_loop_policy, free_tcp_port, free_tcp_port_factory, free_udp_port, free_udp_port_factory, mock_env_vars, mock_logging, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, test_pass_56/test_save_metadata_to_faiss_prompt20_backup.py::<event_loop>, test_pass_56/test_save_metadata_to_faiss_prompt20_backup.py::TestSaveMetadataToFaiss::<event_loop>, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, unused_tcp_port, unused_tcp_port_factory, unused_udp_port, unused_udp_port_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/test_pass_56/test_save_metadata_to_faiss_prompt20_backup.py:985
=================================== FAILURES ===================================
_______________ TestMCPStdioAllTools.test_core_tools_and_errors ________________

self = <test_mcp_agent_all_tools.TestMCPStdioAllTools testMethod=test_core_tools_and_errors>

    def test_core_tools_and_errors(self):
        """Tests multiple basic tools and error conditions using the shared server process."""
        logger.info("Starting test_core_tools_and_errors")

        test_cases = [
            # 1. Add Numbers
            {
                "id": "req-add-1", "request": {"tool": "add_numbers", "input": {"a": 10, "b": 5}},
                "expected_status": "success", "expected_result": 15
            },
            # 2. Multiply Numbers
            {
                "id": "req-mul-2", "request": {"tool": "multiply_numbers", "input": {"a": 7, "b": 3}},
                "expected_status": "success", "expected_result": 21
            },
            # 3. Save Text (Check for success status, ignore specific path)
            {
                "id": "req-sav-3", "request": {"tool": "save_text", "input": {"filename": "test_all_tools.txt", "content": "Testing save..."}},
                "expected_status": "success",
                "expected_result_contains_string": "Saved text: test_all_tools.txt"
            },
             # 4. Nonexistent Tool
            {
                "id": "req-err-4", "request": {"tool": "absolutely_fake_tool", "input": {}},
                "expected_status": "error", "expected_error_contains": "Unknown tool: absolutely_fake_tool"
            },
            # 5. Missing input for a tool that requires it (e.g., add_numbers)
            #    (Behavior depends on tool implementation - might raise TypeError or handle default)
            #    Let's assume add_numbers raises an error if args are missing/wrong type
            {
                "id": "req-err-5", "request": {"tool": "add_numbers", "input": {"a": 5}}, # Missing 'b'
                "expected_status": "error", "expected_error_contains": "missing 1 required positional argument: 'b'" # Or similar TypeError
            },
             # 6. Get Registered Tools (Assuming placeholder server implements this)
             # This test might need adjustment based on local_mcp_server.py implementation
            {
                "id": "req-get-6", "request": {"tool": "get_registered_tools", "input": {}},
                "expected_status": "success", # Or "error" if not implemented
                "expected_result_is_list": True,
                "expected_result_has_tool": "echo" # Check if a known tool is listed
            },
        ]

        for i, case in enumerate(test_cases):
            # Add meta request_id to the request
            case["request"]["meta"] = {"request_id": case["id"]}

            with self.subTest(f"Test Case {i+1}: {case['request'].get('tool', 'Malformed')}"):
                logger.debug(f"--- Running Subtest: {case['request']['tool']} ---")
                self.send_request(case["request"])
                response = self.receive_response()

                self.assertIsNotNone(response, "Did not receive response from server.")
                self.assertEqual(response.get('meta', {}).get('request_id'), case["id"])
>               self.assertEqual(response.get('meta', {}).get('status'), case["expected_status"])
E               AssertionError: 'error' != 'success'
E               - error
E               + success

ADK/agent_data/test_pass_53/test_mcp_agent_all_tools.py:190: AssertionError
---------------------------- Captured log teardown -----------------------------
WARNING  test_mcp_agent_all_tools:test_mcp_agent_all_tools.py:109 Shared MCP Server Stderr:
2025-05-16 18:22:18,178 - INFO - sys.path in Cloud Run at top of register_tools.py: ['/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data', '/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/mcp', '/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents', '/opt/homebrew/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python310.zip', '/opt/homebrew/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python3.10', '/opt/homebrew/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python3.10/lib-dynload', '/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/setup/venv/lib/python3.10/site-packages']
2025-05-16 18:22:18,297 - INFO - Loading faiss.
2025-05-16 18:22:18,314 - INFO - Successfully loaded faiss.
2025-05-16 18:22:18,316 - INFO - Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.
2025-05-16 18:22:19,301 - INFO - OpenAI import successful.
2025-05-16 18:22:19,301 - INFO - FAISS import successful.
2025-05-16 18:22:19,301 - WARNING - OPENAI_API_KEY environment variable not set. Tools requiring OpenAI embeddings will not be available.
2025-05-16 18:22:19,341 - INFO - sys.path in Cloud Run at top of register_tools.py: ['/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data', '/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/mcp', '/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents', '/opt/homebrew/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python310.zip', '/opt/homebrew/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python3.10', '/opt/homebrew/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python3.10/lib-dynload', '/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/setup/venv/lib/python3.10/site-packages']
2025-05-16 18:22:19,343 - INFO - OpenAI import successful.
2025-05-16 18:22:19,343 - INFO - FAISS import successful.
2025-05-16 18:22:19,343 - WARNING - OPENAI_API_KEY environment variable not set. Tools requiring OpenAI embeddings will not be available.
2025-05-16 18:22:19,345 - INFO - Checking dependencies inside get_all_tool_functions: FAISS_AVAILABLE=True, OPENAI_AVAILABLE=True
2025-05-16 18:22:19,345 - INFO - Base local tools collected: ['save_text', 'add_numbers', 'multiply_numbers', 'echo', 'get_registered_tools', 'delay', 'save_document', 'vectorize_document', 'update_metadata', 'query_metadata', 'multi_update_metadata', 'bulk_delete_metadata', 'bulk_update_metadata', 'multi_field_update', 'semantic_search_local', 'find_metadata_by_key', 'semantic_search_metadata', 'semantic_search_by_author', 'semantic_search_by_year', 'semantic_search_by_keyword', 'conditional_search_metadata', 'semantic_search_multiple_fields', 'sort_metadata', 'advanced_semantic_search', 'create_metadata_tree', 'view_metadata_tree', 'delete_metadata_node', 'update_metadata_node', 'depth_first_search', 'rebuild_metadata_tree', 'semantic_search_metadata_tree', 'validate_metadata_tree', 'generate_embedding', 'semantic_similarity_search', 'batch_generate_embeddings', 'semantic_expand_metadata', 'semantic_filter_metadata', 'analyze_metadata_trends', 'aggregate_metadata', 'detect_anomalies', 'metadata_statistics', 'error_tool']
2025-05-16 18:22:19,345 - INFO - FAISS_AVAILABLE is True, attempting to add FAISS tools...
2025-05-16 18:22:19,345 - INFO - FAISS tools added. Current tools: ['save_text', 'add_numbers', 'multiply_numbers', 'echo', 'get_registered_tools', 'delay', 'save_document', 'vectorize_document', 'update_metadata', 'query_metadata', 'multi_update_metadata', 'bulk_delete_metadata', 'bulk_update_metadata', 'multi_field_update', 'semantic_search_local', 'find_metadata_by_key', 'semantic_search_metadata', 'semantic_search_by_author', 'semantic_search_by_year', 'semantic_search_by_keyword', 'conditional_search_metadata', 'semantic_search_multiple_fields', 'sort_metadata', 'advanced_semantic_search', 'create_metadata_tree', 'view_metadata_tree', 'delete_metadata_node', 'update_metadata_node', 'depth_first_search', 'rebuild_metadata_tree', 'semantic_search_metadata_tree', 'validate_metadata_tree', 'generate_embedding', 'semantic_similarity_search', 'batch_generate_embeddings', 'semantic_expand_metadata', 'semantic_filter_metadata', 'analyze_metadata_trends', 'aggregate_metadata', 'detect_anomalies', 'metadata_statistics', 'error_tool', 'save_metadata_to_faiss', 'load_metadata_from_faiss', 'query_metadata_faiss', 'advanced_query_faiss', 'rebuild_metadata_tree_from_faiss']
2025-05-16 18:22:19,345 - INFO - OPENAI_AVAILABLE is True, attempting to add OpenAI tools...
2025-05-16 18:22:19,345 - INFO - OpenAI tools added: ['generate_embedding_real', 'semantic_search_cosine', 'clear_embeddings']
2025-05-16 18:22:19,345 - INFO - Discovered tools: ['save_text', 'add_numbers', 'multiply_numbers', 'echo', 'get_registered_tools', 'delay', 'save_document', 'vectorize_document', 'update_metadata', 'query_metadata', 'multi_update_metadata', 'bulk_delete_metadata', 'bulk_update_metadata', 'multi_field_update', 'semantic_search_local', 'find_metadata_by_key', 'semantic_search_metadata', 'semantic_search_by_author', 'semantic_search_by_year', 'semantic_search_by_keyword', 'conditional_search_metadata', 'semantic_search_multiple_fields', 'sort_metadata', 'advanced_semantic_search', 'create_metadata_tree', 'view_metadata_tree', 'delete_metadata_node', 'update_metadata_node', 'depth_first_search', 'rebuild_metadata_tree', 'semantic_search_metadata_tree', 'validate_metadata_tree', 'generate_embedding', 'semantic_similarity_search', 'batch_generate_embeddings', 'semantic_expand_metadata', 'semantic_filter_metadata', 'analyze_metadata_trends', 'aggregate_metadata', 'detect_anomalies', 'metadata_statistics', 'error_tool', 'save_metadata_to_faiss', 'load_metadata_from_faiss', 'query_metadata_faiss', 'advanced_query_faiss', 'rebuild_metadata_tree_from_faiss', 'generate_embedding_real', 'semantic_search_cosine', 'clear_embeddings']
2025-05-16 18:22:19,345 - INFO - Starting MCP Stdio Server...
2025-05-16 18:22:19,345 - INFO - MCP Stdio Server Started. Waiting for JSON input on stdin...
2025-05-16 18:22:19,730 - INFO - Executing tool 'add_numbers' (ID: req-add-1) with input: {'a': 10, 'b': 5}
2025-05-16 18:22:19,731 - INFO - Executing add_numbers tool with: a=10 (<class 'int'>), b=5 (<class 'int'>)
2025-05-16 18:22:19,731 - INFO - add_numbers calculation result: 15
2025-05-16 18:22:19,731 - INFO - Tool 'add_numbers' executed successfully (ID: req-add-1).
2025-05-16 18:22:19,732 - INFO - Executing tool 'multiply_numbers' (ID: req-mul-2) with input: {'a': 7, 'b': 3}
2025-05-16 18:22:19,732 - INFO - Executing multiply_numbers tool with kwargs: {'a': 7, 'b': 3}
2025-05-16 18:22:19,732 - INFO - Calculation: 7 * 3 = 21
2025-05-16 18:22:19,732 - INFO - Tool 'multiply_numbers' executed successfully (ID: req-mul-2).
2025-05-16 18:22:19,732 - INFO - Executing tool 'save_text' (ID: req-sav-3) with input: {'filename': 'test_all_tools.txt', 'content': 'Testing save...'}
2025-05-16 18:22:19,732 - INFO - Executing save_text tool with filename: test_all_tools.txt
2025-05-16 18:22:19,734 - INFO - Successfully saved text to test_all_tools.txt in /Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/tools/../test_outputs
2025-05-16 18:22:19,734 - INFO - Tool 'save_text' executed successfully (ID: req-sav-3).
2025-05-16 18:22:19,734 - ERROR - Unknown tool requested (ID: req-err-4): absolutely_fake_tool
2025-05-16 18:22:19,734 - INFO - Executing tool 'add_numbers' (ID: req-err-5) with input: {'a': 5}
2025-05-16 18:22:19,734 - ERROR - Error executing tool 'add_numbers' (ID: req-err-5): add_numbers() missing 1 required positional argument: 'b'
Traceback (most recent call last):
  File "/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/mcp/local_mcp_server.py", line 105, in run_mcp_loop
    raise e # Re-raise original TypeError if fallback doesn't match signature
  File "/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/mcp/local_mcp_server.py", line 98, in run_mcp_loop
    tool_result_data = tool_function(**tool_input)
TypeError: add_numbers() missing 1 required positional argument: 'b'
2025-05-16 18:22:19,735 - ERROR - Error executing tool 'add_numbers' (ID: req-err-5): add_numbers() missing 1 required positional argument: 'b'
Traceback (most recent call last):
  File "/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/mcp/local_mcp_server.py", line 120, in run_mcp_loop
    raise exec_err
  File "/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/mcp/local_mcp_server.py", line 105, in run_mcp_loop
    raise e # Re-raise original TypeError if fallback doesn't match signature
  File "/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/mcp/local_mcp_server.py", line 98, in run_mcp_loop
    tool_result_data = tool_function(**tool_input)
TypeError: add_numbers() missing 1 required positional argument: 'b'
2025-05-16 18:22:19,738 - INFO - Executing tool 'get_registered_tools' (ID: req-get-6) with input: {}
2025-05-16 18:22:19,738 - ERROR - Error executing tool 'get_registered_tools' (ID: req-get-6): get_registered_tools() missing 1 required positional argument: 'agent_context'
Traceback (most recent call last):
  File "/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/mcp/local_mcp_server.py", line 94, in run_mcp_loop
    tool_result_data = tool_function() # Call without arguments
TypeError: get_registered_tools() missing 1 required positional argument: 'agent_context'
2025-05-16 18:22:19,738 - ERROR - Error executing tool 'get_registered_tools' (ID: req-get-6): get_registered_tools() missing 1 required positional argument: 'agent_context'
Traceback (most recent call last):
  File "/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/mcp/local_mcp_server.py", line 120, in run_mcp_loop
    raise exec_err
  File "/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/mcp/local_mcp_server.py", line 94, in run_mcp_loop
    tool_result_data = tool_function() # Call without arguments
TypeError: get_registered_tools() missing 1 required positional argument: 'agent_context'
2025-05-16 18:22:19,896 - INFO - Exit command received (ID: None). Shutting down.
2025-05-16 18:22:19,896 - INFO - MCP Stdio Server loop finished.
2025-05-16 18:22:19,896 - INFO - MCP Stdio Server Stopped.
_______ TestMCPAgentLargeBatchDirect.test_large_batch_processing_direct ________

self = <test_mcp_agent_batch_large.TestMCPAgentLargeBatchDirect testMethod=test_large_batch_processing_direct>

    def test_large_batch_processing_direct(self):
        """Test sending a batch of 100 requests directly to the agent instance."""
        num_requests = 100
        batch_requests = []
        expected_outcomes = {} # Store expected outcome (status, result/error check) for each request id

        # Define target counts for different outcomes
        num_timeout = 15
        num_runtime_error = 15
        num_missing_tool = 15
        num_missing_args = 15
        num_success = num_requests - (num_timeout + num_runtime_error + num_missing_tool + num_missing_args)

        # Divide success cases among available success tools
        num_add = math.ceil(num_success / 4)
        num_multiply = math.ceil(num_success / 4)
        num_save = math.ceil(num_success / 4)
        num_echo = num_success - (num_add + num_multiply + num_save) # Remaining go to echo

        request_types = (
            ['timeout'] * num_timeout +
            ['runtime_error'] * num_runtime_error +
            ['missing_tool'] * num_missing_tool +
            ['missing_args'] * num_missing_args +
            # --- START REPLACEMENT: Use calculated counts ---
            # (['add'] * math.ceil(num_success / 3)) +
            # (['multiply'] * math.ceil(num_success / 3)) +
            # (['save'] * math.floor(num_success / 3))
            (['add'] * num_add) +
            (['multiply'] * num_multiply) +
            (['save'] * num_save) +
            (['echo_test'] * num_echo) # Add echo cases
            # --- END REPLACEMENT ---
        )
        # Ensure the list has exactly num_requests elements, trim/pad if needed due to rounding/distribution
        # --- START REPLACEMENT: Ensure exact number of requests ---
        if len(request_types) > num_requests:
            request_types = request_types[:num_requests]
        elif len(request_types) < num_requests:
            # Add more success cases if needed (e.g., echo)
            request_types.extend(['echo_test'] * (num_requests - len(request_types)))
        # --- END REPLACEMENT ---
        random.shuffle(request_types) # Shuffle the types

        print(f"Generating {num_requests} requests with distribution:")
        print(f"  Success (add/multiply/save/echo): {num_success}")
        print(f"  Timeout (delay): {num_timeout}")
        print(f"  Runtime Error (error_tool): {num_runtime_error}")
        print(f"  Missing Tool: {num_missing_tool}")
        print(f"  Missing Args (add_numbers): {num_missing_args}")

        for i in range(num_requests):
            req_id = f"batch-req-{i}"
            tool_choice = request_types[i]
            request = None # Initialize request
            expected_outcome_base = {"expected_status": None} # Ensure this key always exists

            if tool_choice == 'add':
                a, b = random.randint(1, 1000), random.randint(1, 1000)
                request = {"id": req_id, "tool_name": "add_numbers", "kwargs": {"a": a, "b": b}}
                expected_outcome = {**expected_outcome_base, "expected_status": "success", "check_result": True, "expected_value": a + b}
            elif tool_choice == 'multiply':
                a, b = random.randint(1, 100), random.randint(1, 100)
                request = {"id": req_id, "tool_name": "multiply_numbers", "kwargs": {"a": a, "b": b}}
                expected_outcome = {**expected_outcome_base, "expected_status": "success", "check_result": True, "expected_value": a * b}
            elif tool_choice == 'save':
                 file_content = f"Test data {i} for {req_id}"
                 filename = f"test_file_{req_id}.txt"
                 request = {"id": req_id, "tool_name": "save_text", "kwargs": {"filename": filename, "content": file_content}}
                 expected_result_str = f"Saved text: {filename}"
                 expected_outcome = {**expected_outcome_base, "expected_status": "success", "check_result": True, "expected_value": expected_result_str}
            elif tool_choice == 'timeout':
                 request = {"id": req_id, "tool_name": "delay", "kwargs": {"delay_ms": 6000}}
                 expected_outcome = {**expected_outcome_base, "expected_status": "timeout", "check_error": True, "error_contains": "timed out"}
            elif tool_choice == 'runtime_error':
                 request = {"id": req_id, "tool_name": "error_tool", "args": []}
                 expected_outcome = {**expected_outcome_base, "expected_status": "runtime_error", "check_error": True, "error_contains": "Intentional error"}
            elif tool_choice == 'echo_test':
                 echo_arg = f"Echo data for {req_id}"
                 request = {"id": req_id, "tool_name": "echo", "args": [echo_arg]}
                 expected_outcome = {**expected_outcome_base, "expected_status": "success", "check_result": True, "expected_value": echo_arg}
            elif tool_choice == 'missing_args':
                 request = {"id": req_id, "tool_name": "add_numbers", "kwargs": {"a": 1}}
                 expected_outcome = {**expected_outcome_base, "expected_status": "missing_args", "check_error": True, "error_contains": "missing 1 required positional argument"}
            elif tool_choice == 'missing_tool':
                 request = {"id": req_id, "tool_name": "nonexistent_tool", "args": []}
                 expected_outcome = {**expected_outcome_base, "expected_status": "missing_tool", "check_error": True, "error_contains": "Tool 'nonexistent_tool' not found"}

            if request: # Ensure a request was actually generated for the type
                batch_requests.append(request)
                expected_outcomes[req_id] = expected_outcome # Store the generated outcome
            else:
                print(f"Warning: No request generated for type '{tool_choice}' at index {i}. Adjusting num_requests.")
                # If a request type fails generation, we might need to adjust num_requests,
                # but let's assume generation is robust for now. If not, remove the corresponding entry from expected_outcomes too.

        # Ensure every req_id in batch_requests has an entry in expected_outcomes
        # (This check should ideally not be needed if generation logic is correct)
        generated_req_ids = {req['id'] for req in batch_requests}
        missing_outcomes = generated_req_ids - set(expected_outcomes.keys())
        if missing_outcomes:
            print(f"Warning: Missing expected outcomes for req_ids: {missing_outcomes}")
            # Handle missing outcomes if necessary, e.g., by adding default error expectations
            for missing_id in missing_outcomes:
                 expected_outcomes[missing_id] = {"expected_status": "error"} # Or some other default

        # Prune expected_outcomes for requests that weren't generated (if any)
        expected_outcomes = {k: v for k, v in expected_outcomes.items() if k in generated_req_ids}

        # Correct the total number of requests if any weren't generated
        num_requests = len(batch_requests)
        print(f"Actual number of requests being sent: {num_requests}")

        # Run the agent coroutine using asyncio.run
        print(f"Sending batch of {len(batch_requests)} requests directly to agent...")
        start_time = time.monotonic()

        # We need to run the async method within the sync test method
        try:
            # Update call to use the renamed helper
>           response_list = asyncio.run(self._run_agent_batch_async(batch_requests))

ADK/agent_data/test_pass_53/test_mcp_agent_batch_large.py:218:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/runners.py:44: in run
    return loop.run_until_complete(main)
/opt/homebrew/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py:649: in run_until_complete
    return future.result()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <test_mcp_agent_batch_large.TestMCPAgentLargeBatchDirect testMethod=test_large_batch_processing_direct>
input_data = [{'id': 'batch-req-0', 'kwargs': {'a': 74, 'b': 57}, 'tool_name': 'multiply_numbers'}, {'args': [], 'id': 'batch-req-1...h-req-4', 'tool_name': 'nonexistent_tool'}, {'id': 'batch-req-5', 'kwargs': {'a': 1}, 'tool_name': 'add_numbers'}, ...]

    async def _run_agent_batch_async(self, input_data: Union[Dict, List]) -> Union[Dict, List]:
        """Runs the agent's processing method directly."""
        print(f"Running agent directly with input type: {type(input_data).__name__}")
        # Check for and call run_batch
        if not hasattr(self.agent, 'run_batch'):
>            raise AttributeError("MCPAgent instance does not have 'run_batch' method.")
E            AttributeError: MCPAgent instance does not have 'run_batch' method.

ADK/agent_data/test_pass_53/test_mcp_agent_batch_large.py:89: AttributeError

During handling of the above exception, another exception occurred:

self = <test_mcp_agent_batch_large.TestMCPAgentLargeBatchDirect testMethod=test_large_batch_processing_direct>

    def test_large_batch_processing_direct(self):
        """Test sending a batch of 100 requests directly to the agent instance."""
        num_requests = 100
        batch_requests = []
        expected_outcomes = {} # Store expected outcome (status, result/error check) for each request id

        # Define target counts for different outcomes
        num_timeout = 15
        num_runtime_error = 15
        num_missing_tool = 15
        num_missing_args = 15
        num_success = num_requests - (num_timeout + num_runtime_error + num_missing_tool + num_missing_args)

        # Divide success cases among available success tools
        num_add = math.ceil(num_success / 4)
        num_multiply = math.ceil(num_success / 4)
        num_save = math.ceil(num_success / 4)
        num_echo = num_success - (num_add + num_multiply + num_save) # Remaining go to echo

        request_types = (
            ['timeout'] * num_timeout +
            ['runtime_error'] * num_runtime_error +
            ['missing_tool'] * num_missing_tool +
            ['missing_args'] * num_missing_args +
            # --- START REPLACEMENT: Use calculated counts ---
            # (['add'] * math.ceil(num_success / 3)) +
            # (['multiply'] * math.ceil(num_success / 3)) +
            # (['save'] * math.floor(num_success / 3))
            (['add'] * num_add) +
            (['multiply'] * num_multiply) +
            (['save'] * num_save) +
            (['echo_test'] * num_echo) # Add echo cases
            # --- END REPLACEMENT ---
        )
        # Ensure the list has exactly num_requests elements, trim/pad if needed due to rounding/distribution
        # --- START REPLACEMENT: Ensure exact number of requests ---
        if len(request_types) > num_requests:
            request_types = request_types[:num_requests]
        elif len(request_types) < num_requests:
            # Add more success cases if needed (e.g., echo)
            request_types.extend(['echo_test'] * (num_requests - len(request_types)))
        # --- END REPLACEMENT ---
        random.shuffle(request_types) # Shuffle the types

        print(f"Generating {num_requests} requests with distribution:")
        print(f"  Success (add/multiply/save/echo): {num_success}")
        print(f"  Timeout (delay): {num_timeout}")
        print(f"  Runtime Error (error_tool): {num_runtime_error}")
        print(f"  Missing Tool: {num_missing_tool}")
        print(f"  Missing Args (add_numbers): {num_missing_args}")

        for i in range(num_requests):
            req_id = f"batch-req-{i}"
            tool_choice = request_types[i]
            request = None # Initialize request
            expected_outcome_base = {"expected_status": None} # Ensure this key always exists

            if tool_choice == 'add':
                a, b = random.randint(1, 1000), random.randint(1, 1000)
                request = {"id": req_id, "tool_name": "add_numbers", "kwargs": {"a": a, "b": b}}
                expected_outcome = {**expected_outcome_base, "expected_status": "success", "check_result": True, "expected_value": a + b}
            elif tool_choice == 'multiply':
                a, b = random.randint(1, 100), random.randint(1, 100)
                request = {"id": req_id, "tool_name": "multiply_numbers", "kwargs": {"a": a, "b": b}}
                expected_outcome = {**expected_outcome_base, "expected_status": "success", "check_result": True, "expected_value": a * b}
            elif tool_choice == 'save':
                 file_content = f"Test data {i} for {req_id}"
                 filename = f"test_file_{req_id}.txt"
                 request = {"id": req_id, "tool_name": "save_text", "kwargs": {"filename": filename, "content": file_content}}
                 expected_result_str = f"Saved text: {filename}"
                 expected_outcome = {**expected_outcome_base, "expected_status": "success", "check_result": True, "expected_value": expected_result_str}
            elif tool_choice == 'timeout':
                 request = {"id": req_id, "tool_name": "delay", "kwargs": {"delay_ms": 6000}}
                 expected_outcome = {**expected_outcome_base, "expected_status": "timeout", "check_error": True, "error_contains": "timed out"}
            elif tool_choice == 'runtime_error':
                 request = {"id": req_id, "tool_name": "error_tool", "args": []}
                 expected_outcome = {**expected_outcome_base, "expected_status": "runtime_error", "check_error": True, "error_contains": "Intentional error"}
            elif tool_choice == 'echo_test':
                 echo_arg = f"Echo data for {req_id}"
                 request = {"id": req_id, "tool_name": "echo", "args": [echo_arg]}
                 expected_outcome = {**expected_outcome_base, "expected_status": "success", "check_result": True, "expected_value": echo_arg}
            elif tool_choice == 'missing_args':
                 request = {"id": req_id, "tool_name": "add_numbers", "kwargs": {"a": 1}}
                 expected_outcome = {**expected_outcome_base, "expected_status": "missing_args", "check_error": True, "error_contains": "missing 1 required positional argument"}
            elif tool_choice == 'missing_tool':
                 request = {"id": req_id, "tool_name": "nonexistent_tool", "args": []}
                 expected_outcome = {**expected_outcome_base, "expected_status": "missing_tool", "check_error": True, "error_contains": "Tool 'nonexistent_tool' not found"}

            if request: # Ensure a request was actually generated for the type
                batch_requests.append(request)
                expected_outcomes[req_id] = expected_outcome # Store the generated outcome
            else:
                print(f"Warning: No request generated for type '{tool_choice}' at index {i}. Adjusting num_requests.")
                # If a request type fails generation, we might need to adjust num_requests,
                # but let's assume generation is robust for now. If not, remove the corresponding entry from expected_outcomes too.

        # Ensure every req_id in batch_requests has an entry in expected_outcomes
        # (This check should ideally not be needed if generation logic is correct)
        generated_req_ids = {req['id'] for req in batch_requests}
        missing_outcomes = generated_req_ids - set(expected_outcomes.keys())
        if missing_outcomes:
            print(f"Warning: Missing expected outcomes for req_ids: {missing_outcomes}")
            # Handle missing outcomes if necessary, e.g., by adding default error expectations
            for missing_id in missing_outcomes:
                 expected_outcomes[missing_id] = {"expected_status": "error"} # Or some other default

        # Prune expected_outcomes for requests that weren't generated (if any)
        expected_outcomes = {k: v for k, v in expected_outcomes.items() if k in generated_req_ids}

        # Correct the total number of requests if any weren't generated
        num_requests = len(batch_requests)
        print(f"Actual number of requests being sent: {num_requests}")

        # Run the agent coroutine using asyncio.run
        print(f"Sending batch of {len(batch_requests)} requests directly to agent...")
        start_time = time.monotonic()

        # We need to run the async method within the sync test method
        try:
            # Update call to use the renamed helper
            response_list = asyncio.run(self._run_agent_batch_async(batch_requests))
        except Exception as e:
>            self.fail(f"Agent execution failed: {e}\n{traceback.format_exc()}")
E            AssertionError: Agent execution failed: MCPAgent instance does not have 'run_batch' method.
E            Traceback (most recent call last):
E              File "/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/test_pass_53/test_mcp_agent_batch_large.py", line 218, in test_large_batch_processing_direct
E                response_list = asyncio.run(self._run_agent_batch_async(batch_requests))
E              File "/opt/homebrew/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/runners.py", line 44, in run
E                return loop.run_until_complete(main)
E              File "/opt/homebrew/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py", line 649, in run_until_complete
E                return future.result()
E              File "/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/test_pass_53/test_mcp_agent_batch_large.py", line 89, in _run_agent_batch_async
E                raise AttributeError("MCPAgent instance does not have 'run_batch' method.")
E            AttributeError: MCPAgent instance does not have 'run_batch' method.

ADK/agent_data/test_pass_53/test_mcp_agent_batch_large.py:220: AssertionError
---------------------------- Captured stdout setup -----------------------------
Initializing MCPAgent instance for tests...
MCPAgent instance created successfully.
---------------------------- Captured stderr setup -----------------------------
2025-05-16 18:22:20,072 [CORE-INIT] INFO     - Initializing MCPAgent Core...
2025-05-16 18:22:20,073 [CORE-INIT] INFO     - MCPAgent Core initialized. Available tools: ['save_text', 'add_numbers', 'multiply_numbers', 'echo', 'get_registered_tools', 'delay', 'save_document', 'vectorize_document', 'update_metadata', 'query_metadata', 'multi_update_metadata', 'bulk_delete_metadata', 'bulk_update_metadata', 'multi_field_update', 'semantic_search_local', 'find_metadata_by_key', 'semantic_search_metadata', 'semantic_search_by_author', 'semantic_search_by_year', 'semantic_search_by_keyword', 'conditional_search_metadata', 'semantic_search_multiple_fields', 'sort_metadata', 'advanced_semantic_search', 'create_metadata_tree', 'view_metadata_tree', 'delete_metadata_node', 'update_metadata_node', 'depth_first_search', 'rebuild_metadata_tree', 'semantic_search_metadata_tree', 'validate_metadata_tree', 'generate_embedding', 'semantic_similarity_search', 'batch_generate_embeddings', 'semantic_expand_metadata', 'semantic_filter_metadata', 'analyze_metadata_trends', 'aggregate_metadata', 'detect_anomalies', 'metadata_statistics', 'error_tool', 'save_metadata_to_faiss', 'load_metadata_from_faiss', 'query_metadata_faiss', 'advanced_query_faiss', 'rebuild_metadata_tree_from_faiss', 'generate_embedding_real', 'semantic_search_cosine', 'clear_embeddings']
------------------------------ Captured log setup ------------------------------
INFO     MCPAgentCore:mcp_agent_core.py:58 Initializing MCPAgent Core...
INFO     MCPAgentCore:mcp_agent_core.py:62 MCPAgent Core initialized. Available tools: ['save_text', 'add_numbers', 'multiply_numbers', 'echo', 'get_registered_tools', 'delay', 'save_document', 'vectorize_document', 'update_metadata', 'query_metadata', 'multi_update_metadata', 'bulk_delete_metadata', 'bulk_update_metadata', 'multi_field_update', 'semantic_search_local', 'find_metadata_by_key', 'semantic_search_metadata', 'semantic_search_by_author', 'semantic_search_by_year', 'semantic_search_by_keyword', 'conditional_search_metadata', 'semantic_search_multiple_fields', 'sort_metadata', 'advanced_semantic_search', 'create_metadata_tree', 'view_metadata_tree', 'delete_metadata_node', 'update_metadata_node', 'depth_first_search', 'rebuild_metadata_tree', 'semantic_search_metadata_tree', 'validate_metadata_tree', 'generate_embedding', 'semantic_similarity_search', 'batch_generate_embeddings', 'semantic_expand_metadata', 'semantic_filter_metadata', 'analyze_metadata_trends', 'aggregate_metadata', 'detect_anomalies', 'metadata_statistics', 'error_tool', 'save_metadata_to_faiss', 'load_metadata_from_faiss', 'query_metadata_faiss', 'advanced_query_faiss', 'rebuild_metadata_tree_from_faiss', 'generate_embedding_real', 'semantic_search_cosine', 'clear_embeddings']
----------------------------- Captured stdout call -----------------------------
Generating 100 requests with distribution:
  Success (add/multiply/save/echo): 40
  Timeout (delay): 15
  Runtime Error (error_tool): 15
  Missing Tool: 15
  Missing Args (add_numbers): 15
Actual number of requests being sent: 100
Sending batch of 100 requests directly to agent...
Running agent directly with input type: list
--------------------------- Captured stdout teardown ---------------------------
Cleaning up test output files...
___________ TestMCPBatchToolsStdio.test_batch_mixed_success_failure ____________

self = <test_mcp_agent_batch_tools.TestMCPBatchToolsStdio testMethod=test_batch_mixed_success_failure>

    def test_batch_mixed_success_failure(self):
        logger.info("Starting test_batch_mixed_success_failure")
        batch_request = [
            {"id": "mix-1", "tool_name": "save_document", "args": {"filename": "mix.txt"}}, # Success
            {"id": "mix-2", "tool_name": "update_metadata", "args": {"key": "fail_key"}}, # Fail (simulated)
            {"id": "mix-3", "tool_name": "nonexistent_tool", "args": {}}, # Fail (tool not found)
            "invalid_item", # Fail (invalid type in batch)
            {"id": "mix-5", "tool_name": "query_metadata", "args": {"key": "meta1"}} # Success
        ]

>       output = self.run_mcp_request(batch_request)

ADK/agent_data/test_pass_53/test_mcp_agent_batch_tools.py:281:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <test_mcp_agent_batch_tools.TestMCPBatchToolsStdio testMethod=test_batch_mixed_success_failure>
input_data = [{'args': {'filename': 'mix.txt'}, 'id': 'mix-1', 'tool_name': 'save_document'}, {'args': {'key': 'fail_key'}, 'id': '...l_name': 'nonexistent_tool'}, 'invalid_item', {'args': {'key': 'meta1'}, 'id': 'mix-5', 'tool_name': 'query_metadata'}]

    def run_mcp_request(self, input_data: Union[Dict, List]):
        """Helper to send one request (single or batch) and get the response via mocked stdio."""
        input_json = json.dumps(input_data) + "\n"
        self.mock_stdin.write(input_json)
        self.mock_stdin.seek(0) # Prepare stdin to be read

        # Clear previous stdout before processing
        self.mock_stdout.seek(0)
        self.mock_stdout.truncate(0)

        # Process the single request/batch using the existing mcp_agent instance
        # Revert back to run_agent_with_input as agent instance lacks run_batch
>       response = asyncio.run(self.mcp_agent.run_agent_with_input(json.loads(input_json.strip())))
E       AttributeError: 'MCPAgent' object has no attribute 'run_agent_with_input'

ADK/agent_data/test_pass_53/test_mcp_agent_batch_tools.py:151: AttributeError
----------------------------- Captured stderr call -----------------------------
2025-05-16 18:22:20,101 [CORE-INIT] INFO     - Initializing MCPAgent Core...
2025-05-16 18:22:20,103 [CORE-INIT] INFO     - MCPAgent Core initialized. Available tools: []
------------------------------ Captured log call -------------------------------
INFO     MCPAgentCore:mcp_agent_core.py:58 Initializing MCPAgent Core...
INFO     MCPAgentCore:mcp_agent_core.py:62 MCPAgent Core initialized. Available tools: []
_________________ TestMCPBatchToolsStdio.test_batch_successful _________________

self = <test_mcp_agent_batch_tools.TestMCPBatchToolsStdio testMethod=test_batch_successful>

    def test_batch_successful(self):
        logger.info("Starting test_batch_successful")
        batch_request = [
            {"id": "b-req-1", "tool_name": "add_numbers", "args": {"a": 10, "b": 5}}, # Mock not defined, should fail in _execute_tool
            {"id": "b-req-2", "tool_name": "save_document", "args": {"filename": "b.txt", "content": "batch"}},
            {"id": "b-req-3", "tool_name": "vectorize_document", "args": {"doc_key": "doc1"}}
        ]
        # Redefine mock outputs needed for this batch
        self.tool_outputs["add_numbers"] = {"status": "success", "result": 15}

>       output = self.run_mcp_request(batch_request)

ADK/agent_data/test_pass_53/test_mcp_agent_batch_tools.py:252:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <test_mcp_agent_batch_tools.TestMCPBatchToolsStdio testMethod=test_batch_successful>
input_data = [{'args': {'a': 10, 'b': 5}, 'id': 'b-req-1', 'tool_name': 'add_numbers'}, {'args': {'content': 'batch', 'filename': '...q-2', 'tool_name': 'save_document'}, {'args': {'doc_key': 'doc1'}, 'id': 'b-req-3', 'tool_name': 'vectorize_document'}]

    def run_mcp_request(self, input_data: Union[Dict, List]):
        """Helper to send one request (single or batch) and get the response via mocked stdio."""
        input_json = json.dumps(input_data) + "\n"
        self.mock_stdin.write(input_json)
        self.mock_stdin.seek(0) # Prepare stdin to be read

        # Clear previous stdout before processing
        self.mock_stdout.seek(0)
        self.mock_stdout.truncate(0)

        # Process the single request/batch using the existing mcp_agent instance
        # Revert back to run_agent_with_input as agent instance lacks run_batch
>       response = asyncio.run(self.mcp_agent.run_agent_with_input(json.loads(input_json.strip())))
E       AttributeError: 'MCPAgent' object has no attribute 'run_agent_with_input'

ADK/agent_data/test_pass_53/test_mcp_agent_batch_tools.py:151: AttributeError
----------------------------- Captured stderr call -----------------------------
2025-05-16 18:22:20,111 [CORE-INIT] INFO     - Initializing MCPAgent Core...
2025-05-16 18:22:20,112 [CORE-INIT] INFO     - MCPAgent Core initialized. Available tools: []
------------------------------ Captured log call -------------------------------
INFO     MCPAgentCore:mcp_agent_core.py:58 Initializing MCPAgent Core...
INFO     MCPAgentCore:mcp_agent_core.py:62 MCPAgent Core initialized. Available tools: []
__________________ TestMCPBatchToolsStdio.test_query_metadata __________________

self = <test_mcp_agent_batch_tools.TestMCPBatchToolsStdio testMethod=test_query_metadata>

    def test_query_metadata(self):
        req_id = "qmeta-1"
>       output = self.run_mcp_request({"id": req_id, "tool_name": "query_metadata", "args": {"key": "meta1"}})

ADK/agent_data/test_pass_53/test_mcp_agent_batch_tools.py:230:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <test_mcp_agent_batch_tools.TestMCPBatchToolsStdio testMethod=test_query_metadata>
input_data = {'args': {'key': 'meta1'}, 'id': 'qmeta-1', 'tool_name': 'query_metadata'}

    def run_mcp_request(self, input_data: Union[Dict, List]):
        """Helper to send one request (single or batch) and get the response via mocked stdio."""
        input_json = json.dumps(input_data) + "\n"
        self.mock_stdin.write(input_json)
        self.mock_stdin.seek(0) # Prepare stdin to be read

        # Clear previous stdout before processing
        self.mock_stdout.seek(0)
        self.mock_stdout.truncate(0)

        # Process the single request/batch using the existing mcp_agent instance
        # Revert back to run_agent_with_input as agent instance lacks run_batch
>       response = asyncio.run(self.mcp_agent.run_agent_with_input(json.loads(input_json.strip())))
E       AttributeError: 'MCPAgent' object has no attribute 'run_agent_with_input'

ADK/agent_data/test_pass_53/test_mcp_agent_batch_tools.py:151: AttributeError
----------------------------- Captured stderr call -----------------------------
2025-05-16 18:22:20,119 [CORE-INIT] INFO     - Initializing MCPAgent Core...
2025-05-16 18:22:20,120 [CORE-INIT] INFO     - MCPAgent Core initialized. Available tools: []
------------------------------ Captured log call -------------------------------
INFO     MCPAgentCore:mcp_agent_core.py:58 Initializing MCPAgent Core...
INFO     MCPAgentCore:mcp_agent_core.py:62 MCPAgent Core initialized. Available tools: []
__________________ TestMCPBatchToolsStdio.test_save_document ___________________

self = <test_mcp_agent_batch_tools.TestMCPBatchToolsStdio testMethod=test_save_document>

    def test_save_document(self):
        req_id = "svdoc-1"
>       output = self.run_mcp_request({"id": req_id, "tool_name": "save_document", "args": {"content": "test", "filename": "f.txt"}})

ADK/agent_data/test_pass_53/test_mcp_agent_batch_tools.py:205:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <test_mcp_agent_batch_tools.TestMCPBatchToolsStdio testMethod=test_save_document>
input_data = {'args': {'content': 'test', 'filename': 'f.txt'}, 'id': 'svdoc-1', 'tool_name': 'save_document'}

    def run_mcp_request(self, input_data: Union[Dict, List]):
        """Helper to send one request (single or batch) and get the response via mocked stdio."""
        input_json = json.dumps(input_data) + "\n"
        self.mock_stdin.write(input_json)
        self.mock_stdin.seek(0) # Prepare stdin to be read

        # Clear previous stdout before processing
        self.mock_stdout.seek(0)
        self.mock_stdout.truncate(0)

        # Process the single request/batch using the existing mcp_agent instance
        # Revert back to run_agent_with_input as agent instance lacks run_batch
>       response = asyncio.run(self.mcp_agent.run_agent_with_input(json.loads(input_json.strip())))
E       AttributeError: 'MCPAgent' object has no attribute 'run_agent_with_input'

ADK/agent_data/test_pass_53/test_mcp_agent_batch_tools.py:151: AttributeError
----------------------------- Captured stderr call -----------------------------
2025-05-16 18:22:20,127 [CORE-INIT] INFO     - Initializing MCPAgent Core...
2025-05-16 18:22:20,128 [CORE-INIT] INFO     - MCPAgent Core initialized. Available tools: []
------------------------------ Captured log call -------------------------------
INFO     MCPAgentCore:mcp_agent_core.py:58 Initializing MCPAgent Core...
INFO     MCPAgentCore:mcp_agent_core.py:62 MCPAgent Core initialized. Available tools: []
______________ TestMCPBatchToolsStdio.test_semantic_search_local _______________

self = <test_mcp_agent_batch_tools.TestMCPBatchToolsStdio testMethod=test_semantic_search_local>

    def test_semantic_search_local(self):
        req_id = "semsrch-1"
>       output = self.run_mcp_request({"id": req_id, "tool_name": "semantic_search_local", "args": {"query": "test"}})

ADK/agent_data/test_pass_53/test_mcp_agent_batch_tools.py:236:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <test_mcp_agent_batch_tools.TestMCPBatchToolsStdio testMethod=test_semantic_search_local>
input_data = {'args': {'query': 'test'}, 'id': 'semsrch-1', 'tool_name': 'semantic_search_local'}

    def run_mcp_request(self, input_data: Union[Dict, List]):
        """Helper to send one request (single or batch) and get the response via mocked stdio."""
        input_json = json.dumps(input_data) + "\n"
        self.mock_stdin.write(input_json)
        self.mock_stdin.seek(0) # Prepare stdin to be read

        # Clear previous stdout before processing
        self.mock_stdout.seek(0)
        self.mock_stdout.truncate(0)

        # Process the single request/batch using the existing mcp_agent instance
        # Revert back to run_agent_with_input as agent instance lacks run_batch
>       response = asyncio.run(self.mcp_agent.run_agent_with_input(json.loads(input_json.strip())))
E       AttributeError: 'MCPAgent' object has no attribute 'run_agent_with_input'

ADK/agent_data/test_pass_53/test_mcp_agent_batch_tools.py:151: AttributeError
----------------------------- Captured stderr call -----------------------------
2025-05-16 18:22:20,134 [CORE-INIT] INFO     - Initializing MCPAgent Core...
2025-05-16 18:22:20,136 [CORE-INIT] INFO     - MCPAgent Core initialized. Available tools: []
------------------------------ Captured log call -------------------------------
INFO     MCPAgentCore:mcp_agent_core.py:58 Initializing MCPAgent Core...
INFO     MCPAgentCore:mcp_agent_core.py:62 MCPAgent Core initialized. Available tools: []
_________________ TestMCPBatchToolsStdio.test_update_metadata __________________

self = <test_mcp_agent_batch_tools.TestMCPBatchToolsStdio testMethod=test_update_metadata>

    def test_update_metadata(self):
        req_id = "upmeta-1"
>       output = self.run_mcp_request({"id": req_id, "tool_name": "update_metadata", "args": {"key": "meta1", "data": {"status": "updated"}}})

ADK/agent_data/test_pass_53/test_mcp_agent_batch_tools.py:217:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <test_mcp_agent_batch_tools.TestMCPBatchToolsStdio testMethod=test_update_metadata>
input_data = {'args': {'data': {'status': 'updated'}, 'key': 'meta1'}, 'id': 'upmeta-1', 'tool_name': 'update_metadata'}

    def run_mcp_request(self, input_data: Union[Dict, List]):
        """Helper to send one request (single or batch) and get the response via mocked stdio."""
        input_json = json.dumps(input_data) + "\n"
        self.mock_stdin.write(input_json)
        self.mock_stdin.seek(0) # Prepare stdin to be read

        # Clear previous stdout before processing
        self.mock_stdout.seek(0)
        self.mock_stdout.truncate(0)

        # Process the single request/batch using the existing mcp_agent instance
        # Revert back to run_agent_with_input as agent instance lacks run_batch
>       response = asyncio.run(self.mcp_agent.run_agent_with_input(json.loads(input_json.strip())))
E       AttributeError: 'MCPAgent' object has no attribute 'run_agent_with_input'

ADK/agent_data/test_pass_53/test_mcp_agent_batch_tools.py:151: AttributeError
----------------------------- Captured stderr call -----------------------------
2025-05-16 18:22:20,142 [CORE-INIT] INFO     - Initializing MCPAgent Core...
2025-05-16 18:22:20,143 [CORE-INIT] INFO     - MCPAgent Core initialized. Available tools: []
------------------------------ Captured log call -------------------------------
INFO     MCPAgentCore:mcp_agent_core.py:58 Initializing MCPAgent Core...
INFO     MCPAgentCore:mcp_agent_core.py:62 MCPAgent Core initialized. Available tools: []
_______________ TestMCPBatchToolsStdio.test_update_metadata_fail _______________

self = <test_mcp_agent_batch_tools.TestMCPBatchToolsStdio testMethod=test_update_metadata_fail>

    def test_update_metadata_fail(self):
        req_id = "upmeta-fail-1"
>       output = self.run_mcp_request({"id": req_id, "tool_name": "update_metadata", "args": {"key": "fail_key"}})

ADK/agent_data/test_pass_53/test_mcp_agent_batch_tools.py:223:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <test_mcp_agent_batch_tools.TestMCPBatchToolsStdio testMethod=test_update_metadata_fail>
input_data = {'args': {'key': 'fail_key'}, 'id': 'upmeta-fail-1', 'tool_name': 'update_metadata'}

    def run_mcp_request(self, input_data: Union[Dict, List]):
        """Helper to send one request (single or batch) and get the response via mocked stdio."""
        input_json = json.dumps(input_data) + "\n"
        self.mock_stdin.write(input_json)
        self.mock_stdin.seek(0) # Prepare stdin to be read

        # Clear previous stdout before processing
        self.mock_stdout.seek(0)
        self.mock_stdout.truncate(0)

        # Process the single request/batch using the existing mcp_agent instance
        # Revert back to run_agent_with_input as agent instance lacks run_batch
>       response = asyncio.run(self.mcp_agent.run_agent_with_input(json.loads(input_json.strip())))
E       AttributeError: 'MCPAgent' object has no attribute 'run_agent_with_input'

ADK/agent_data/test_pass_53/test_mcp_agent_batch_tools.py:151: AttributeError
----------------------------- Captured stderr call -----------------------------
2025-05-16 18:22:20,150 [CORE-INIT] INFO     - Initializing MCPAgent Core...
2025-05-16 18:22:20,151 [CORE-INIT] INFO     - MCPAgent Core initialized. Available tools: []
------------------------------ Captured log call -------------------------------
INFO     MCPAgentCore:mcp_agent_core.py:58 Initializing MCPAgent Core...
INFO     MCPAgentCore:mcp_agent_core.py:62 MCPAgent Core initialized. Available tools: []
________________ TestMCPBatchToolsStdio.test_vectorize_document ________________

self = <test_mcp_agent_batch_tools.TestMCPBatchToolsStdio testMethod=test_vectorize_document>

    def test_vectorize_document(self):
        req_id = "vecdoc-1"
>       output = self.run_mcp_request({"id": req_id, "tool_name": "vectorize_document", "args": {"doc_key": "doc1"}})

ADK/agent_data/test_pass_53/test_mcp_agent_batch_tools.py:211:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <test_mcp_agent_batch_tools.TestMCPBatchToolsStdio testMethod=test_vectorize_document>
input_data = {'args': {'doc_key': 'doc1'}, 'id': 'vecdoc-1', 'tool_name': 'vectorize_document'}

    def run_mcp_request(self, input_data: Union[Dict, List]):
        """Helper to send one request (single or batch) and get the response via mocked stdio."""
        input_json = json.dumps(input_data) + "\n"
        self.mock_stdin.write(input_json)
        self.mock_stdin.seek(0) # Prepare stdin to be read

        # Clear previous stdout before processing
        self.mock_stdout.seek(0)
        self.mock_stdout.truncate(0)

        # Process the single request/batch using the existing mcp_agent instance
        # Revert back to run_agent_with_input as agent instance lacks run_batch
>       response = asyncio.run(self.mcp_agent.run_agent_with_input(json.loads(input_json.strip())))
E       AttributeError: 'MCPAgent' object has no attribute 'run_agent_with_input'

ADK/agent_data/test_pass_53/test_mcp_agent_batch_tools.py:151: AttributeError
----------------------------- Captured stderr call -----------------------------
2025-05-16 18:22:20,158 [CORE-INIT] INFO     - Initializing MCPAgent Core...
2025-05-16 18:22:20,159 [CORE-INIT] INFO     - MCPAgent Core initialized. Available tools: []
------------------------------ Captured log call -------------------------------
INFO     MCPAgentCore:mcp_agent_core.py:58 Initializing MCPAgent Core...
INFO     MCPAgentCore:mcp_agent_core.py:62 MCPAgent Core initialized. Available tools: []
______ TestMCPStdioGetRegisteredTools.test_get_registered_tools_mcp_stdio ______

self = <test_mcp_registered_tools.TestMCPStdioGetRegisteredTools testMethod=test_get_registered_tools_mcp_stdio>

    def test_get_registered_tools_mcp_stdio(self):
        """Test the get_registered_tools tool via MCP stdio server."""
        logger.info("Starting test_get_registered_tools_mcp_stdio")

        # Add a small delay to ensure the server is fully ready after setup
        time.sleep(0.5)

        # Check if the server started correctly in setUp
        if not self.process or self.process.poll() is not None:
             self.fail("MCP server process did not start correctly in setUp.")

        # Assuming get_registered_tools is available and takes no input args
        # NOTE: The current local_mcp_server.py has a placeholder dispatch.
        # It might not actually call get_registered_tools yet.
        # This test assumes the server WILL eventually support it.
        request = {
            "tool": "get_registered_tools",
            "input": {},
            "meta": {"request_id": "get-tools-test-1"}
        }

        try:
            self.send_request(request)
            response = self.receive_response()
        except Exception as e:
            # Log stderr before failing
            stderr_output = "".join(iter(self.stderr_queue.get_nowait, None))
            logger.error(f"Exception during test execution. Stderr:\n{stderr_output}")
            self.fail(f"Test execution failed: {e}")

        self.assertIsInstance(response, dict)
        self.assertEqual(response.get("meta", {}).get("request_id"), "get-tools-test-1")

        # Check for success or potential 'Unknown tool' error depending on server implementation
        status = response.get("meta", {}).get("status")
        error = response.get("error")
        result = response.get("result")

        if status == "success":
            self.assertIsInstance(result, list, "Result should be the list returned by the tool.")
            self.assertIn("echo", result) # Check for a basic tool
            self.assertIn("get_registered_tools", result) # Should list itself
            self.assertIsNone(error)
            logger.info(f"Successfully received tool list: {result}")
        elif error and "Unknown tool" in error:
            logger.warning(f"Received 'Unknown tool' error as expected for placeholder server: {error}")
            # This might be the expected outcome until local_mcp_server fully implements tool dispatch
            self.assertEqual(status, "error")
            self.assertIsNone(result)
        else:
>           self.fail(f"Unexpected response status='{status}', error='{error}', result='{result}'")
E           AssertionError: Unexpected response status='error', error='Error in tool 'get_registered_tools': TypeError: get_registered_tools() missing 1 required positional argument: 'agent_context'', result='None'

ADK/agent_data/test_pass_53/test_mcp_registered_tools.py:191: AssertionError
------------------------------ Captured log call -------------------------------
WARNING  test_mcp_registered_tools:test_mcp_registered_tools.py:114 MCP Server Stderr:
2025-05-16 18:22:25,040 - INFO - sys.path in Cloud Run at top of register_tools.py: ['/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data', '/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/mcp', '/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents', '/opt/homebrew/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python310.zip', '/opt/homebrew/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python3.10', '/opt/homebrew/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python3.10/lib-dynload', '/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/setup/venv/lib/python3.10/site-packages']
2025-05-16 18:22:25,118 - INFO - Loading faiss.
2025-05-16 18:22:25,135 - INFO - Successfully loaded faiss.
2025-05-16 18:22:25,140 - INFO - Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.
2025-05-16 18:22:25,985 - INFO - OpenAI import successful.
2025-05-16 18:22:25,985 - INFO - FAISS import successful.
2025-05-16 18:22:25,985 - WARNING - OPENAI_API_KEY environment variable not set. Tools requiring OpenAI embeddings will not be available.
2025-05-16 18:22:26,011 - INFO - sys.path in Cloud Run at top of register_tools.py: ['/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data', '/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/mcp', '/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents', '/opt/homebrew/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python310.zip', '/opt/homebrew/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python3.10', '/opt/homebrew/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python3.10/lib-dynload', '/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/setup/venv/lib/python3.10/site-packages']
2025-05-16 18:22:26,014 - INFO - OpenAI import successful.
2025-05-16 18:22:26,014 - INFO - FAISS import successful.
2025-05-16 18:22:26,014 - WARNING - OPENAI_API_KEY environment variable not set. Tools requiring OpenAI embeddings will not be available.
2025-05-16 18:22:26,016 - INFO - Checking dependencies inside get_all_tool_functions: FAISS_AVAILABLE=True, OPENAI_AVAILABLE=True
2025-05-16 18:22:26,016 - INFO - Base local tools collected: ['save_text', 'add_numbers', 'multiply_numbers', 'echo', 'get_registered_tools', 'delay', 'save_document', 'vectorize_document', 'update_metadata', 'query_metadata', 'multi_update_metadata', 'bulk_delete_metadata', 'bulk_update_metadata', 'multi_field_update', 'semantic_search_local', 'find_metadata_by_key', 'semantic_search_metadata', 'semantic_search_by_author', 'semantic_search_by_year', 'semantic_search_by_keyword', 'conditional_search_metadata', 'semantic_search_multiple_fields', 'sort_metadata', 'advanced_semantic_search', 'create_metadata_tree', 'view_metadata_tree', 'delete_metadata_node', 'update_metadata_node', 'depth_first_search', 'rebuild_metadata_tree', 'semantic_search_metadata_tree', 'validate_metadata_tree', 'generate_embedding', 'semantic_similarity_search', 'batch_generate_embeddings', 'semantic_expand_metadata', 'semantic_filter_metadata', 'analyze_metadata_trends', 'aggregate_metadata', 'detect_anomalies', 'metadata_statistics', 'error_tool']
2025-05-16 18:22:26,016 - INFO - FAISS_AVAILABLE is True, attempting to add FAISS tools...
2025-05-16 18:22:26,016 - INFO - FAISS tools added. Current tools: ['save_text', 'add_numbers', 'multiply_numbers', 'echo', 'get_registered_tools', 'delay', 'save_document', 'vectorize_document', 'update_metadata', 'query_metadata', 'multi_update_metadata', 'bulk_delete_metadata', 'bulk_update_metadata', 'multi_field_update', 'semantic_search_local', 'find_metadata_by_key', 'semantic_search_metadata', 'semantic_search_by_author', 'semantic_search_by_year', 'semantic_search_by_keyword', 'conditional_search_metadata', 'semantic_search_multiple_fields', 'sort_metadata', 'advanced_semantic_search', 'create_metadata_tree', 'view_metadata_tree', 'delete_metadata_node', 'update_metadata_node', 'depth_first_search', 'rebuild_metadata_tree', 'semantic_search_metadata_tree', 'validate_metadata_tree', 'generate_embedding', 'semantic_similarity_search', 'batch_generate_embeddings', 'semantic_expand_metadata', 'semantic_filter_metadata', 'analyze_metadata_trends', 'aggregate_metadata', 'detect_anomalies', 'metadata_statistics', 'error_tool', 'save_metadata_to_faiss', 'load_metadata_from_faiss', 'query_metadata_faiss', 'advanced_query_faiss', 'rebuild_metadata_tree_from_faiss']
2025-05-16 18:22:26,016 - INFO - OPENAI_AVAILABLE is True, attempting to add OpenAI tools...
2025-05-16 18:22:26,016 - INFO - OpenAI tools added: ['generate_embedding_real', 'semantic_search_cosine', 'clear_embeddings']
2025-05-16 18:22:26,016 - INFO - Discovered tools: ['save_text', 'add_numbers', 'multiply_numbers', 'echo', 'get_registered_tools', 'delay', 'save_document', 'vectorize_document', 'update_metadata', 'query_metadata', 'multi_update_metadata', 'bulk_delete_metadata', 'bulk_update_metadata', 'multi_field_update', 'semantic_search_local', 'find_metadata_by_key', 'semantic_search_metadata', 'semantic_search_by_author', 'semantic_search_by_year', 'semantic_search_by_keyword', 'conditional_search_metadata', 'semantic_search_multiple_fields', 'sort_metadata', 'advanced_semantic_search', 'create_metadata_tree', 'view_metadata_tree', 'delete_metadata_node', 'update_metadata_node', 'depth_first_search', 'rebuild_metadata_tree', 'semantic_search_metadata_tree', 'validate_metadata_tree', 'generate_embedding', 'semantic_similarity_search', 'batch_generate_embeddings', 'semantic_expand_metadata', 'semantic_filter_metadata', 'analyze_metadata_trends', 'aggregate_metadata', 'detect_anomalies', 'metadata_statistics', 'error_tool', 'save_metadata_to_faiss', 'load_metadata_from_faiss', 'query_metadata_faiss', 'advanced_query_faiss', 'rebuild_metadata_tree_from_faiss', 'generate_embedding_real', 'semantic_search_cosine', 'clear_embeddings']
2025-05-16 18:22:26,016 - INFO - Starting MCP Stdio Server...
2025-05-16 18:22:26,016 - INFO - MCP Stdio Server Started. Waiting for JSON input on stdin...
2025-05-16 18:22:26,524 - INFO - Executing tool 'get_registered_tools' (ID: get-tools-test-1) with input: {}
2025-05-16 18:22:26,524 - ERROR - Error executing tool 'get_registered_tools' (ID: get-tools-test-1): get_registered_tools() missing 1 required positional argument: 'agent_context'
Traceback (most recent call last):
  File "/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/mcp/local_mcp_server.py", line 94, in run_mcp_loop
    tool_result_data = tool_function() # Call without arguments
TypeError: get_registered_tools() missing 1 required positional argument: 'agent_context'
2025-05-16 18:22:26,525 - ERROR - Error executing tool 'get_registered_tools' (ID: get-tools-test-1): get_registered_tools() missing 1 required positional argument: 'agent_context'
Traceback (most recent call last):
  File "/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/mcp/local_mcp_server.py", line 120, in run_mcp_loop
    raise exec_err
  File "/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/mcp/local_mcp_server.py", line 94, in run_mcp_loop
    tool_result_data = tool_function() # Call without arguments
TypeError: get_registered_tools() missing 1 required positional argument: 'agent_context'
2025-05-16 18:22:26,526 - INFO - Exit command received (ID: None). Shutting down.
2025-05-16 18:22:26,527 - INFO - MCP Stdio Server loop finished.
2025-05-16 18:22:26,527 - INFO - MCP Stdio Server Stopped.
__________________________ test_mcp_timeout_handling ___________________________

mcp_agent_instance = <ADK.agent_data.mcp.mcp_agent_core.MCPAgent object at 0x163f2eb30>

    @pytest.mark.asyncio
    async def test_mcp_timeout_handling(mcp_agent_instance: MCPAgent):
        """
        Tests MCPAgent's handling of timeouts in batch processing.
        - 20 echo requests (should succeed)
        - 10 delay requests with 1000ms (should succeed)
        - 10 delay requests with 6000ms (should timeout)
        - 10 invalid tool requests (should fail)
        """
        test_status_logger.info("Starting test_mcp_timeout_handling...")
        agent = mcp_agent_instance
        # Access timeout constant directly from the module if it's defined there,
        # or via the agent instance if it holds it. Let's assume it's in the core module based on previous reading.
        try:
            from ADK.agent_data.mcp.mcp_agent_core import TOOL_EXECUTION_TIMEOUT_SECONDS
            timeout_duration_seconds = TOOL_EXECUTION_TIMEOUT_SECONDS
        except ImportError:
            # Fallback or fail if not found - assuming 5s as per requirement
            test_status_logger.warning("Could not import TOOL_EXECUTION_TIMEOUT_SECONDS, assuming 5.0s")
            timeout_duration_seconds = 5.0

        requests = []
        # 1. Successful echo requests
        for i in range(20):
            requests.append({
                "id": f"echo-{i+1}",
                "tool_name": "echo",
                "args": ["test string"]
            })

        # 2. Successful delay requests (< timeout)
        for i in range(10):
            requests.append({
                "id": f"delay-ok-{i+1}",
                "tool_name": "delay",
                "kwargs": {"delay_ms": 1000} # 1 second, less than timeout
            })

        # 3. Timeout delay requests (> timeout)
        delay_timeout_ms = 6000 # 6 seconds
        for i in range(10):
            requests.append({
                "id": f"delay-timeout-{i+1}",
                "tool_name": "delay",
                "kwargs": {"delay_ms": delay_timeout_ms}
            })

        # 4. Invalid tool requests
        for i in range(10):
            requests.append({
                "id": f"invalid-{i+1}",
                "tool_name": "nonexistent_tool",
                "args": []
            })

        assert len(requests) == 50, "Incorrect number of requests generated."

        test_status_logger.info(f"Executing batch of {len(requests)} requests (timeout={timeout_duration_seconds}s)...")
        start_time = time.monotonic()
        # Assuming run_agent_with_input handles batch lists
        # Revert back to run_agent_with_input based on latest error
>       results = await agent.run_agent_with_input(requests)
E       AttributeError: 'MCPAgent' object has no attribute 'run_agent_with_input'

ADK/agent_data/test_pass_53/test_mcp_timeout.py:172: AttributeError
---------------------------- Captured stderr setup -----------------------------
2025-05-16 18:22:26,718 - PytestTimeoutStatus - INFO - Setting up MCPAgent fixture. Redirecting MCPAgentCore logs to /Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/test_pass_53/logs/pytest_timeout.log
2025-05-16 18:22:26,719 - PytestTimeoutStatus - INFO - MCPAgent instance created for timeout test.
------------------------------ Captured log setup ------------------------------
INFO     PytestTimeoutStatus:test_mcp_timeout.py:72 Setting up MCPAgent fixture. Redirecting MCPAgentCore logs to /Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/test_pass_53/logs/pytest_timeout.log
INFO     MCPAgentCore:mcp_agent_core.py:58 Initializing MCPAgent Core...
INFO     MCPAgentCore:mcp_agent_core.py:62 MCPAgent Core initialized. Available tools: ['save_text', 'add_numbers', 'multiply_numbers', 'echo', 'get_registered_tools', 'delay', 'save_document', 'vectorize_document', 'update_metadata', 'query_metadata', 'multi_update_metadata', 'bulk_delete_metadata', 'bulk_update_metadata', 'multi_field_update', 'semantic_search_local', 'find_metadata_by_key', 'semantic_search_metadata', 'semantic_search_by_author', 'semantic_search_by_year', 'semantic_search_by_keyword', 'conditional_search_metadata', 'semantic_search_multiple_fields', 'sort_metadata', 'advanced_semantic_search', 'create_metadata_tree', 'view_metadata_tree', 'delete_metadata_node', 'update_metadata_node', 'depth_first_search', 'rebuild_metadata_tree', 'semantic_search_metadata_tree', 'validate_metadata_tree', 'generate_embedding', 'semantic_similarity_search', 'batch_generate_embeddings', 'semantic_expand_metadata', 'semantic_filter_metadata', 'analyze_metadata_trends', 'aggregate_metadata', 'detect_anomalies', 'metadata_statistics', 'error_tool', 'save_metadata_to_faiss', 'load_metadata_from_faiss', 'query_metadata_faiss', 'advanced_query_faiss', 'rebuild_metadata_tree_from_faiss', 'generate_embedding_real', 'semantic_search_cosine', 'clear_embeddings']
INFO     PytestTimeoutStatus:test_mcp_timeout.py:93 MCPAgent instance created for timeout test.
----------------------------- Captured stderr call -----------------------------
2025-05-16 18:22:26,720 - PytestTimeoutStatus - INFO - Starting test_mcp_timeout_handling...
2025-05-16 18:22:26,720 - PytestTimeoutStatus - INFO - Executing batch of 50 requests (timeout=10.0s)...
------------------------------ Captured log call -------------------------------
INFO     PytestTimeoutStatus:test_mcp_timeout.py:120 Starting test_mcp_timeout_handling...
INFO     PytestTimeoutStatus:test_mcp_timeout.py:168 Executing batch of 50 requests (timeout=10.0s)...
_________________________ test_log_output_verification _________________________

    @pytest.mark.asyncio
    async def test_log_output_verification():
        """Verifies the content of the generated JSON log file."""
        test_status_logger.info("Verifying log output...")
        log_entries = read_log_file(PYTEST_LOG_FILE)

        assert len(log_entries) > 0, "Log file is empty or could not be read."

        # Example verification: Count log entries for each status
        log_statuses = {
            "success": 0,
            "timeout": 0,
            "failed": 0,
            "error": 0, # If 'error' status is used by the logger
        }
        request_ids_in_log = set()

        for entry in log_entries:
            # Check if the entry contains the keys we expect from our logging format
            assert "request_id" in entry, f"Log entry missing 'request_id': {entry}"
            assert "batch_id" in entry, f"Log entry missing 'batch_id': {entry}"
            req_id = entry["request_id"]
            # Only analyze status for actual request logs, not init/core/batch status messages
            is_request_result_log = req_id not in ['N/A', 'CORE-INIT', 'CORE-MSG'] and "message" not in entry and "tool_name" in entry

            if is_request_result_log:
                assert "status" in entry, f"Log entry for request {req_id} missing 'status': {entry}"
                status = entry.get("status") # Get status safely
                if status in log_statuses:
                    log_statuses[status] += 1
                else:
                     test_status_logger.warning(f"Unknown status '{status}' found in log entry for request {req_id}")
                request_ids_in_log.add(req_id)

        # Expected counts based on the previous test (50 requests processed)
        # Note: Logging might occur multiple times per request (start, end), so counts might differ
        # from the result counts. Let's focus on checking if expected IDs are present with final status.

        # Check if all 50 unique request IDs were logged at least once
        # We generated IDs like echo-1, delay-ok-1, delay-timeout-1, invalid-1
        expected_id_prefixes = ["echo-", "delay-ok-", "delay-timeout-", "invalid-"]
        all_ids_found = True
        for i in range(1, 21):
            if f"echo-{i}" not in request_ids_in_log:
                all_ids_found = False; break
        for i in range(1, 11):
            if f"delay-ok-{i}" not in request_ids_in_log: all_ids_found = False; break
            if f"delay-timeout-{i}" not in request_ids_in_log: all_ids_found = False; break
            if f"invalid-{i}" not in request_ids_in_log: all_ids_found = False; break

>       assert all_ids_found, "Not all expected request IDs were found in the logs."
E       AssertionError: Not all expected request IDs were found in the logs.
E       assert False

ADK/agent_data/test_pass_53/test_mcp_timeout.py:387: AssertionError
----------------------------- Captured stderr call -----------------------------
2025-05-16 18:22:26,727 - PytestTimeoutStatus - INFO - Verifying log output...
------------------------------ Captured log call -------------------------------
INFO     PytestTimeoutStatus:test_mcp_timeout.py:340 Verifying log output...
--------------------------- Captured stderr teardown ---------------------------
2025-05-16 18:22:26,731 - PytestTimeoutStatus - INFO - Cleaning up MCPAgent fixture.
2025-05-16 18:22:26,731 - PytestTimeoutStatus - INFO - MCPAgentCore logger restored.
---------------------------- Captured log teardown -----------------------------
INFO     PytestTimeoutStatus:test_mcp_timeout.py:97 Cleaning up MCPAgent fixture.
INFO     PytestTimeoutStatus:test_mcp_timeout.py:107 MCPAgentCore logger restored.
______ TestQueryMetadataFaiss.test_query_firestore_missing_gcs_faiss_path ______

self = <MagicMock name='Client' id='4380804000'>, args = ()
kwargs = {'database': 'test-db', 'project': 'test-project'}
expected = call(project='test-project', database='test-db')
actual = call(project='chatgpt-db-project', database='test-default')
_error_message = <function NonCallableMock.assert_called_with.<locals>._error_message at 0x163b56440>
cause = None

    def assert_called_with(self, /, *args, **kwargs):
        """assert that the last call was made with the specified arguments.

        Raises an AssertionError if the args and keyword args passed in are
        different to the last call to the mock."""
        if self.call_args is None:
            expected = self._format_mock_call_signature(args, kwargs)
            actual = 'not called.'
            error_message = ('expected call not found.\nExpected: %s\nActual: %s'
                    % (expected, actual))
            raise AssertionError(error_message)

        def _error_message():
            msg = self._format_mock_failure_message(args, kwargs)
            return msg
        expected = self._call_matcher(_Call((args, kwargs), two=True))
        actual = self._call_matcher(self.call_args)
        if actual != expected:
            cause = expected if isinstance(expected, Exception) else None
>           raise AssertionError(_error_message()) from cause
E           AssertionError: expected call not found.
E           Expected: Client(project='test-project', database='test-db')
E           Actual: Client(project='chatgpt-db-project', database='test-default')

/opt/homebrew/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python3.10/unittest/mock.py:929: AssertionError

During handling of the above exception, another exception occurred:

self = <MagicMock name='Client' id='4380804000'>, args = ()
kwargs = {'database': 'test-db', 'project': 'test-project'}

    def assert_called_once_with(self, /, *args, **kwargs):
        """assert that the mock was called exactly once and that that call was
        with the specified arguments."""
        if not self.call_count == 1:
            msg = ("Expected '%s' to be called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
            raise AssertionError(msg)
>       return self.assert_called_with(*args, **kwargs)
E       AssertionError: expected call not found.
E       Expected: Client(project='test-project', database='test-db')
E       Actual: Client(project='chatgpt-db-project', database='test-default')
E
E       pytest introspection follows:
E
E       Kwargs:
E       assert {'database': ...t-db-project'} == {'database': ...test-project'}
E
E         Differing items:
E         {'database': 'test-default'} != {'database': 'test-db'}
E         {'project': 'chatgpt-db-project'} != {'project': 'test-project'}
E
E         Full diff:
E           {...
E
E         ...Full output truncated (9 lines hidden), use '-vv' to show

/opt/homebrew/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python3.10/unittest/mock.py:941: AssertionError

During handling of the above exception, another exception occurred:

self = <test_query_metadata_from_faiss.TestQueryMetadataFaiss object at 0x16382d7b0>
MockStorageClient = <MagicMock name='Client' id='5967647632'>
mock_internal_download_gcs = <MagicMock name='_download_gcs_file' id='5967653296'>
mock_firestore_constructor = <MagicMock name='Client' id='4380804000'>
mock_get_embedding = <AsyncMock name='get_openai_embedding' id='4378588304'>
mocker = <pytest_mock.plugin.MockerFixture object at 0x163e20070>
request = <FixtureRequest for <Coroutine test_query_firestore_missing_gcs_faiss_path>>

    @patch(f"{QUERY_TOOL_MODULE_PATH}.get_openai_embedding", new_callable=AsyncMock)
    @patch(FIRESTORE_CLIENT_PATH)
    @patch(DOWNLOAD_GCS_QUERY_PATH)
    @patch(STORAGE_CLIENT_QUERY_PATH)
    async def test_query_firestore_missing_gcs_faiss_path(self, MockStorageClient,
                                                          mock_internal_download_gcs,
                                                          mock_firestore_constructor,
                                                          mock_get_embedding,
                                                          mocker, request):
        # Patch pickle.load and open using mocker inside the test
        mock_pickle_load = mocker.patch(PICKLE_LOAD_QUERY_PATH)
        mock_builtin_open = mocker.patch(f"{QUERY_TOOL_MODULE_PATH}.open")

        from ADK.agent_data.tools.query_metadata_faiss_tool import query_metadata_faiss, FirestoreCommunicationError
        index_name = "test_missing_gcs_path_query"
        mock_agent_context = None
        mock_query_embedding_vector = np.array([0.1]*5, dtype=np.float32)
        mock_get_embedding.return_value = {"embedding": mock_query_embedding_vector, "status": "success", "total_tokens": 3}

        # Mock Firestore Client and Document Reference setup
        mock_fs_instance = MagicMock()
        mock_firestore_constructor.return_value = mock_fs_instance
        mock_collection_ref = MagicMock()
        mock_doc_ref = MagicMock()
        mock_fs_instance.collection.return_value = mock_collection_ref
        mock_collection_ref.document.return_value = mock_doc_ref

        mock_doc_snapshot = MagicMock()
        mock_doc_snapshot.exists = True
        # Simulate Firestore doc missing the 'gcs_faiss_path' key
        doc_data_from_firestore = {
            "vectorStatus": "completed",
            # "gcs_faiss_path": "gs://some/path/file.faiss", # Missing this line
            "gcs_meta_path": f"gs://{MOCKED_ENV_VARS['GCS_BUCKET_NAME']}/{index_name}.meta",
            "dimension": 5,
        }

        # Use side_effect for to_dict to accurately simulate missing key behavior
        def snapshot_get_side_effect(key, default=None):
            return doc_data_from_firestore.get(key, default)

        mock_doc_snapshot.to_dict.return_value = doc_data_from_firestore
        # mock_doc_snapshot.get = MagicMock(side_effect=snapshot_get_side_effect)
        mock_doc_ref.get.return_value = mock_doc_snapshot

        # Mock Storage Client setup (needed for _download_gcs_file check)
        mock_storage_instance = MockStorageClient.return_value
        # REMOVED ASSERTION BELOW
        # MockStorageClient.assert_called_once_with(project=MOCKED_ENV_VARS["FIRESTORE_PROJECT_ID"])
        # mock_storage_instance.bucket.assert_called_once_with(MOCKED_ENV_VARS["GCS_BUCKET_NAME"]) # Also likely not needed

        result = await query_metadata_faiss(
            agent_context=mock_agent_context,
            index_name=index_name,
            key="query for missing path test",
            top_k=1
        )

        print(f"\nResult for {request.node.name}: {result}\n")
        assert result is not None
        assert result.get("meta", {}).get("status") == "error"
        assert result["error"] == "Missing GCS FAISS path"

        # Verify mocks
>       mock_firestore_constructor.assert_called_once_with(project=MOCKED_ENV_VARS["FIRESTORE_PROJECT_ID"], database=MOCKED_ENV_VARS["FIRESTORE_DATABASE_ID"])
E       AssertionError: expected call not found.
E       Expected: Client(project='test-project', database='test-db')
E       Actual: Client(project='chatgpt-db-project', database='test-default')
E
E       pytest introspection follows:
E
E       Kwargs:
E       assert {'database': ...t-db-project'} == {'database': ...test-project'}
E
E         Differing items:
E         {'database': 'test-default'} != {'database': 'test-db'}
E         {'project': 'chatgpt-db-project'} != {'project': 'test-project'}
E
E         Full diff:
E           {...
E
E         ...Full output truncated (9 lines hidden), use '-vv' to show

ADK/agent_data/test_pass_53/test_query_metadata_from_faiss.py:440: AssertionError
----------------------------- Captured stdout call -----------------------------

Result for test_query_firestore_missing_gcs_faiss_path: {'meta': {'status': 'error', 'error_type': 'FaissReadError', 'message': 'Missing GCS FAISS path'}, 'error': 'Missing GCS FAISS path'}

_______________ TestMCPStdioAllTools.test_core_tools_and_errors ________________

self = <ADK.agent_data.tests.test_mcp_agent_all_tools.TestMCPStdioAllTools testMethod=test_core_tools_and_errors>

    def test_core_tools_and_errors(self):
        """Tests multiple basic tools and error conditions using the shared server process."""
        logger.info("Starting test_core_tools_and_errors")

        test_cases = [
            # 1. Add Numbers
            {
                "id": "req-add-1", "request": {"tool": "add_numbers", "input": {"a": 10, "b": 5}},
                "expected_status": "success", "expected_result": 15
            },
            # 2. Multiply Numbers
            {
                "id": "req-mul-2", "request": {"tool": "multiply_numbers", "input": {"a": 7, "b": 3}},
                "expected_status": "success", "expected_result": 21
            },
            # 3. Save Text (Check for success status, ignore specific path)
            {
                "id": "req-sav-3", "request": {"tool": "save_text", "input": {"filename": "test_all_tools.txt", "content": "Testing save..."}},
                "expected_status": "success",
                "expected_result_contains_string": "Saved text: test_all_tools.txt"
            },
             # 4. Nonexistent Tool
            {
                "id": "req-err-4", "request": {"tool": "absolutely_fake_tool", "input": {}},
                "expected_status": "error", "expected_error_contains": "Unknown tool: absolutely_fake_tool"
            },
            # 5. Missing input for a tool that requires it (e.g., add_numbers)
            #    (Behavior depends on tool implementation - might raise TypeError or handle default)
            #    Let's assume add_numbers raises an error if args are missing/wrong type
            {
                "id": "req-err-5", "request": {"tool": "add_numbers", "input": {"a": 5}}, # Missing 'b'
                "expected_status": "error", "expected_error_contains": "missing 1 required positional argument: 'b'" # Or similar TypeError
            },
             # 6. Get Registered Tools (Assuming placeholder server implements this)
             # This test might need adjustment based on local_mcp_server.py implementation
            {
                "id": "req-get-6", "request": {"tool": "get_registered_tools", "input": {}},
                "expected_status": "success", # Or "error" if not implemented
                "expected_result_is_list": True,
                "expected_result_has_tool": "echo" # Check if a known tool is listed
            },
        ]

        for i, case in enumerate(test_cases):
            # Add meta request_id to the request
            case["request"]["meta"] = {"request_id": case["id"]}

            with self.subTest(f"Test Case {i+1}: {case['request'].get('tool', 'Malformed')}"):
                logger.debug(f"--- Running Subtest: {case['request']['tool']} ---")
                self.send_request(case["request"])
                response = self.receive_response()

                self.assertIsNotNone(response, "Did not receive response from server.")
                self.assertEqual(response.get('meta', {}).get('request_id'), case["id"])
>               self.assertEqual(response.get('meta', {}).get('status'), case["expected_status"])
E               AssertionError: 'error' != 'success'
E               - error
E               + success

ADK/agent_data/tests/test_mcp_agent_all_tools.py:190: AssertionError
---------------------------- Captured log teardown -----------------------------
WARNING  ADK.agent_data.tests.test_mcp_agent_all_tools:test_mcp_agent_all_tools.py:109 Shared MCP Server Stderr:
2025-05-16 18:22:26,921 - INFO - sys.path in Cloud Run at top of register_tools.py: ['/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data', '/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/mcp', '/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents', '/opt/homebrew/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python310.zip', '/opt/homebrew/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python3.10', '/opt/homebrew/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python3.10/lib-dynload', '/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/setup/venv/lib/python3.10/site-packages']
2025-05-16 18:22:26,988 - INFO - Loading faiss.
2025-05-16 18:22:27,004 - INFO - Successfully loaded faiss.
2025-05-16 18:22:27,009 - INFO - Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.
2025-05-16 18:22:27,946 - INFO - OpenAI import successful.
2025-05-16 18:22:27,946 - INFO - FAISS import successful.
2025-05-16 18:22:27,946 - WARNING - OPENAI_API_KEY environment variable not set. Tools requiring OpenAI embeddings will not be available.
2025-05-16 18:22:27,974 - INFO - sys.path in Cloud Run at top of register_tools.py: ['/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data', '/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/mcp', '/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents', '/opt/homebrew/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python310.zip', '/opt/homebrew/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python3.10', '/opt/homebrew/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python3.10/lib-dynload', '/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/setup/venv/lib/python3.10/site-packages']
2025-05-16 18:22:27,977 - INFO - OpenAI import successful.
2025-05-16 18:22:27,977 - INFO - FAISS import successful.
2025-05-16 18:22:27,977 - WARNING - OPENAI_API_KEY environment variable not set. Tools requiring OpenAI embeddings will not be available.
2025-05-16 18:22:27,978 - INFO - Checking dependencies inside get_all_tool_functions: FAISS_AVAILABLE=True, OPENAI_AVAILABLE=True
2025-05-16 18:22:27,978 - INFO - Base local tools collected: ['save_text', 'add_numbers', 'multiply_numbers', 'echo', 'get_registered_tools', 'delay', 'save_document', 'vectorize_document', 'update_metadata', 'query_metadata', 'multi_update_metadata', 'bulk_delete_metadata', 'bulk_update_metadata', 'multi_field_update', 'semantic_search_local', 'find_metadata_by_key', 'semantic_search_metadata', 'semantic_search_by_author', 'semantic_search_by_year', 'semantic_search_by_keyword', 'conditional_search_metadata', 'semantic_search_multiple_fields', 'sort_metadata', 'advanced_semantic_search', 'create_metadata_tree', 'view_metadata_tree', 'delete_metadata_node', 'update_metadata_node', 'depth_first_search', 'rebuild_metadata_tree', 'semantic_search_metadata_tree', 'validate_metadata_tree', 'generate_embedding', 'semantic_similarity_search', 'batch_generate_embeddings', 'semantic_expand_metadata', 'semantic_filter_metadata', 'analyze_metadata_trends', 'aggregate_metadata', 'detect_anomalies', 'metadata_statistics', 'error_tool']
2025-05-16 18:22:27,978 - INFO - FAISS_AVAILABLE is True, attempting to add FAISS tools...
2025-05-16 18:22:27,978 - INFO - FAISS tools added. Current tools: ['save_text', 'add_numbers', 'multiply_numbers', 'echo', 'get_registered_tools', 'delay', 'save_document', 'vectorize_document', 'update_metadata', 'query_metadata', 'multi_update_metadata', 'bulk_delete_metadata', 'bulk_update_metadata', 'multi_field_update', 'semantic_search_local', 'find_metadata_by_key', 'semantic_search_metadata', 'semantic_search_by_author', 'semantic_search_by_year', 'semantic_search_by_keyword', 'conditional_search_metadata', 'semantic_search_multiple_fields', 'sort_metadata', 'advanced_semantic_search', 'create_metadata_tree', 'view_metadata_tree', 'delete_metadata_node', 'update_metadata_node', 'depth_first_search', 'rebuild_metadata_tree', 'semantic_search_metadata_tree', 'validate_metadata_tree', 'generate_embedding', 'semantic_similarity_search', 'batch_generate_embeddings', 'semantic_expand_metadata', 'semantic_filter_metadata', 'analyze_metadata_trends', 'aggregate_metadata', 'detect_anomalies', 'metadata_statistics', 'error_tool', 'save_metadata_to_faiss', 'load_metadata_from_faiss', 'query_metadata_faiss', 'advanced_query_faiss', 'rebuild_metadata_tree_from_faiss']
2025-05-16 18:22:27,978 - INFO - OPENAI_AVAILABLE is True, attempting to add OpenAI tools...
2025-05-16 18:22:27,978 - INFO - OpenAI tools added: ['generate_embedding_real', 'semantic_search_cosine', 'clear_embeddings']
2025-05-16 18:22:27,978 - INFO - Discovered tools: ['save_text', 'add_numbers', 'multiply_numbers', 'echo', 'get_registered_tools', 'delay', 'save_document', 'vectorize_document', 'update_metadata', 'query_metadata', 'multi_update_metadata', 'bulk_delete_metadata', 'bulk_update_metadata', 'multi_field_update', 'semantic_search_local', 'find_metadata_by_key', 'semantic_search_metadata', 'semantic_search_by_author', 'semantic_search_by_year', 'semantic_search_by_keyword', 'conditional_search_metadata', 'semantic_search_multiple_fields', 'sort_metadata', 'advanced_semantic_search', 'create_metadata_tree', 'view_metadata_tree', 'delete_metadata_node', 'update_metadata_node', 'depth_first_search', 'rebuild_metadata_tree', 'semantic_search_metadata_tree', 'validate_metadata_tree', 'generate_embedding', 'semantic_similarity_search', 'batch_generate_embeddings', 'semantic_expand_metadata', 'semantic_filter_metadata', 'analyze_metadata_trends', 'aggregate_metadata', 'detect_anomalies', 'metadata_statistics', 'error_tool', 'save_metadata_to_faiss', 'load_metadata_from_faiss', 'query_metadata_faiss', 'advanced_query_faiss', 'rebuild_metadata_tree_from_faiss', 'generate_embedding_real', 'semantic_search_cosine', 'clear_embeddings']
2025-05-16 18:22:27,978 - INFO - Starting MCP Stdio Server...
2025-05-16 18:22:27,978 - INFO - MCP Stdio Server Started. Waiting for JSON input on stdin...
2025-05-16 18:22:28,403 - INFO - Executing tool 'add_numbers' (ID: req-add-1) with input: {'a': 10, 'b': 5}
2025-05-16 18:22:28,404 - INFO - Executing add_numbers tool with: a=10 (<class 'int'>), b=5 (<class 'int'>)
2025-05-16 18:22:28,404 - INFO - add_numbers calculation result: 15
2025-05-16 18:22:28,404 - INFO - Tool 'add_numbers' executed successfully (ID: req-add-1).
2025-05-16 18:22:28,404 - INFO - Executing tool 'multiply_numbers' (ID: req-mul-2) with input: {'a': 7, 'b': 3}
2025-05-16 18:22:28,404 - INFO - Executing multiply_numbers tool with kwargs: {'a': 7, 'b': 3}
2025-05-16 18:22:28,404 - INFO - Calculation: 7 * 3 = 21
2025-05-16 18:22:28,404 - INFO - Tool 'multiply_numbers' executed successfully (ID: req-mul-2).
2025-05-16 18:22:28,404 - INFO - Executing tool 'save_text' (ID: req-sav-3) with input: {'filename': 'test_all_tools.txt', 'content': 'Testing save...'}
2025-05-16 18:22:28,404 - INFO - Executing save_text tool with filename: test_all_tools.txt
2025-05-16 18:22:28,405 - INFO - Successfully saved text to test_all_tools.txt in /Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/tools/../test_outputs
2025-05-16 18:22:28,405 - INFO - Tool 'save_text' executed successfully (ID: req-sav-3).
2025-05-16 18:22:28,405 - ERROR - Unknown tool requested (ID: req-err-4): absolutely_fake_tool
2025-05-16 18:22:28,406 - INFO - Executing tool 'add_numbers' (ID: req-err-5) with input: {'a': 5}
2025-05-16 18:22:28,406 - ERROR - Error executing tool 'add_numbers' (ID: req-err-5): add_numbers() missing 1 required positional argument: 'b'
Traceback (most recent call last):
  File "/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/mcp/local_mcp_server.py", line 105, in run_mcp_loop
    raise e # Re-raise original TypeError if fallback doesn't match signature
  File "/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/mcp/local_mcp_server.py", line 98, in run_mcp_loop
    tool_result_data = tool_function(**tool_input)
TypeError: add_numbers() missing 1 required positional argument: 'b'
2025-05-16 18:22:28,407 - ERROR - Error executing tool 'add_numbers' (ID: req-err-5): add_numbers() missing 1 required positional argument: 'b'
Traceback (most recent call last):
  File "/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/mcp/local_mcp_server.py", line 120, in run_mcp_loop
    raise exec_err
  File "/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/mcp/local_mcp_server.py", line 105, in run_mcp_loop
    raise e # Re-raise original TypeError if fallback doesn't match signature
  File "/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/mcp/local_mcp_server.py", line 98, in run_mcp_loop
    tool_result_data = tool_function(**tool_input)
TypeError: add_numbers() missing 1 required positional argument: 'b'
2025-05-16 18:22:28,407 - INFO - Executing tool 'get_registered_tools' (ID: req-get-6) with input: {}
2025-05-16 18:22:28,407 - ERROR - Error executing tool 'get_registered_tools' (ID: req-get-6): get_registered_tools() missing 1 required positional argument: 'agent_context'
Traceback (most recent call last):
  File "/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/mcp/local_mcp_server.py", line 94, in run_mcp_loop
    tool_result_data = tool_function() # Call without arguments
TypeError: get_registered_tools() missing 1 required positional argument: 'agent_context'
2025-05-16 18:22:28,408 - ERROR - Error executing tool 'get_registered_tools' (ID: req-get-6): get_registered_tools() missing 1 required positional argument: 'agent_context'
Traceback (most recent call last):
  File "/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/mcp/local_mcp_server.py", line 120, in run_mcp_loop
    raise exec_err
  File "/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/mcp/local_mcp_server.py", line 94, in run_mcp_loop
    tool_result_data = tool_function() # Call without arguments
TypeError: get_registered_tools() missing 1 required positional argument: 'agent_context'
2025-05-16 18:22:28,419 - INFO - Exit command received (ID: None). Shutting down.
2025-05-16 18:22:28,419 - INFO - MCP Stdio Server loop finished.
2025-05-16 18:22:28,419 - INFO - MCP Stdio Server Stopped.
_______ TestMCPAgentLargeBatchDirect.test_large_batch_processing_direct ________

self = <ADK.agent_data.tests.test_mcp_agent_batch_large.TestMCPAgentLargeBatchDirect testMethod=test_large_batch_processing_direct>

    def test_large_batch_processing_direct(self):
        """Test sending a batch of 100 requests directly to the agent instance."""
        num_requests = 100
        batch_requests = []
        expected_outcomes = {} # Store expected outcome (status, result/error check) for each request id

        # Define target counts for different outcomes
        num_timeout = 15
        num_runtime_error = 15
        num_missing_tool = 15
        num_missing_args = 15
        num_success = num_requests - (num_timeout + num_runtime_error + num_missing_tool + num_missing_args)

        # Divide success cases among available success tools
        num_add = math.ceil(num_success / 4)
        num_multiply = math.ceil(num_success / 4)
        num_save = math.ceil(num_success / 4)
        num_echo = num_success - (num_add + num_multiply + num_save) # Remaining go to echo

        request_types = (
            ['timeout'] * num_timeout +
            ['runtime_error'] * num_runtime_error +
            ['missing_tool'] * num_missing_tool +
            ['missing_args'] * num_missing_args +
            # --- START REPLACEMENT: Use calculated counts ---
            # (['add'] * math.ceil(num_success / 3)) +
            # (['multiply'] * math.ceil(num_success / 3)) +
            # (['save'] * math.floor(num_success / 3))
            (['add'] * num_add) +
            (['multiply'] * num_multiply) +
            (['save'] * num_save) +
            (['echo_test'] * num_echo) # Add echo cases
            # --- END REPLACEMENT ---
        )
        # Ensure the list has exactly num_requests elements, trim/pad if needed due to rounding/distribution
        # --- START REPLACEMENT: Ensure exact number of requests ---
        if len(request_types) > num_requests:
            request_types = request_types[:num_requests]
        elif len(request_types) < num_requests:
            # Add more success cases if needed (e.g., echo)
            request_types.extend(['echo_test'] * (num_requests - len(request_types)))
        # --- END REPLACEMENT ---
        random.shuffle(request_types) # Shuffle the types

        print(f"Generating {num_requests} requests with distribution:")
        print(f"  Success (add/multiply/save/echo): {num_success}")
        print(f"  Timeout (delay): {num_timeout}")
        print(f"  Runtime Error (error_tool): {num_runtime_error}")
        print(f"  Missing Tool: {num_missing_tool}")
        print(f"  Missing Args (add_numbers): {num_missing_args}")

        for i in range(num_requests):
            req_id = f"batch-req-{i}"
            tool_choice = request_types[i]
            request = None # Initialize request
            expected_outcome_base = {"expected_status": None} # Ensure this key always exists

            if tool_choice == 'add':
                a, b = random.randint(1, 1000), random.randint(1, 1000)
                request = {"id": req_id, "tool_name": "add_numbers", "kwargs": {"a": a, "b": b}}
                expected_outcome = {**expected_outcome_base, "expected_status": "success", "check_result": True, "expected_value": a + b}
            elif tool_choice == 'multiply':
                a, b = random.randint(1, 100), random.randint(1, 100)
                request = {"id": req_id, "tool_name": "multiply_numbers", "kwargs": {"a": a, "b": b}}
                expected_outcome = {**expected_outcome_base, "expected_status": "success", "check_result": True, "expected_value": a * b}
            elif tool_choice == 'save':
                 file_content = f"Test data {i} for {req_id}"
                 filename = f"test_file_{req_id}.txt"
                 request = {"id": req_id, "tool_name": "save_text", "kwargs": {"filename": filename, "content": file_content}}
                 expected_result_str = f"Saved text: {filename}"
                 expected_outcome = {**expected_outcome_base, "expected_status": "success", "check_result": True, "expected_value": expected_result_str}
            elif tool_choice == 'timeout':
                 request = {"id": req_id, "tool_name": "delay", "kwargs": {"delay_ms": 6000}}
                 expected_outcome = {**expected_outcome_base, "expected_status": "timeout", "check_error": True, "error_contains": "timed out"}
            elif tool_choice == 'runtime_error':
                 request = {"id": req_id, "tool_name": "error_tool", "args": []}
                 expected_outcome = {**expected_outcome_base, "expected_status": "runtime_error", "check_error": True, "error_contains": "Intentional error"}
            elif tool_choice == 'echo_test':
                 echo_arg = f"Echo data for {req_id}"
                 request = {"id": req_id, "tool_name": "echo", "args": [echo_arg]}
                 expected_outcome = {**expected_outcome_base, "expected_status": "success", "check_result": True, "expected_value": echo_arg}
            elif tool_choice == 'missing_args':
                 request = {"id": req_id, "tool_name": "add_numbers", "kwargs": {"a": 1}}
                 expected_outcome = {**expected_outcome_base, "expected_status": "missing_args", "check_error": True, "error_contains": "missing 1 required positional argument"}
            elif tool_choice == 'missing_tool':
                 request = {"id": req_id, "tool_name": "nonexistent_tool", "args": []}
                 expected_outcome = {**expected_outcome_base, "expected_status": "missing_tool", "check_error": True, "error_contains": "Tool 'nonexistent_tool' not found"}

            if request: # Ensure a request was actually generated for the type
                batch_requests.append(request)
                expected_outcomes[req_id] = expected_outcome # Store the generated outcome
            else:
                print(f"Warning: No request generated for type '{tool_choice}' at index {i}. Adjusting num_requests.")
                # If a request type fails generation, we might need to adjust num_requests,
                # but let's assume generation is robust for now. If not, remove the corresponding entry from expected_outcomes too.

        # Ensure every req_id in batch_requests has an entry in expected_outcomes
        # (This check should ideally not be needed if generation logic is correct)
        generated_req_ids = {req['id'] for req in batch_requests}
        missing_outcomes = generated_req_ids - set(expected_outcomes.keys())
        if missing_outcomes:
            print(f"Warning: Missing expected outcomes for req_ids: {missing_outcomes}")
            # Handle missing outcomes if necessary, e.g., by adding default error expectations
            for missing_id in missing_outcomes:
                 expected_outcomes[missing_id] = {"expected_status": "error"} # Or some other default

        # Prune expected_outcomes for requests that weren't generated (if any)
        expected_outcomes = {k: v for k, v in expected_outcomes.items() if k in generated_req_ids}

        # Correct the total number of requests if any weren't generated
        num_requests = len(batch_requests)
        print(f"Actual number of requests being sent: {num_requests}")

        # Run the agent coroutine using asyncio.run
        print(f"Sending batch of {len(batch_requests)} requests directly to agent...")
        start_time = time.monotonic()

        # We need to run the async method within the sync test method
        try:
            # Update call to use the renamed helper
>           response_list = asyncio.run(self._run_agent_batch_async(batch_requests))

ADK/agent_data/tests/test_mcp_agent_batch_large.py:218:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
/opt/homebrew/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/runners.py:44: in run
    return loop.run_until_complete(main)
/opt/homebrew/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py:649: in run_until_complete
    return future.result()
ADK/agent_data/tests/test_mcp_agent_batch_large.py:95: in _run_agent_batch_async
    return await self.agent.run(input_data) # Changed to agent.run
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <ADK.agent_data.mcp.mcp_agent_core.MCPAgent object at 0x163dd5a50>
request_data = [{'id': 'batch-req-0', 'kwargs': {'delay_ms': 6000}, 'tool_name': 'delay'}, {'id': 'batch-req-1', 'kwargs': {'content'... 'tool_name': 'add_numbers'}, {'id': 'batch-req-5', 'kwargs': {'a': 2, 'b': 97}, 'tool_name': 'multiply_numbers'}, ...]
request_id = None

    def run(self, request_data: Dict, request_id: Optional[str] = None) -> Dict:
        """Processes a single tool request synchronously."""
        exec_request_id = self._get_next_request_id(request_id)
>       request_data['id'] = exec_request_id # Ensure ID is set in the input data
E       TypeError: list indices must be integers or slices, not str

ADK/agent_data/mcp/mcp_agent_core.py:183: TypeError

During handling of the above exception, another exception occurred:

self = <ADK.agent_data.tests.test_mcp_agent_batch_large.TestMCPAgentLargeBatchDirect testMethod=test_large_batch_processing_direct>

    def test_large_batch_processing_direct(self):
        """Test sending a batch of 100 requests directly to the agent instance."""
        num_requests = 100
        batch_requests = []
        expected_outcomes = {} # Store expected outcome (status, result/error check) for each request id

        # Define target counts for different outcomes
        num_timeout = 15
        num_runtime_error = 15
        num_missing_tool = 15
        num_missing_args = 15
        num_success = num_requests - (num_timeout + num_runtime_error + num_missing_tool + num_missing_args)

        # Divide success cases among available success tools
        num_add = math.ceil(num_success / 4)
        num_multiply = math.ceil(num_success / 4)
        num_save = math.ceil(num_success / 4)
        num_echo = num_success - (num_add + num_multiply + num_save) # Remaining go to echo

        request_types = (
            ['timeout'] * num_timeout +
            ['runtime_error'] * num_runtime_error +
            ['missing_tool'] * num_missing_tool +
            ['missing_args'] * num_missing_args +
            # --- START REPLACEMENT: Use calculated counts ---
            # (['add'] * math.ceil(num_success / 3)) +
            # (['multiply'] * math.ceil(num_success / 3)) +
            # (['save'] * math.floor(num_success / 3))
            (['add'] * num_add) +
            (['multiply'] * num_multiply) +
            (['save'] * num_save) +
            (['echo_test'] * num_echo) # Add echo cases
            # --- END REPLACEMENT ---
        )
        # Ensure the list has exactly num_requests elements, trim/pad if needed due to rounding/distribution
        # --- START REPLACEMENT: Ensure exact number of requests ---
        if len(request_types) > num_requests:
            request_types = request_types[:num_requests]
        elif len(request_types) < num_requests:
            # Add more success cases if needed (e.g., echo)
            request_types.extend(['echo_test'] * (num_requests - len(request_types)))
        # --- END REPLACEMENT ---
        random.shuffle(request_types) # Shuffle the types

        print(f"Generating {num_requests} requests with distribution:")
        print(f"  Success (add/multiply/save/echo): {num_success}")
        print(f"  Timeout (delay): {num_timeout}")
        print(f"  Runtime Error (error_tool): {num_runtime_error}")
        print(f"  Missing Tool: {num_missing_tool}")
        print(f"  Missing Args (add_numbers): {num_missing_args}")

        for i in range(num_requests):
            req_id = f"batch-req-{i}"
            tool_choice = request_types[i]
            request = None # Initialize request
            expected_outcome_base = {"expected_status": None} # Ensure this key always exists

            if tool_choice == 'add':
                a, b = random.randint(1, 1000), random.randint(1, 1000)
                request = {"id": req_id, "tool_name": "add_numbers", "kwargs": {"a": a, "b": b}}
                expected_outcome = {**expected_outcome_base, "expected_status": "success", "check_result": True, "expected_value": a + b}
            elif tool_choice == 'multiply':
                a, b = random.randint(1, 100), random.randint(1, 100)
                request = {"id": req_id, "tool_name": "multiply_numbers", "kwargs": {"a": a, "b": b}}
                expected_outcome = {**expected_outcome_base, "expected_status": "success", "check_result": True, "expected_value": a * b}
            elif tool_choice == 'save':
                 file_content = f"Test data {i} for {req_id}"
                 filename = f"test_file_{req_id}.txt"
                 request = {"id": req_id, "tool_name": "save_text", "kwargs": {"filename": filename, "content": file_content}}
                 expected_result_str = f"Saved text: {filename}"
                 expected_outcome = {**expected_outcome_base, "expected_status": "success", "check_result": True, "expected_value": expected_result_str}
            elif tool_choice == 'timeout':
                 request = {"id": req_id, "tool_name": "delay", "kwargs": {"delay_ms": 6000}}
                 expected_outcome = {**expected_outcome_base, "expected_status": "timeout", "check_error": True, "error_contains": "timed out"}
            elif tool_choice == 'runtime_error':
                 request = {"id": req_id, "tool_name": "error_tool", "args": []}
                 expected_outcome = {**expected_outcome_base, "expected_status": "runtime_error", "check_error": True, "error_contains": "Intentional error"}
            elif tool_choice == 'echo_test':
                 echo_arg = f"Echo data for {req_id}"
                 request = {"id": req_id, "tool_name": "echo", "args": [echo_arg]}
                 expected_outcome = {**expected_outcome_base, "expected_status": "success", "check_result": True, "expected_value": echo_arg}
            elif tool_choice == 'missing_args':
                 request = {"id": req_id, "tool_name": "add_numbers", "kwargs": {"a": 1}}
                 expected_outcome = {**expected_outcome_base, "expected_status": "missing_args", "check_error": True, "error_contains": "missing 1 required positional argument"}
            elif tool_choice == 'missing_tool':
                 request = {"id": req_id, "tool_name": "nonexistent_tool", "args": []}
                 expected_outcome = {**expected_outcome_base, "expected_status": "missing_tool", "check_error": True, "error_contains": "Tool 'nonexistent_tool' not found"}

            if request: # Ensure a request was actually generated for the type
                batch_requests.append(request)
                expected_outcomes[req_id] = expected_outcome # Store the generated outcome
            else:
                print(f"Warning: No request generated for type '{tool_choice}' at index {i}. Adjusting num_requests.")
                # If a request type fails generation, we might need to adjust num_requests,
                # but let's assume generation is robust for now. If not, remove the corresponding entry from expected_outcomes too.

        # Ensure every req_id in batch_requests has an entry in expected_outcomes
        # (This check should ideally not be needed if generation logic is correct)
        generated_req_ids = {req['id'] for req in batch_requests}
        missing_outcomes = generated_req_ids - set(expected_outcomes.keys())
        if missing_outcomes:
            print(f"Warning: Missing expected outcomes for req_ids: {missing_outcomes}")
            # Handle missing outcomes if necessary, e.g., by adding default error expectations
            for missing_id in missing_outcomes:
                 expected_outcomes[missing_id] = {"expected_status": "error"} # Or some other default

        # Prune expected_outcomes for requests that weren't generated (if any)
        expected_outcomes = {k: v for k, v in expected_outcomes.items() if k in generated_req_ids}

        # Correct the total number of requests if any weren't generated
        num_requests = len(batch_requests)
        print(f"Actual number of requests being sent: {num_requests}")

        # Run the agent coroutine using asyncio.run
        print(f"Sending batch of {len(batch_requests)} requests directly to agent...")
        start_time = time.monotonic()

        # We need to run the async method within the sync test method
        try:
            # Update call to use the renamed helper
            response_list = asyncio.run(self._run_agent_batch_async(batch_requests))
        except Exception as e:
>            self.fail(f"Agent execution failed: {e}\n{traceback.format_exc()}")
E            AssertionError: Agent execution failed: list indices must be integers or slices, not str
E            Traceback (most recent call last):
E              File "/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/tests/test_mcp_agent_batch_large.py", line 218, in test_large_batch_processing_direct
E                response_list = asyncio.run(self._run_agent_batch_async(batch_requests))
E              File "/opt/homebrew/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/runners.py", line 44, in run
E                return loop.run_until_complete(main)
E              File "/opt/homebrew/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py", line 649, in run_until_complete
E                return future.result()
E              File "/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/tests/test_mcp_agent_batch_large.py", line 95, in _run_agent_batch_async
E                return await self.agent.run(input_data) # Changed to agent.run
E              File "/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/mcp/mcp_agent_core.py", line 183, in run
E                request_data['id'] = exec_request_id # Ensure ID is set in the input data
E            TypeError: list indices must be integers or slices, not str

ADK/agent_data/tests/test_mcp_agent_batch_large.py:220: AssertionError
---------------------------- Captured stdout setup -----------------------------
Initializing MCPAgent instance for tests...
MCPAgent instance created successfully.
---------------------------- Captured stderr setup -----------------------------
2025-05-16 18:22:28,606 [CORE-INIT] INFO     - Initializing MCPAgent Core...
2025-05-16 18:22:28,606 [CORE-INIT] INFO     - MCPAgent Core initialized. Available tools: ['save_text', 'add_numbers', 'multiply_numbers', 'echo', 'get_registered_tools', 'delay', 'save_document', 'vectorize_document', 'update_metadata', 'query_metadata', 'multi_update_metadata', 'bulk_delete_metadata', 'bulk_update_metadata', 'multi_field_update', 'semantic_search_local', 'find_metadata_by_key', 'semantic_search_metadata', 'semantic_search_by_author', 'semantic_search_by_year', 'semantic_search_by_keyword', 'conditional_search_metadata', 'semantic_search_multiple_fields', 'sort_metadata', 'advanced_semantic_search', 'create_metadata_tree', 'view_metadata_tree', 'delete_metadata_node', 'update_metadata_node', 'depth_first_search', 'rebuild_metadata_tree', 'semantic_search_metadata_tree', 'validate_metadata_tree', 'generate_embedding', 'semantic_similarity_search', 'batch_generate_embeddings', 'semantic_expand_metadata', 'semantic_filter_metadata', 'analyze_metadata_trends', 'aggregate_metadata', 'detect_anomalies', 'metadata_statistics', 'error_tool', 'save_metadata_to_faiss', 'load_metadata_from_faiss', 'query_metadata_faiss', 'advanced_query_faiss', 'rebuild_metadata_tree_from_faiss', 'generate_embedding_real', 'semantic_search_cosine', 'clear_embeddings']
------------------------------ Captured log setup ------------------------------
INFO     MCPAgentCore:mcp_agent_core.py:58 Initializing MCPAgent Core...
INFO     MCPAgentCore:mcp_agent_core.py:62 MCPAgent Core initialized. Available tools: ['save_text', 'add_numbers', 'multiply_numbers', 'echo', 'get_registered_tools', 'delay', 'save_document', 'vectorize_document', 'update_metadata', 'query_metadata', 'multi_update_metadata', 'bulk_delete_metadata', 'bulk_update_metadata', 'multi_field_update', 'semantic_search_local', 'find_metadata_by_key', 'semantic_search_metadata', 'semantic_search_by_author', 'semantic_search_by_year', 'semantic_search_by_keyword', 'conditional_search_metadata', 'semantic_search_multiple_fields', 'sort_metadata', 'advanced_semantic_search', 'create_metadata_tree', 'view_metadata_tree', 'delete_metadata_node', 'update_metadata_node', 'depth_first_search', 'rebuild_metadata_tree', 'semantic_search_metadata_tree', 'validate_metadata_tree', 'generate_embedding', 'semantic_similarity_search', 'batch_generate_embeddings', 'semantic_expand_metadata', 'semantic_filter_metadata', 'analyze_metadata_trends', 'aggregate_metadata', 'detect_anomalies', 'metadata_statistics', 'error_tool', 'save_metadata_to_faiss', 'load_metadata_from_faiss', 'query_metadata_faiss', 'advanced_query_faiss', 'rebuild_metadata_tree_from_faiss', 'generate_embedding_real', 'semantic_search_cosine', 'clear_embeddings']
----------------------------- Captured stdout call -----------------------------
Generating 100 requests with distribution:
  Success (add/multiply/save/echo): 40
  Timeout (delay): 15
  Runtime Error (error_tool): 15
  Missing Tool: 15
  Missing Args (add_numbers): 15
Actual number of requests being sent: 100
Sending batch of 100 requests directly to agent...
Running agent directly with input type: list
--------------------------- Captured stdout teardown ---------------------------
Cleaning up test output files...
___________ TestMCPBatchToolsStdio.test_batch_mixed_success_failure ____________

self = <ADK.agent_data.tests.test_mcp_agent_batch_tools.TestMCPBatchToolsStdio testMethod=test_batch_mixed_success_failure>

    def test_batch_mixed_success_failure(self):
        logger.info("Starting test_batch_mixed_success_failure")
        batch_request = [
            {"id": "mix-1", "tool_name": "save_document", "args": {"filename": "mix.txt"}}, # Success
            {"id": "mix-2", "tool_name": "update_metadata", "args": {"key": "fail_key"}}, # Fail (simulated)
            {"id": "mix-3", "tool_name": "nonexistent_tool", "args": {}}, # Fail (tool not found)
            "invalid_item", # Fail (invalid type in batch)
            {"id": "mix-5", "tool_name": "query_metadata", "args": {"key": "meta1"}} # Success
        ]

>       output = self.run_mcp_request(batch_request)

ADK/agent_data/tests/test_mcp_agent_batch_tools.py:281:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
ADK/agent_data/tests/test_mcp_agent_batch_tools.py:151: in run_mcp_request
    response = asyncio.run(self.mcp_agent.run(json.loads(input_json.strip())))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <ADK.agent_data.mcp.mcp_agent_core.MCPAgent object at 0x163d87b20>
request_data = [{'args': {'filename': 'mix.txt'}, 'id': 'mix-1', 'tool_name': 'save_document'}, {'args': {'key': 'fail_key'}, 'id': '...l_name': 'nonexistent_tool'}, 'invalid_item', {'args': {'key': 'meta1'}, 'id': 'mix-5', 'tool_name': 'query_metadata'}]
request_id = None

    def run(self, request_data: Dict, request_id: Optional[str] = None) -> Dict:
        """Processes a single tool request synchronously."""
        exec_request_id = self._get_next_request_id(request_id)
>       request_data['id'] = exec_request_id # Ensure ID is set in the input data
E       TypeError: list indices must be integers or slices, not str

ADK/agent_data/mcp/mcp_agent_core.py:183: TypeError
----------------------------- Captured stderr call -----------------------------
2025-05-16 18:22:28,675 [CORE-INIT] INFO     - Initializing MCPAgent Core...
2025-05-16 18:22:28,677 [CORE-INIT] INFO     - MCPAgent Core initialized. Available tools: []
------------------------------ Captured log call -------------------------------
INFO     MCPAgentCore:mcp_agent_core.py:58 Initializing MCPAgent Core...
INFO     MCPAgentCore:mcp_agent_core.py:62 MCPAgent Core initialized. Available tools: []
_________________ TestMCPBatchToolsStdio.test_batch_successful _________________

self = <ADK.agent_data.tests.test_mcp_agent_batch_tools.TestMCPBatchToolsStdio testMethod=test_batch_successful>

    def test_batch_successful(self):
        logger.info("Starting test_batch_successful")
        batch_request = [
            {"id": "b-req-1", "tool_name": "add_numbers", "args": {"a": 10, "b": 5}}, # Mock not defined, should fail in _execute_tool
            {"id": "b-req-2", "tool_name": "save_document", "args": {"filename": "b.txt", "content": "batch"}},
            {"id": "b-req-3", "tool_name": "vectorize_document", "args": {"doc_key": "doc1"}}
        ]
        # Redefine mock outputs needed for this batch
        self.tool_outputs["add_numbers"] = {"status": "success", "result": 15}

>       output = self.run_mcp_request(batch_request)

ADK/agent_data/tests/test_mcp_agent_batch_tools.py:252:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
ADK/agent_data/tests/test_mcp_agent_batch_tools.py:151: in run_mcp_request
    response = asyncio.run(self.mcp_agent.run(json.loads(input_json.strip())))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <ADK.agent_data.mcp.mcp_agent_core.MCPAgent object at 0x163eca710>
request_data = [{'args': {'a': 10, 'b': 5}, 'id': 'b-req-1', 'tool_name': 'add_numbers'}, {'args': {'content': 'batch', 'filename': '...q-2', 'tool_name': 'save_document'}, {'args': {'doc_key': 'doc1'}, 'id': 'b-req-3', 'tool_name': 'vectorize_document'}]
request_id = None

    def run(self, request_data: Dict, request_id: Optional[str] = None) -> Dict:
        """Processes a single tool request synchronously."""
        exec_request_id = self._get_next_request_id(request_id)
>       request_data['id'] = exec_request_id # Ensure ID is set in the input data
E       TypeError: list indices must be integers or slices, not str

ADK/agent_data/mcp/mcp_agent_core.py:183: TypeError
----------------------------- Captured stderr call -----------------------------
2025-05-16 18:22:28,685 [CORE-INIT] INFO     - Initializing MCPAgent Core...
2025-05-16 18:22:28,686 [CORE-INIT] INFO     - MCPAgent Core initialized. Available tools: []
------------------------------ Captured log call -------------------------------
INFO     MCPAgentCore:mcp_agent_core.py:58 Initializing MCPAgent Core...
INFO     MCPAgentCore:mcp_agent_core.py:62 MCPAgent Core initialized. Available tools: []
__________________ TestMCPBatchToolsStdio.test_query_metadata __________________

self = <ADK.agent_data.tests.test_mcp_agent_batch_tools.TestMCPBatchToolsStdio testMethod=test_query_metadata>

    def test_query_metadata(self):
        req_id = "qmeta-1"
>       output = self.run_mcp_request({"id": req_id, "tool_name": "query_metadata", "args": {"key": "meta1"}})

ADK/agent_data/tests/test_mcp_agent_batch_tools.py:230:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
ADK/agent_data/tests/test_mcp_agent_batch_tools.py:151: in run_mcp_request
    response = asyncio.run(self.mcp_agent.run(json.loads(input_json.strip())))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

main = {'error': "Input validation error: a coroutine was expected, got <MagicMock name='mock.tools_manager.execute_tool()' id='5971736512'>", 'meta': {'duration_ms': 100.0, 'request_id': 'mcp-core-1747394548694-1', 'status': 'failed'}}

    def run(main, *, debug=None):
        """Execute the coroutine and return the result.

        This function runs the passed coroutine, taking care of
        managing the asyncio event loop and finalizing asynchronous
        generators.

        This function cannot be called when another asyncio event loop is
        running in the same thread.

        If debug is True, the event loop will be run in debug mode.

        This function always creates a new event loop and closes it at the end.
        It should be used as a main entry point for asyncio programs, and should
        ideally only be called once.

        Example:

            async def main():
                await asyncio.sleep(1)
                print('hello')

            asyncio.run(main())
        """
        if events._get_running_loop() is not None:
            raise RuntimeError(
                "asyncio.run() cannot be called from a running event loop")

        if not coroutines.iscoroutine(main):
>           raise ValueError("a coroutine was expected, got {!r}".format(main))
E           ValueError: a coroutine was expected, got {'error': "Input validation error: a coroutine was expected, got <MagicMock name='mock.tools_manager.execute_tool()' id='5971736512'>", 'meta': {'duration_ms': 100.0, 'request_id': 'mcp-core-1747394548694-1', 'status': 'failed'}}

/opt/homebrew/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/runners.py:37: ValueError
----------------------------- Captured stderr call -----------------------------
2025-05-16 18:22:28,693 [CORE-INIT] INFO     - Initializing MCPAgent Core...
2025-05-16 18:22:28,694 [CORE-INIT] INFO     - MCPAgent Core initialized. Available tools: []
2025-05-16 18:22:28,694 [mcp-core-1747394548694-1] INFO     - Processing single request: query_metadata
2025-05-16 18:22:28,694 [mcp-core-1747394548694-1] INFO     - [_execute_tool_sync] Received request_data: {'id': 'mcp-core-1747394548694-1', 'tool_name': 'query_metadata', 'args': {'key': 'meta1'}}
2025-05-16 18:22:28,694 [mcp-core-1747394548694-1] INFO     - [_execute_tool_sync] Extracted input_payload_from_request: {}
2025-05-16 18:22:28,694 [mcp-core-1747394548694-1] INFO     - [_execute_tool_sync] Type of input_payload_from_request: <class 'dict'>
2025-05-16 18:22:28,694 [mcp-core-1747394548694-1] INFO     - Executing tool 'query_metadata' with args: [], kwargs: {}
2025-05-16 18:22:28,694 [mcp-core-1747394548694-1] INFO     - Final check before calling execute_tool - Args: [], Kwargs: {}
2025-05-16 18:22:28,694 [mcp-core-1747394548694-1] INFO     - Attempting asyncio.run for tool: query_metadata
2025-05-16 18:22:28,694 [mcp-core-1747394548694-1] ERROR    - Input validation error: a coroutine was expected, got <MagicMock name='mock.tools_manager.execute_tool()' id='5971736512'> - Request Data: {'id': 'mcp-core-1747394548694-1', 'tool_name': 'query_metadata', 'args': {'key': 'meta1'}}
Traceback (most recent call last):
  File "/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/mcp/mcp_agent_core.py", line 140, in _execute_tool_sync
    tool_result_data = asyncio.run(self.core_agent.tools_manager.execute_tool(
  File "/opt/homebrew/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/runners.py", line 37, in run
    raise ValueError("a coroutine was expected, got {!r}".format(main))
ValueError: a coroutine was expected, got <MagicMock name='mock.tools_manager.execute_tool()' id='5971736512'>
2025-05-16 18:22:28,695 [mcp-core-1747394548694-1] ERROR    - Tool 'query_metadata' failed. Error: Input validation error: a coroutine was expected, got <MagicMock name='mock.tools_manager.execute_tool()' id='5971736512'>
------------------------------ Captured log call -------------------------------
INFO     MCPAgentCore:mcp_agent_core.py:58 Initializing MCPAgent Core...
INFO     MCPAgentCore:mcp_agent_core.py:62 MCPAgent Core initialized. Available tools: []
INFO     MCPAgentCore:mcp_agent_core.py:186 Processing single request: query_metadata
INFO     MCPAgentCore:mcp_agent_core.py:100 [_execute_tool_sync] Received request_data: {'id': 'mcp-core-1747394548694-1', 'tool_name': 'query_metadata', 'args': {'key': 'meta1'}}
INFO     MCPAgentCore:mcp_agent_core.py:107 [_execute_tool_sync] Extracted input_payload_from_request: {}
INFO     MCPAgentCore:mcp_agent_core.py:108 [_execute_tool_sync] Type of input_payload_from_request: <class 'dict'>
INFO     MCPAgentCore:mcp_agent_core.py:133 Executing tool 'query_metadata' with args: [], kwargs: {}
INFO     MCPAgentCore:mcp_agent_core.py:136 Final check before calling execute_tool - Args: [], Kwargs: {}
INFO     MCPAgentCore:mcp_agent_core.py:138 Attempting asyncio.run for tool: query_metadata
ERROR    MCPAgentCore:mcp_agent_core.py:154 Input validation error: a coroutine was expected, got <MagicMock name='mock.tools_manager.execute_tool()' id='5971736512'> - Request Data: {'id': 'mcp-core-1747394548694-1', 'tool_name': 'query_metadata', 'args': {'key': 'meta1'}}
Traceback (most recent call last):
  File "/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/mcp/mcp_agent_core.py", line 140, in _execute_tool_sync
    tool_result_data = asyncio.run(self.core_agent.tools_manager.execute_tool(
  File "/opt/homebrew/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/runners.py", line 37, in run
    raise ValueError("a coroutine was expected, got {!r}".format(main))
ValueError: a coroutine was expected, got <MagicMock name='mock.tools_manager.execute_tool()' id='5971736512'>
ERROR    MCPAgentCore:mcp_agent_core.py:168 Tool 'query_metadata' failed. Error: Input validation error: a coroutine was expected, got <MagicMock name='mock.tools_manager.execute_tool()' id='5971736512'>
__________________ TestMCPBatchToolsStdio.test_save_document ___________________

self = <ADK.agent_data.tests.test_mcp_agent_batch_tools.TestMCPBatchToolsStdio testMethod=test_save_document>

    def test_save_document(self):
        req_id = "svdoc-1"
>       output = self.run_mcp_request({"id": req_id, "tool_name": "save_document", "args": {"content": "test", "filename": "f.txt"}})

ADK/agent_data/tests/test_mcp_agent_batch_tools.py:205:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
ADK/agent_data/tests/test_mcp_agent_batch_tools.py:151: in run_mcp_request
    response = asyncio.run(self.mcp_agent.run(json.loads(input_json.strip())))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

main = {'error': "Input validation error: a coroutine was expected, got <MagicMock name='mock.tools_manager.execute_tool()' id='5968161200'>", 'meta': {'duration_ms': 100.0, 'request_id': 'mcp-core-1747394548702-1', 'status': 'failed'}}

    def run(main, *, debug=None):
        """Execute the coroutine and return the result.

        This function runs the passed coroutine, taking care of
        managing the asyncio event loop and finalizing asynchronous
        generators.

        This function cannot be called when another asyncio event loop is
        running in the same thread.

        If debug is True, the event loop will be run in debug mode.

        This function always creates a new event loop and closes it at the end.
        It should be used as a main entry point for asyncio programs, and should
        ideally only be called once.

        Example:

            async def main():
                await asyncio.sleep(1)
                print('hello')

            asyncio.run(main())
        """
        if events._get_running_loop() is not None:
            raise RuntimeError(
                "asyncio.run() cannot be called from a running event loop")

        if not coroutines.iscoroutine(main):
>           raise ValueError("a coroutine was expected, got {!r}".format(main))
E           ValueError: a coroutine was expected, got {'error': "Input validation error: a coroutine was expected, got <MagicMock name='mock.tools_manager.execute_tool()' id='5968161200'>", 'meta': {'duration_ms': 100.0, 'request_id': 'mcp-core-1747394548702-1', 'status': 'failed'}}

/opt/homebrew/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/runners.py:37: ValueError
----------------------------- Captured stderr call -----------------------------
2025-05-16 18:22:28,701 [CORE-INIT] INFO     - Initializing MCPAgent Core...
2025-05-16 18:22:28,702 [CORE-INIT] INFO     - MCPAgent Core initialized. Available tools: []
2025-05-16 18:22:28,702 [mcp-core-1747394548702-1] INFO     - Processing single request: save_document
2025-05-16 18:22:28,702 [mcp-core-1747394548702-1] INFO     - [_execute_tool_sync] Received request_data: {'id': 'mcp-core-1747394548702-1', 'tool_name': 'save_document', 'args': {'content': 'test', 'filename': 'f.txt'}}
2025-05-16 18:22:28,702 [mcp-core-1747394548702-1] INFO     - [_execute_tool_sync] Extracted input_payload_from_request: {}
2025-05-16 18:22:28,702 [mcp-core-1747394548702-1] INFO     - [_execute_tool_sync] Type of input_payload_from_request: <class 'dict'>
2025-05-16 18:22:28,702 [mcp-core-1747394548702-1] INFO     - Executing tool 'save_document' with args: [], kwargs: {}
2025-05-16 18:22:28,702 [mcp-core-1747394548702-1] INFO     - Final check before calling execute_tool - Args: [], Kwargs: {}
2025-05-16 18:22:28,702 [mcp-core-1747394548702-1] INFO     - Attempting asyncio.run for tool: save_document
2025-05-16 18:22:28,702 [mcp-core-1747394548702-1] ERROR    - Input validation error: a coroutine was expected, got <MagicMock name='mock.tools_manager.execute_tool()' id='5968161200'> - Request Data: {'id': 'mcp-core-1747394548702-1', 'tool_name': 'save_document', 'args': {'content': 'test', 'filename': 'f.txt'}}
Traceback (most recent call last):
  File "/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/mcp/mcp_agent_core.py", line 140, in _execute_tool_sync
    tool_result_data = asyncio.run(self.core_agent.tools_manager.execute_tool(
  File "/opt/homebrew/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/runners.py", line 37, in run
    raise ValueError("a coroutine was expected, got {!r}".format(main))
ValueError: a coroutine was expected, got <MagicMock name='mock.tools_manager.execute_tool()' id='5968161200'>
2025-05-16 18:22:28,702 [mcp-core-1747394548702-1] ERROR    - Tool 'save_document' failed. Error: Input validation error: a coroutine was expected, got <MagicMock name='mock.tools_manager.execute_tool()' id='5968161200'>
------------------------------ Captured log call -------------------------------
INFO     MCPAgentCore:mcp_agent_core.py:58 Initializing MCPAgent Core...
INFO     MCPAgentCore:mcp_agent_core.py:62 MCPAgent Core initialized. Available tools: []
INFO     MCPAgentCore:mcp_agent_core.py:186 Processing single request: save_document
INFO     MCPAgentCore:mcp_agent_core.py:100 [_execute_tool_sync] Received request_data: {'id': 'mcp-core-1747394548702-1', 'tool_name': 'save_document', 'args': {'content': 'test', 'filename': 'f.txt'}}
INFO     MCPAgentCore:mcp_agent_core.py:107 [_execute_tool_sync] Extracted input_payload_from_request: {}
INFO     MCPAgentCore:mcp_agent_core.py:108 [_execute_tool_sync] Type of input_payload_from_request: <class 'dict'>
INFO     MCPAgentCore:mcp_agent_core.py:133 Executing tool 'save_document' with args: [], kwargs: {}
INFO     MCPAgentCore:mcp_agent_core.py:136 Final check before calling execute_tool - Args: [], Kwargs: {}
INFO     MCPAgentCore:mcp_agent_core.py:138 Attempting asyncio.run for tool: save_document
ERROR    MCPAgentCore:mcp_agent_core.py:154 Input validation error: a coroutine was expected, got <MagicMock name='mock.tools_manager.execute_tool()' id='5968161200'> - Request Data: {'id': 'mcp-core-1747394548702-1', 'tool_name': 'save_document', 'args': {'content': 'test', 'filename': 'f.txt'}}
Traceback (most recent call last):
  File "/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/mcp/mcp_agent_core.py", line 140, in _execute_tool_sync
    tool_result_data = asyncio.run(self.core_agent.tools_manager.execute_tool(
  File "/opt/homebrew/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/runners.py", line 37, in run
    raise ValueError("a coroutine was expected, got {!r}".format(main))
ValueError: a coroutine was expected, got <MagicMock name='mock.tools_manager.execute_tool()' id='5968161200'>
ERROR    MCPAgentCore:mcp_agent_core.py:168 Tool 'save_document' failed. Error: Input validation error: a coroutine was expected, got <MagicMock name='mock.tools_manager.execute_tool()' id='5968161200'>
______________ TestMCPBatchToolsStdio.test_semantic_search_local _______________

self = <ADK.agent_data.tests.test_mcp_agent_batch_tools.TestMCPBatchToolsStdio testMethod=test_semantic_search_local>

    def test_semantic_search_local(self):
        req_id = "semsrch-1"
>       output = self.run_mcp_request({"id": req_id, "tool_name": "semantic_search_local", "args": {"query": "test"}})

ADK/agent_data/tests/test_mcp_agent_batch_tools.py:236:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
ADK/agent_data/tests/test_mcp_agent_batch_tools.py:151: in run_mcp_request
    response = asyncio.run(self.mcp_agent.run(json.loads(input_json.strip())))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

main = {'error': "Input validation error: a coroutine was expected, got <MagicMock name='mock.tools_manager.execute_tool()' id='5971730800'>", 'meta': {'duration_ms': 100.0, 'request_id': 'mcp-core-1747394548710-1', 'status': 'failed'}}

    def run(main, *, debug=None):
        """Execute the coroutine and return the result.

        This function runs the passed coroutine, taking care of
        managing the asyncio event loop and finalizing asynchronous
        generators.

        This function cannot be called when another asyncio event loop is
        running in the same thread.

        If debug is True, the event loop will be run in debug mode.

        This function always creates a new event loop and closes it at the end.
        It should be used as a main entry point for asyncio programs, and should
        ideally only be called once.

        Example:

            async def main():
                await asyncio.sleep(1)
                print('hello')

            asyncio.run(main())
        """
        if events._get_running_loop() is not None:
            raise RuntimeError(
                "asyncio.run() cannot be called from a running event loop")

        if not coroutines.iscoroutine(main):
>           raise ValueError("a coroutine was expected, got {!r}".format(main))
E           ValueError: a coroutine was expected, got {'error': "Input validation error: a coroutine was expected, got <MagicMock name='mock.tools_manager.execute_tool()' id='5971730800'>", 'meta': {'duration_ms': 100.0, 'request_id': 'mcp-core-1747394548710-1', 'status': 'failed'}}

/opt/homebrew/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/runners.py:37: ValueError
----------------------------- Captured stderr call -----------------------------
2025-05-16 18:22:28,709 [CORE-INIT] INFO     - Initializing MCPAgent Core...
2025-05-16 18:22:28,710 [CORE-INIT] INFO     - MCPAgent Core initialized. Available tools: []
2025-05-16 18:22:28,710 [mcp-core-1747394548710-1] INFO     - Processing single request: semantic_search_local
2025-05-16 18:22:28,710 [mcp-core-1747394548710-1] INFO     - [_execute_tool_sync] Received request_data: {'id': 'mcp-core-1747394548710-1', 'tool_name': 'semantic_search_local', 'args': {'query': 'test'}}
2025-05-16 18:22:28,710 [mcp-core-1747394548710-1] INFO     - [_execute_tool_sync] Extracted input_payload_from_request: {}
2025-05-16 18:22:28,710 [mcp-core-1747394548710-1] INFO     - [_execute_tool_sync] Type of input_payload_from_request: <class 'dict'>
2025-05-16 18:22:28,710 [mcp-core-1747394548710-1] INFO     - Executing tool 'semantic_search_local' with args: [], kwargs: {}
2025-05-16 18:22:28,710 [mcp-core-1747394548710-1] INFO     - Final check before calling execute_tool - Args: [], Kwargs: {}
2025-05-16 18:22:28,710 [mcp-core-1747394548710-1] INFO     - Attempting asyncio.run for tool: semantic_search_local
2025-05-16 18:22:28,710 [mcp-core-1747394548710-1] ERROR    - Input validation error: a coroutine was expected, got <MagicMock name='mock.tools_manager.execute_tool()' id='5971730800'> - Request Data: {'id': 'mcp-core-1747394548710-1', 'tool_name': 'semantic_search_local', 'args': {'query': 'test'}}
Traceback (most recent call last):
  File "/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/mcp/mcp_agent_core.py", line 140, in _execute_tool_sync
    tool_result_data = asyncio.run(self.core_agent.tools_manager.execute_tool(
  File "/opt/homebrew/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/runners.py", line 37, in run
    raise ValueError("a coroutine was expected, got {!r}".format(main))
ValueError: a coroutine was expected, got <MagicMock name='mock.tools_manager.execute_tool()' id='5971730800'>
2025-05-16 18:22:28,710 [mcp-core-1747394548710-1] ERROR    - Tool 'semantic_search_local' failed. Error: Input validation error: a coroutine was expected, got <MagicMock name='mock.tools_manager.execute_tool()' id='5971730800'>
------------------------------ Captured log call -------------------------------
INFO     MCPAgentCore:mcp_agent_core.py:58 Initializing MCPAgent Core...
INFO     MCPAgentCore:mcp_agent_core.py:62 MCPAgent Core initialized. Available tools: []
INFO     MCPAgentCore:mcp_agent_core.py:186 Processing single request: semantic_search_local
INFO     MCPAgentCore:mcp_agent_core.py:100 [_execute_tool_sync] Received request_data: {'id': 'mcp-core-1747394548710-1', 'tool_name': 'semantic_search_local', 'args': {'query': 'test'}}
INFO     MCPAgentCore:mcp_agent_core.py:107 [_execute_tool_sync] Extracted input_payload_from_request: {}
INFO     MCPAgentCore:mcp_agent_core.py:108 [_execute_tool_sync] Type of input_payload_from_request: <class 'dict'>
INFO     MCPAgentCore:mcp_agent_core.py:133 Executing tool 'semantic_search_local' with args: [], kwargs: {}
INFO     MCPAgentCore:mcp_agent_core.py:136 Final check before calling execute_tool - Args: [], Kwargs: {}
INFO     MCPAgentCore:mcp_agent_core.py:138 Attempting asyncio.run for tool: semantic_search_local
ERROR    MCPAgentCore:mcp_agent_core.py:154 Input validation error: a coroutine was expected, got <MagicMock name='mock.tools_manager.execute_tool()' id='5971730800'> - Request Data: {'id': 'mcp-core-1747394548710-1', 'tool_name': 'semantic_search_local', 'args': {'query': 'test'}}
Traceback (most recent call last):
  File "/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/mcp/mcp_agent_core.py", line 140, in _execute_tool_sync
    tool_result_data = asyncio.run(self.core_agent.tools_manager.execute_tool(
  File "/opt/homebrew/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/runners.py", line 37, in run
    raise ValueError("a coroutine was expected, got {!r}".format(main))
ValueError: a coroutine was expected, got <MagicMock name='mock.tools_manager.execute_tool()' id='5971730800'>
ERROR    MCPAgentCore:mcp_agent_core.py:168 Tool 'semantic_search_local' failed. Error: Input validation error: a coroutine was expected, got <MagicMock name='mock.tools_manager.execute_tool()' id='5971730800'>
_________________ TestMCPBatchToolsStdio.test_update_metadata __________________

self = <ADK.agent_data.tests.test_mcp_agent_batch_tools.TestMCPBatchToolsStdio testMethod=test_update_metadata>

    def test_update_metadata(self):
        req_id = "upmeta-1"
>       output = self.run_mcp_request({"id": req_id, "tool_name": "update_metadata", "args": {"key": "meta1", "data": {"status": "updated"}}})

ADK/agent_data/tests/test_mcp_agent_batch_tools.py:217:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
ADK/agent_data/tests/test_mcp_agent_batch_tools.py:151: in run_mcp_request
    response = asyncio.run(self.mcp_agent.run(json.loads(input_json.strip())))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

main = {'error': "Input validation error: a coroutine was expected, got <MagicMock name='mock.tools_manager.execute_tool()' id='4380941408'>", 'meta': {'duration_ms': 100.0, 'request_id': 'mcp-core-1747394548718-1', 'status': 'failed'}}

    def run(main, *, debug=None):
        """Execute the coroutine and return the result.

        This function runs the passed coroutine, taking care of
        managing the asyncio event loop and finalizing asynchronous
        generators.

        This function cannot be called when another asyncio event loop is
        running in the same thread.

        If debug is True, the event loop will be run in debug mode.

        This function always creates a new event loop and closes it at the end.
        It should be used as a main entry point for asyncio programs, and should
        ideally only be called once.

        Example:

            async def main():
                await asyncio.sleep(1)
                print('hello')

            asyncio.run(main())
        """
        if events._get_running_loop() is not None:
            raise RuntimeError(
                "asyncio.run() cannot be called from a running event loop")

        if not coroutines.iscoroutine(main):
>           raise ValueError("a coroutine was expected, got {!r}".format(main))
E           ValueError: a coroutine was expected, got {'error': "Input validation error: a coroutine was expected, got <MagicMock name='mock.tools_manager.execute_tool()' id='4380941408'>", 'meta': {'duration_ms': 100.0, 'request_id': 'mcp-core-1747394548718-1', 'status': 'failed'}}

/opt/homebrew/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/runners.py:37: ValueError
----------------------------- Captured stderr call -----------------------------
2025-05-16 18:22:28,716 [CORE-INIT] INFO     - Initializing MCPAgent Core...
2025-05-16 18:22:28,718 [CORE-INIT] INFO     - MCPAgent Core initialized. Available tools: []
2025-05-16 18:22:28,718 [mcp-core-1747394548718-1] INFO     - Processing single request: update_metadata
2025-05-16 18:22:28,718 [mcp-core-1747394548718-1] INFO     - [_execute_tool_sync] Received request_data: {'id': 'mcp-core-1747394548718-1', 'tool_name': 'update_metadata', 'args': {'key': 'meta1', 'data': {'status': 'updated'}}}
2025-05-16 18:22:28,718 [mcp-core-1747394548718-1] INFO     - [_execute_tool_sync] Extracted input_payload_from_request: {}
2025-05-16 18:22:28,718 [mcp-core-1747394548718-1] INFO     - [_execute_tool_sync] Type of input_payload_from_request: <class 'dict'>
2025-05-16 18:22:28,718 [mcp-core-1747394548718-1] INFO     - Executing tool 'update_metadata' with args: [], kwargs: {}
2025-05-16 18:22:28,718 [mcp-core-1747394548718-1] INFO     - Final check before calling execute_tool - Args: [], Kwargs: {}
2025-05-16 18:22:28,718 [mcp-core-1747394548718-1] INFO     - Attempting asyncio.run for tool: update_metadata
2025-05-16 18:22:28,718 [mcp-core-1747394548718-1] ERROR    - Input validation error: a coroutine was expected, got <MagicMock name='mock.tools_manager.execute_tool()' id='4380941408'> - Request Data: {'id': 'mcp-core-1747394548718-1', 'tool_name': 'update_metadata', 'args': {'key': 'meta1', 'data': {'status': 'updated'}}}
Traceback (most recent call last):
  File "/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/mcp/mcp_agent_core.py", line 140, in _execute_tool_sync
    tool_result_data = asyncio.run(self.core_agent.tools_manager.execute_tool(
  File "/opt/homebrew/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/runners.py", line 37, in run
    raise ValueError("a coroutine was expected, got {!r}".format(main))
ValueError: a coroutine was expected, got <MagicMock name='mock.tools_manager.execute_tool()' id='4380941408'>
2025-05-16 18:22:28,718 [mcp-core-1747394548718-1] ERROR    - Tool 'update_metadata' failed. Error: Input validation error: a coroutine was expected, got <MagicMock name='mock.tools_manager.execute_tool()' id='4380941408'>
------------------------------ Captured log call -------------------------------
INFO     MCPAgentCore:mcp_agent_core.py:58 Initializing MCPAgent Core...
INFO     MCPAgentCore:mcp_agent_core.py:62 MCPAgent Core initialized. Available tools: []
INFO     MCPAgentCore:mcp_agent_core.py:186 Processing single request: update_metadata
INFO     MCPAgentCore:mcp_agent_core.py:100 [_execute_tool_sync] Received request_data: {'id': 'mcp-core-1747394548718-1', 'tool_name': 'update_metadata', 'args': {'key': 'meta1', 'data': {'status': 'updated'}}}
INFO     MCPAgentCore:mcp_agent_core.py:107 [_execute_tool_sync] Extracted input_payload_from_request: {}
INFO     MCPAgentCore:mcp_agent_core.py:108 [_execute_tool_sync] Type of input_payload_from_request: <class 'dict'>
INFO     MCPAgentCore:mcp_agent_core.py:133 Executing tool 'update_metadata' with args: [], kwargs: {}
INFO     MCPAgentCore:mcp_agent_core.py:136 Final check before calling execute_tool - Args: [], Kwargs: {}
INFO     MCPAgentCore:mcp_agent_core.py:138 Attempting asyncio.run for tool: update_metadata
ERROR    MCPAgentCore:mcp_agent_core.py:154 Input validation error: a coroutine was expected, got <MagicMock name='mock.tools_manager.execute_tool()' id='4380941408'> - Request Data: {'id': 'mcp-core-1747394548718-1', 'tool_name': 'update_metadata', 'args': {'key': 'meta1', 'data': {'status': 'updated'}}}
Traceback (most recent call last):
  File "/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/mcp/mcp_agent_core.py", line 140, in _execute_tool_sync
    tool_result_data = asyncio.run(self.core_agent.tools_manager.execute_tool(
  File "/opt/homebrew/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/runners.py", line 37, in run
    raise ValueError("a coroutine was expected, got {!r}".format(main))
ValueError: a coroutine was expected, got <MagicMock name='mock.tools_manager.execute_tool()' id='4380941408'>
ERROR    MCPAgentCore:mcp_agent_core.py:168 Tool 'update_metadata' failed. Error: Input validation error: a coroutine was expected, got <MagicMock name='mock.tools_manager.execute_tool()' id='4380941408'>
_______________ TestMCPBatchToolsStdio.test_update_metadata_fail _______________

self = <ADK.agent_data.tests.test_mcp_agent_batch_tools.TestMCPBatchToolsStdio testMethod=test_update_metadata_fail>

    def test_update_metadata_fail(self):
        req_id = "upmeta-fail-1"
>       output = self.run_mcp_request({"id": req_id, "tool_name": "update_metadata", "args": {"key": "fail_key"}})

ADK/agent_data/tests/test_mcp_agent_batch_tools.py:223:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
ADK/agent_data/tests/test_mcp_agent_batch_tools.py:151: in run_mcp_request
    response = asyncio.run(self.mcp_agent.run(json.loads(input_json.strip())))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

main = {'error': "Input validation error: a coroutine was expected, got <MagicMock name='mock.tools_manager.execute_tool()' id='5969619136'>", 'meta': {'duration_ms': 100.0, 'request_id': 'mcp-core-1747394548725-1', 'status': 'failed'}}

    def run(main, *, debug=None):
        """Execute the coroutine and return the result.

        This function runs the passed coroutine, taking care of
        managing the asyncio event loop and finalizing asynchronous
        generators.

        This function cannot be called when another asyncio event loop is
        running in the same thread.

        If debug is True, the event loop will be run in debug mode.

        This function always creates a new event loop and closes it at the end.
        It should be used as a main entry point for asyncio programs, and should
        ideally only be called once.

        Example:

            async def main():
                await asyncio.sleep(1)
                print('hello')

            asyncio.run(main())
        """
        if events._get_running_loop() is not None:
            raise RuntimeError(
                "asyncio.run() cannot be called from a running event loop")

        if not coroutines.iscoroutine(main):
>           raise ValueError("a coroutine was expected, got {!r}".format(main))
E           ValueError: a coroutine was expected, got {'error': "Input validation error: a coroutine was expected, got <MagicMock name='mock.tools_manager.execute_tool()' id='5969619136'>", 'meta': {'duration_ms': 100.0, 'request_id': 'mcp-core-1747394548725-1', 'status': 'failed'}}

/opt/homebrew/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/runners.py:37: ValueError
----------------------------- Captured stderr call -----------------------------
2025-05-16 18:22:28,724 [CORE-INIT] INFO     - Initializing MCPAgent Core...
2025-05-16 18:22:28,725 [CORE-INIT] INFO     - MCPAgent Core initialized. Available tools: []
2025-05-16 18:22:28,725 [mcp-core-1747394548725-1] INFO     - Processing single request: update_metadata
2025-05-16 18:22:28,726 [mcp-core-1747394548725-1] INFO     - [_execute_tool_sync] Received request_data: {'id': 'mcp-core-1747394548725-1', 'tool_name': 'update_metadata', 'args': {'key': 'fail_key'}}
2025-05-16 18:22:28,726 [mcp-core-1747394548725-1] INFO     - [_execute_tool_sync] Extracted input_payload_from_request: {}
2025-05-16 18:22:28,726 [mcp-core-1747394548725-1] INFO     - [_execute_tool_sync] Type of input_payload_from_request: <class 'dict'>
2025-05-16 18:22:28,726 [mcp-core-1747394548725-1] INFO     - Executing tool 'update_metadata' with args: [], kwargs: {}
2025-05-16 18:22:28,726 [mcp-core-1747394548725-1] INFO     - Final check before calling execute_tool - Args: [], Kwargs: {}
2025-05-16 18:22:28,726 [mcp-core-1747394548725-1] INFO     - Attempting asyncio.run for tool: update_metadata
2025-05-16 18:22:28,726 [mcp-core-1747394548725-1] ERROR    - Input validation error: a coroutine was expected, got <MagicMock name='mock.tools_manager.execute_tool()' id='5969619136'> - Request Data: {'id': 'mcp-core-1747394548725-1', 'tool_name': 'update_metadata', 'args': {'key': 'fail_key'}}
Traceback (most recent call last):
  File "/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/mcp/mcp_agent_core.py", line 140, in _execute_tool_sync
    tool_result_data = asyncio.run(self.core_agent.tools_manager.execute_tool(
  File "/opt/homebrew/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/runners.py", line 37, in run
    raise ValueError("a coroutine was expected, got {!r}".format(main))
ValueError: a coroutine was expected, got <MagicMock name='mock.tools_manager.execute_tool()' id='5969619136'>
2025-05-16 18:22:28,726 [mcp-core-1747394548725-1] ERROR    - Tool 'update_metadata' failed. Error: Input validation error: a coroutine was expected, got <MagicMock name='mock.tools_manager.execute_tool()' id='5969619136'>
------------------------------ Captured log call -------------------------------
INFO     MCPAgentCore:mcp_agent_core.py:58 Initializing MCPAgent Core...
INFO     MCPAgentCore:mcp_agent_core.py:62 MCPAgent Core initialized. Available tools: []
INFO     MCPAgentCore:mcp_agent_core.py:186 Processing single request: update_metadata
INFO     MCPAgentCore:mcp_agent_core.py:100 [_execute_tool_sync] Received request_data: {'id': 'mcp-core-1747394548725-1', 'tool_name': 'update_metadata', 'args': {'key': 'fail_key'}}
INFO     MCPAgentCore:mcp_agent_core.py:107 [_execute_tool_sync] Extracted input_payload_from_request: {}
INFO     MCPAgentCore:mcp_agent_core.py:108 [_execute_tool_sync] Type of input_payload_from_request: <class 'dict'>
INFO     MCPAgentCore:mcp_agent_core.py:133 Executing tool 'update_metadata' with args: [], kwargs: {}
INFO     MCPAgentCore:mcp_agent_core.py:136 Final check before calling execute_tool - Args: [], Kwargs: {}
INFO     MCPAgentCore:mcp_agent_core.py:138 Attempting asyncio.run for tool: update_metadata
ERROR    MCPAgentCore:mcp_agent_core.py:154 Input validation error: a coroutine was expected, got <MagicMock name='mock.tools_manager.execute_tool()' id='5969619136'> - Request Data: {'id': 'mcp-core-1747394548725-1', 'tool_name': 'update_metadata', 'args': {'key': 'fail_key'}}
Traceback (most recent call last):
  File "/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/mcp/mcp_agent_core.py", line 140, in _execute_tool_sync
    tool_result_data = asyncio.run(self.core_agent.tools_manager.execute_tool(
  File "/opt/homebrew/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/runners.py", line 37, in run
    raise ValueError("a coroutine was expected, got {!r}".format(main))
ValueError: a coroutine was expected, got <MagicMock name='mock.tools_manager.execute_tool()' id='5969619136'>
ERROR    MCPAgentCore:mcp_agent_core.py:168 Tool 'update_metadata' failed. Error: Input validation error: a coroutine was expected, got <MagicMock name='mock.tools_manager.execute_tool()' id='5969619136'>
________________ TestMCPBatchToolsStdio.test_vectorize_document ________________

self = <ADK.agent_data.tests.test_mcp_agent_batch_tools.TestMCPBatchToolsStdio testMethod=test_vectorize_document>

    def test_vectorize_document(self):
        req_id = "vecdoc-1"
>       output = self.run_mcp_request({"id": req_id, "tool_name": "vectorize_document", "args": {"doc_key": "doc1"}})

ADK/agent_data/tests/test_mcp_agent_batch_tools.py:211:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
ADK/agent_data/tests/test_mcp_agent_batch_tools.py:151: in run_mcp_request
    response = asyncio.run(self.mcp_agent.run(json.loads(input_json.strip())))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

main = {'error': "Input validation error: a coroutine was expected, got <MagicMock name='mock.tools_manager.execute_tool()' id='4380813648'>", 'meta': {'duration_ms': 100.0, 'request_id': 'mcp-core-1747394548733-1', 'status': 'failed'}}

    def run(main, *, debug=None):
        """Execute the coroutine and return the result.

        This function runs the passed coroutine, taking care of
        managing the asyncio event loop and finalizing asynchronous
        generators.

        This function cannot be called when another asyncio event loop is
        running in the same thread.

        If debug is True, the event loop will be run in debug mode.

        This function always creates a new event loop and closes it at the end.
        It should be used as a main entry point for asyncio programs, and should
        ideally only be called once.

        Example:

            async def main():
                await asyncio.sleep(1)
                print('hello')

            asyncio.run(main())
        """
        if events._get_running_loop() is not None:
            raise RuntimeError(
                "asyncio.run() cannot be called from a running event loop")

        if not coroutines.iscoroutine(main):
>           raise ValueError("a coroutine was expected, got {!r}".format(main))
E           ValueError: a coroutine was expected, got {'error': "Input validation error: a coroutine was expected, got <MagicMock name='mock.tools_manager.execute_tool()' id='4380813648'>", 'meta': {'duration_ms': 100.0, 'request_id': 'mcp-core-1747394548733-1', 'status': 'failed'}}

/opt/homebrew/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/runners.py:37: ValueError
----------------------------- Captured stderr call -----------------------------
2025-05-16 18:22:28,732 [CORE-INIT] INFO     - Initializing MCPAgent Core...
2025-05-16 18:22:28,733 [CORE-INIT] INFO     - MCPAgent Core initialized. Available tools: []
2025-05-16 18:22:28,733 [mcp-core-1747394548733-1] INFO     - Processing single request: vectorize_document
2025-05-16 18:22:28,733 [mcp-core-1747394548733-1] INFO     - [_execute_tool_sync] Received request_data: {'id': 'mcp-core-1747394548733-1', 'tool_name': 'vectorize_document', 'args': {'doc_key': 'doc1'}}
2025-05-16 18:22:28,733 [mcp-core-1747394548733-1] INFO     - [_execute_tool_sync] Extracted input_payload_from_request: {}
2025-05-16 18:22:28,733 [mcp-core-1747394548733-1] INFO     - [_execute_tool_sync] Type of input_payload_from_request: <class 'dict'>
2025-05-16 18:22:28,733 [mcp-core-1747394548733-1] INFO     - Executing tool 'vectorize_document' with args: [], kwargs: {}
2025-05-16 18:22:28,733 [mcp-core-1747394548733-1] INFO     - Final check before calling execute_tool - Args: [], Kwargs: {}
2025-05-16 18:22:28,733 [mcp-core-1747394548733-1] INFO     - Attempting asyncio.run for tool: vectorize_document
2025-05-16 18:22:28,734 [mcp-core-1747394548733-1] ERROR    - Input validation error: a coroutine was expected, got <MagicMock name='mock.tools_manager.execute_tool()' id='4380813648'> - Request Data: {'id': 'mcp-core-1747394548733-1', 'tool_name': 'vectorize_document', 'args': {'doc_key': 'doc1'}}
Traceback (most recent call last):
  File "/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/mcp/mcp_agent_core.py", line 140, in _execute_tool_sync
    tool_result_data = asyncio.run(self.core_agent.tools_manager.execute_tool(
  File "/opt/homebrew/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/runners.py", line 37, in run
    raise ValueError("a coroutine was expected, got {!r}".format(main))
ValueError: a coroutine was expected, got <MagicMock name='mock.tools_manager.execute_tool()' id='4380813648'>
2025-05-16 18:22:28,734 [mcp-core-1747394548733-1] ERROR    - Tool 'vectorize_document' failed. Error: Input validation error: a coroutine was expected, got <MagicMock name='mock.tools_manager.execute_tool()' id='4380813648'>
------------------------------ Captured log call -------------------------------
INFO     MCPAgentCore:mcp_agent_core.py:58 Initializing MCPAgent Core...
INFO     MCPAgentCore:mcp_agent_core.py:62 MCPAgent Core initialized. Available tools: []
INFO     MCPAgentCore:mcp_agent_core.py:186 Processing single request: vectorize_document
INFO     MCPAgentCore:mcp_agent_core.py:100 [_execute_tool_sync] Received request_data: {'id': 'mcp-core-1747394548733-1', 'tool_name': 'vectorize_document', 'args': {'doc_key': 'doc1'}}
INFO     MCPAgentCore:mcp_agent_core.py:107 [_execute_tool_sync] Extracted input_payload_from_request: {}
INFO     MCPAgentCore:mcp_agent_core.py:108 [_execute_tool_sync] Type of input_payload_from_request: <class 'dict'>
INFO     MCPAgentCore:mcp_agent_core.py:133 Executing tool 'vectorize_document' with args: [], kwargs: {}
INFO     MCPAgentCore:mcp_agent_core.py:136 Final check before calling execute_tool - Args: [], Kwargs: {}
INFO     MCPAgentCore:mcp_agent_core.py:138 Attempting asyncio.run for tool: vectorize_document
ERROR    MCPAgentCore:mcp_agent_core.py:154 Input validation error: a coroutine was expected, got <MagicMock name='mock.tools_manager.execute_tool()' id='4380813648'> - Request Data: {'id': 'mcp-core-1747394548733-1', 'tool_name': 'vectorize_document', 'args': {'doc_key': 'doc1'}}
Traceback (most recent call last):
  File "/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/mcp/mcp_agent_core.py", line 140, in _execute_tool_sync
    tool_result_data = asyncio.run(self.core_agent.tools_manager.execute_tool(
  File "/opt/homebrew/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/runners.py", line 37, in run
    raise ValueError("a coroutine was expected, got {!r}".format(main))
ValueError: a coroutine was expected, got <MagicMock name='mock.tools_manager.execute_tool()' id='4380813648'>
ERROR    MCPAgentCore:mcp_agent_core.py:168 Tool 'vectorize_document' failed. Error: Input validation error: a coroutine was expected, got <MagicMock name='mock.tools_manager.execute_tool()' id='4380813648'>
______ TestMCPStdioGetRegisteredTools.test_get_registered_tools_mcp_stdio ______

self = <ADK.agent_data.tests.test_mcp_registered_tools.TestMCPStdioGetRegisteredTools testMethod=test_get_registered_tools_mcp_stdio>

    def test_get_registered_tools_mcp_stdio(self):
        """Test the get_registered_tools tool via MCP stdio server."""
        logger.info("Starting test_get_registered_tools_mcp_stdio")

        # Add a small delay to ensure the server is fully ready after setup
        time.sleep(0.5)

        # Check if the server started correctly in setUp
        if not self.process or self.process.poll() is not None:
             self.fail("MCP server process did not start correctly in setUp.")

        # Assuming get_registered_tools is available and takes no input args
        # NOTE: The current local_mcp_server.py has a placeholder dispatch.
        # It might not actually call get_registered_tools yet.
        # This test assumes the server WILL eventually support it.
        request = {
            "tool": "get_registered_tools",
            "input": {},
            "meta": {"request_id": "get-tools-test-1"}
        }

        try:
            self.send_request(request)
            response = self.receive_response()
        except Exception as e:
            # Log stderr before failing
            stderr_output = "".join(iter(self.stderr_queue.get_nowait, None))
            logger.error(f"Exception during test execution. Stderr:\n{stderr_output}")
            self.fail(f"Test execution failed: {e}")

        self.assertIsInstance(response, dict)
        self.assertEqual(response.get("meta", {}).get("request_id"), "get-tools-test-1")

        # Check for success or potential 'Unknown tool' error depending on server implementation
        status = response.get("meta", {}).get("status")
        error = response.get("error")
        result = response.get("result")

        if status == "success":
            self.assertIsInstance(result, list, "Result should be the list returned by the tool.")
            self.assertIn("echo", result) # Check for a basic tool
            self.assertIn("get_registered_tools", result) # Should list itself
            self.assertIsNone(error)
            logger.info(f"Successfully received tool list: {result}")
        elif error and "Unknown tool" in error:
            logger.warning(f"Received 'Unknown tool' error as expected for placeholder server: {error}")
            # This might be the expected outcome until local_mcp_server fully implements tool dispatch
            self.assertEqual(status, "error")
            self.assertIsNone(result)
        else:
>           self.fail(f"Unexpected response status='{status}', error='{error}', result='{result}'")
E           AssertionError: Unexpected response status='error', error='Error in tool 'get_registered_tools': TypeError: get_registered_tools() missing 1 required positional argument: 'agent_context'', result='None'

ADK/agent_data/tests/test_mcp_registered_tools.py:191: AssertionError
------------------------------ Captured log call -------------------------------
WARNING  ADK.agent_data.tests.test_mcp_registered_tools:test_mcp_registered_tools.py:114 MCP Server Stderr:
2025-05-16 18:22:33,768 - INFO - sys.path in Cloud Run at top of register_tools.py: ['/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data', '/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/mcp', '/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents', '/opt/homebrew/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python310.zip', '/opt/homebrew/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python3.10', '/opt/homebrew/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python3.10/lib-dynload', '/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/setup/venv/lib/python3.10/site-packages']
2025-05-16 18:22:33,841 - INFO - Loading faiss.
2025-05-16 18:22:33,864 - INFO - Successfully loaded faiss.
2025-05-16 18:22:33,866 - INFO - Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.
2025-05-16 18:22:34,725 - INFO - OpenAI import successful.
2025-05-16 18:22:34,725 - INFO - FAISS import successful.
2025-05-16 18:22:34,725 - WARNING - OPENAI_API_KEY environment variable not set. Tools requiring OpenAI embeddings will not be available.
2025-05-16 18:22:34,751 - INFO - sys.path in Cloud Run at top of register_tools.py: ['/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data', '/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/mcp', '/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents', '/opt/homebrew/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python310.zip', '/opt/homebrew/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python3.10', '/opt/homebrew/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python3.10/lib-dynload', '/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/setup/venv/lib/python3.10/site-packages']
2025-05-16 18:22:34,754 - INFO - OpenAI import successful.
2025-05-16 18:22:34,754 - INFO - FAISS import successful.
2025-05-16 18:22:34,754 - WARNING - OPENAI_API_KEY environment variable not set. Tools requiring OpenAI embeddings will not be available.
2025-05-16 18:22:34,755 - INFO - Checking dependencies inside get_all_tool_functions: FAISS_AVAILABLE=True, OPENAI_AVAILABLE=True
2025-05-16 18:22:34,755 - INFO - Base local tools collected: ['save_text', 'add_numbers', 'multiply_numbers', 'echo', 'get_registered_tools', 'delay', 'save_document', 'vectorize_document', 'update_metadata', 'query_metadata', 'multi_update_metadata', 'bulk_delete_metadata', 'bulk_update_metadata', 'multi_field_update', 'semantic_search_local', 'find_metadata_by_key', 'semantic_search_metadata', 'semantic_search_by_author', 'semantic_search_by_year', 'semantic_search_by_keyword', 'conditional_search_metadata', 'semantic_search_multiple_fields', 'sort_metadata', 'advanced_semantic_search', 'create_metadata_tree', 'view_metadata_tree', 'delete_metadata_node', 'update_metadata_node', 'depth_first_search', 'rebuild_metadata_tree', 'semantic_search_metadata_tree', 'validate_metadata_tree', 'generate_embedding', 'semantic_similarity_search', 'batch_generate_embeddings', 'semantic_expand_metadata', 'semantic_filter_metadata', 'analyze_metadata_trends', 'aggregate_metadata', 'detect_anomalies', 'metadata_statistics', 'error_tool']
2025-05-16 18:22:34,755 - INFO - FAISS_AVAILABLE is True, attempting to add FAISS tools...
2025-05-16 18:22:34,755 - INFO - FAISS tools added. Current tools: ['save_text', 'add_numbers', 'multiply_numbers', 'echo', 'get_registered_tools', 'delay', 'save_document', 'vectorize_document', 'update_metadata', 'query_metadata', 'multi_update_metadata', 'bulk_delete_metadata', 'bulk_update_metadata', 'multi_field_update', 'semantic_search_local', 'find_metadata_by_key', 'semantic_search_metadata', 'semantic_search_by_author', 'semantic_search_by_year', 'semantic_search_by_keyword', 'conditional_search_metadata', 'semantic_search_multiple_fields', 'sort_metadata', 'advanced_semantic_search', 'create_metadata_tree', 'view_metadata_tree', 'delete_metadata_node', 'update_metadata_node', 'depth_first_search', 'rebuild_metadata_tree', 'semantic_search_metadata_tree', 'validate_metadata_tree', 'generate_embedding', 'semantic_similarity_search', 'batch_generate_embeddings', 'semantic_expand_metadata', 'semantic_filter_metadata', 'analyze_metadata_trends', 'aggregate_metadata', 'detect_anomalies', 'metadata_statistics', 'error_tool', 'save_metadata_to_faiss', 'load_metadata_from_faiss', 'query_metadata_faiss', 'advanced_query_faiss', 'rebuild_metadata_tree_from_faiss']
2025-05-16 18:22:34,755 - INFO - OPENAI_AVAILABLE is True, attempting to add OpenAI tools...
2025-05-16 18:22:34,755 - INFO - OpenAI tools added: ['generate_embedding_real', 'semantic_search_cosine', 'clear_embeddings']
2025-05-16 18:22:34,755 - INFO - Discovered tools: ['save_text', 'add_numbers', 'multiply_numbers', 'echo', 'get_registered_tools', 'delay', 'save_document', 'vectorize_document', 'update_metadata', 'query_metadata', 'multi_update_metadata', 'bulk_delete_metadata', 'bulk_update_metadata', 'multi_field_update', 'semantic_search_local', 'find_metadata_by_key', 'semantic_search_metadata', 'semantic_search_by_author', 'semantic_search_by_year', 'semantic_search_by_keyword', 'conditional_search_metadata', 'semantic_search_multiple_fields', 'sort_metadata', 'advanced_semantic_search', 'create_metadata_tree', 'view_metadata_tree', 'delete_metadata_node', 'update_metadata_node', 'depth_first_search', 'rebuild_metadata_tree', 'semantic_search_metadata_tree', 'validate_metadata_tree', 'generate_embedding', 'semantic_similarity_search', 'batch_generate_embeddings', 'semantic_expand_metadata', 'semantic_filter_metadata', 'analyze_metadata_trends', 'aggregate_metadata', 'detect_anomalies', 'metadata_statistics', 'error_tool', 'save_metadata_to_faiss', 'load_metadata_from_faiss', 'query_metadata_faiss', 'advanced_query_faiss', 'rebuild_metadata_tree_from_faiss', 'generate_embedding_real', 'semantic_search_cosine', 'clear_embeddings']
2025-05-16 18:22:34,755 - INFO - Starting MCP Stdio Server...
2025-05-16 18:22:34,755 - INFO - MCP Stdio Server Started. Waiting for JSON input on stdin...
2025-05-16 18:22:35,252 - INFO - Executing tool 'get_registered_tools' (ID: get-tools-test-1) with input: {}
2025-05-16 18:22:35,252 - ERROR - Error executing tool 'get_registered_tools' (ID: get-tools-test-1): get_registered_tools() missing 1 required positional argument: 'agent_context'
Traceback (most recent call last):
  File "/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/mcp/local_mcp_server.py", line 94, in run_mcp_loop
    tool_result_data = tool_function() # Call without arguments
TypeError: get_registered_tools() missing 1 required positional argument: 'agent_context'
2025-05-16 18:22:35,253 - ERROR - Error executing tool 'get_registered_tools' (ID: get-tools-test-1): get_registered_tools() missing 1 required positional argument: 'agent_context'
Traceback (most recent call last):
  File "/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/mcp/local_mcp_server.py", line 120, in run_mcp_loop
    raise exec_err
  File "/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/mcp/local_mcp_server.py", line 94, in run_mcp_loop
    tool_result_data = tool_function() # Call without arguments
TypeError: get_registered_tools() missing 1 required positional argument: 'agent_context'
2025-05-16 18:22:35,254 - INFO - Exit command received (ID: None). Shutting down.
2025-05-16 18:22:35,254 - INFO - MCP Stdio Server loop finished.
2025-05-16 18:22:35,254 - INFO - MCP Stdio Server Stopped.
__________________________ test_mcp_timeout_handling ___________________________

mcp_agent_instance = <ADK.agent_data.mcp.mcp_agent_core.MCPAgent object at 0x163d0f100>

    @pytest.mark.asyncio
    async def test_mcp_timeout_handling(mcp_agent_instance: MCPAgent):
        """
        Tests MCPAgent's handling of timeouts in batch processing.
        - 20 echo requests (should succeed)
        - 10 delay requests with 1000ms (should succeed)
        - 10 delay requests with 6000ms (should timeout)
        - 10 invalid tool requests (should fail)
        """
        test_status_logger.info("Starting test_mcp_timeout_handling...")
        agent = mcp_agent_instance
        # Access timeout constant directly from the module if it's defined there,
        # or via the agent instance if it holds it. Let's assume it's in the core module based on previous reading.
        try:
            from ADK.agent_data.mcp.mcp_agent_core import TOOL_EXECUTION_TIMEOUT_SECONDS
            timeout_duration_seconds = TOOL_EXECUTION_TIMEOUT_SECONDS
        except ImportError:
            # Fallback or fail if not found - assuming 5s as per requirement
            test_status_logger.warning("Could not import TOOL_EXECUTION_TIMEOUT_SECONDS, assuming 5.0s")
            timeout_duration_seconds = 5.0

        requests = []
        # 1. Successful echo requests
        for i in range(20):
            requests.append({
                "id": f"echo-{i+1}",
                "tool_name": "echo",
                "args": ["test string"]
            })

        # 2. Successful delay requests (< timeout)
        for i in range(10):
            requests.append({
                "id": f"delay-ok-{i+1}",
                "tool_name": "delay",
                "kwargs": {"delay_ms": 1000} # 1 second, less than timeout
            })

        # 3. Timeout delay requests (> timeout)
        delay_timeout_ms = 6000 # 6 seconds
        for i in range(10):
            requests.append({
                "id": f"delay-timeout-{i+1}",
                "tool_name": "delay",
                "kwargs": {"delay_ms": delay_timeout_ms}
            })

        # 4. Invalid tool requests
        for i in range(10):
            requests.append({
                "id": f"invalid-{i+1}",
                "tool_name": "nonexistent_tool",
                "args": []
            })

        assert len(requests) == 50, "Incorrect number of requests generated."

        test_status_logger.info(f"Executing batch of {len(requests)} requests (timeout={timeout_duration_seconds}s)...")
        start_time = time.monotonic()
        # Change to agent.run as MCPAgent has run(self, request_data: Union[Dict, List])
>       results = await agent.run(requests)

ADK/agent_data/tests/test_mcp_timeout.py:171:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

self = <ADK.agent_data.mcp.mcp_agent_core.MCPAgent object at 0x163d0f100>
request_data = [{'args': ['test string'], 'id': 'echo-1', 'tool_name': 'echo'}, {'args': ['test string'], 'id': 'echo-2', 'tool_name'...st string'], 'id': 'echo-5', 'tool_name': 'echo'}, {'args': ['test string'], 'id': 'echo-6', 'tool_name': 'echo'}, ...]
request_id = None

    def run(self, request_data: Dict, request_id: Optional[str] = None) -> Dict:
        """Processes a single tool request synchronously."""
        exec_request_id = self._get_next_request_id(request_id)
>       request_data['id'] = exec_request_id # Ensure ID is set in the input data
E       TypeError: list indices must be integers or slices, not str

ADK/agent_data/mcp/mcp_agent_core.py:183: TypeError
---------------------------- Captured stderr setup -----------------------------
2025-05-16 18:22:35,444 - PytestTimeoutStatus - INFO - Setting up MCPAgent fixture. Redirecting MCPAgentCore logs to /Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/tests/logs/pytest_timeout.log
2025-05-16 18:22:35,446 - PytestTimeoutStatus - INFO - MCPAgent instance created for timeout test.
------------------------------ Captured log setup ------------------------------
INFO     PytestTimeoutStatus:test_mcp_timeout.py:72 Setting up MCPAgent fixture. Redirecting MCPAgentCore logs to /Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/tests/logs/pytest_timeout.log
INFO     MCPAgentCore:mcp_agent_core.py:58 Initializing MCPAgent Core...
INFO     MCPAgentCore:mcp_agent_core.py:62 MCPAgent Core initialized. Available tools: ['save_text', 'add_numbers', 'multiply_numbers', 'echo', 'get_registered_tools', 'delay', 'save_document', 'vectorize_document', 'update_metadata', 'query_metadata', 'multi_update_metadata', 'bulk_delete_metadata', 'bulk_update_metadata', 'multi_field_update', 'semantic_search_local', 'find_metadata_by_key', 'semantic_search_metadata', 'semantic_search_by_author', 'semantic_search_by_year', 'semantic_search_by_keyword', 'conditional_search_metadata', 'semantic_search_multiple_fields', 'sort_metadata', 'advanced_semantic_search', 'create_metadata_tree', 'view_metadata_tree', 'delete_metadata_node', 'update_metadata_node', 'depth_first_search', 'rebuild_metadata_tree', 'semantic_search_metadata_tree', 'validate_metadata_tree', 'generate_embedding', 'semantic_similarity_search', 'batch_generate_embeddings', 'semantic_expand_metadata', 'semantic_filter_metadata', 'analyze_metadata_trends', 'aggregate_metadata', 'detect_anomalies', 'metadata_statistics', 'error_tool', 'save_metadata_to_faiss', 'load_metadata_from_faiss', 'query_metadata_faiss', 'advanced_query_faiss', 'rebuild_metadata_tree_from_faiss', 'generate_embedding_real', 'semantic_search_cosine', 'clear_embeddings']
INFO     PytestTimeoutStatus:test_mcp_timeout.py:93 MCPAgent instance created for timeout test.
----------------------------- Captured stderr call -----------------------------
2025-05-16 18:22:35,447 - PytestTimeoutStatus - INFO - Starting test_mcp_timeout_handling...
2025-05-16 18:22:35,447 - PytestTimeoutStatus - INFO - Executing batch of 50 requests (timeout=10.0s)...
------------------------------ Captured log call -------------------------------
INFO     PytestTimeoutStatus:test_mcp_timeout.py:120 Starting test_mcp_timeout_handling...
INFO     PytestTimeoutStatus:test_mcp_timeout.py:168 Executing batch of 50 requests (timeout=10.0s)...
_________________________ test_log_output_verification _________________________

    @pytest.mark.asyncio
    async def test_log_output_verification():
        """Verifies the content of the generated JSON log file."""
        test_status_logger.info("Verifying log output...")
        log_entries = read_log_file(PYTEST_LOG_FILE)

        assert len(log_entries) > 0, "Log file is empty or could not be read."

        # Example verification: Count log entries for each status
        log_statuses = {
            "success": 0,
            "timeout": 0,
            "failed": 0,
            "error": 0, # If 'error' status is used by the logger
        }
        request_ids_in_log = set()

        for entry in log_entries:
            # Check if the entry contains the keys we expect from our logging format
            assert "request_id" in entry, f"Log entry missing 'request_id': {entry}"
            assert "batch_id" in entry, f"Log entry missing 'batch_id': {entry}"
            req_id = entry["request_id"]
            # Only analyze status for actual request logs, not init/core/batch status messages
            is_request_result_log = req_id not in ['N/A', 'CORE-INIT', 'CORE-MSG'] and "message" not in entry and "tool_name" in entry

            if is_request_result_log:
                assert "status" in entry, f"Log entry for request {req_id} missing 'status': {entry}"
                status = entry.get("status") # Get status safely
                if status in log_statuses:
                    log_statuses[status] += 1
                else:
                     test_status_logger.warning(f"Unknown status '{status}' found in log entry for request {req_id}")
                request_ids_in_log.add(req_id)

        # Expected counts based on the previous test (50 requests processed)
        # Note: Logging might occur multiple times per request (start, end), so counts might differ
        # from the result counts. Let's focus on checking if expected IDs are present with final status.

        # Check if all 50 unique request IDs were logged at least once
        # We generated IDs like echo-1, delay-ok-1, delay-timeout-1, invalid-1
        expected_id_prefixes = ["echo-", "delay-ok-", "delay-timeout-", "invalid-"]
        all_ids_found = True
        for i in range(1, 21):
            if f"echo-{i}" not in request_ids_in_log:
                all_ids_found = False; break
        for i in range(1, 11):
            if f"delay-ok-{i}" not in request_ids_in_log: all_ids_found = False; break
            if f"delay-timeout-{i}" not in request_ids_in_log: all_ids_found = False; break
            if f"invalid-{i}" not in request_ids_in_log: all_ids_found = False; break

>       assert all_ids_found, "Not all expected request IDs were found in the logs."
E       AssertionError: Not all expected request IDs were found in the logs.
E       assert False

ADK/agent_data/tests/test_mcp_timeout.py:386: AssertionError
----------------------------- Captured stderr call -----------------------------
2025-05-16 18:22:35,455 - PytestTimeoutStatus - INFO - Verifying log output...
------------------------------ Captured log call -------------------------------
INFO     PytestTimeoutStatus:test_mcp_timeout.py:339 Verifying log output...
--------------------------- Captured stderr teardown ---------------------------
2025-05-16 18:22:35,459 - PytestTimeoutStatus - INFO - Cleaning up MCPAgent fixture.
2025-05-16 18:22:35,459 - PytestTimeoutStatus - INFO - MCPAgentCore logger restored.
---------------------------- Captured log teardown -----------------------------
INFO     PytestTimeoutStatus:test_mcp_timeout.py:97 Cleaning up MCPAgent fixture.
INFO     PytestTimeoutStatus:test_mcp_timeout.py:107 MCPAgentCore logger restored.
___________ TestSaveMetadataToFaiss.test_save_openai_embedding_error ___________

self = <ADK.agent_data.tests.tools.test_save_metadata_to_faiss.TestSaveMetadataToFaiss object at 0x1639c8730>
MockFaissIndexFlatL2 = <AsyncMock name='get_openai_embedding' id='5970090576'>
mock_get_embedding = <MagicMock name='IndexFlatL2' id='6040389424'>
mock_faiss_write_index = <MagicMock name='write_index' id='6040396768'>
mock_pickle_dump = <MagicMock name='dump' id='4378580432'>
mocker = <pytest_mock.plugin.MockerFixture object at 0x1680a1b10>
request = <FixtureRequest for <Coroutine test_save_openai_embedding_error>>

    @patch(f"{SAVE_TOOL_MODULE_PATH}.openai_client", MagicMock())
    @patch(f"{SAVE_TOOL_MODULE_PATH}.pickle.dump")
    @patch(f"{SAVE_TOOL_MODULE_PATH}.faiss.write_index")
    @patch(f"{SAVE_TOOL_MODULE_PATH}.faiss.IndexFlatL2")
    @patch(f"{SAVE_TOOL_MODULE_PATH}.get_openai_embedding", new_callable=AsyncMock)
    async def test_save_openai_embedding_error(self,
                                               MockFaissIndexFlatL2,
                                               mock_get_embedding,
                                               mock_faiss_write_index,
                                               mock_pickle_dump,
                                               mocker, request):
        # Mock dependencies using mocker
        mock_firestore_constructor = mocker.patch(FIRESTORE_CLIENT_PATH)
        mock_upload_with_retry_local = mocker.patch(UPLOAD_WITH_RETRY_PATH, return_value=MagicMock())
        mock_storage_client = mocker.patch(STORAGE_CLIENT_SAVE_PATH)

        from ADK.agent_data.tools.save_metadata_to_faiss_tool import save_metadata_to_faiss, EmbeddingGenerationError
        # Setup mock instances
        mock_fs_instance = mock_firestore_constructor.return_value
        mock_collection_ref = mock_fs_instance.collection.return_value
        mock_doc_ref = mock_collection_ref.document.return_value

        mock_actual_storage_client_instance = MagicMock(spec=storage.Client)
        mock_storage_client.return_value = mock_actual_storage_client_instance
        mock_bucket = MagicMock()
        mock_blob = MagicMock()
        mock_actual_storage_client_instance.bucket = MagicMock()
        mock_actual_storage_client_instance.bucket.return_value = mock_bucket
        mock_bucket.blob.return_value = mock_blob

        mock_index_instance = MockFaissIndexFlatL2.return_value
        mock_index_instance.ntotal = 1 # For one successful embedding
        mock_get_embedding.side_effect = [
            {"embedding": np.array([0.1]*10, dtype=np.float32), "total_tokens": 5, "status": "success"}, # doc1 succeeds
            EmbeddingGenerationError("Simulated embedding error for doc2") # doc2 fails
        ]

        input_metadata = {
            "doc1": {"text": "Text that will succeed"},
            "doc2": {"text": "Text that will cause embedding error"}
        }
        input_data = {
            "index_name": "test_openai_error_index",
            "metadata_dict": input_metadata,
            "text_field_to_embed": "text",
            "dimension": 10
        }
        result = await save_metadata_to_faiss(
            index_name=input_data["index_name"],
            metadata_dict=input_data["metadata_dict"],
            text_field_to_embed=input_data["text_field_to_embed"],
            dimension=input_data["dimension"]
        )
        print(f"\nResult dictionary for {request.node.name}: {result}\n")
        assert result is not None
        assert isinstance(result, dict)
>       assert result.get("status") == "success"  # Expect success as one doc is processed
E       AssertionError: assert 'error' == 'success'
E
E         - success
E         + error

ADK/agent_data/tests/tools/test_save_metadata_to_faiss.py:383: AssertionError
----------------------------- Captured stdout call -----------------------------

Result dictionary for test_save_openai_embedding_error: {'status': 'error', 'error': "No embeddings could be successfully processed. First detailed error for doc_id 'doc1': No result from embedding function", 'message': "No embeddings could be successfully processed. First detailed error for doc_id 'doc1': No result from embedding function", 'meta': {'error_type': 'EmbeddingGenerationError', 'index_name': 'test_openai_error_index', 'duration_seconds': 0.0008, 'embedding_generation_errors': {'doc1': 'No result from embedding function', 'doc2': 'No result from embedding function'}, 'original_docs_count': 2, 'embedded_docs_count': 0, 'failed_doc_ids': ['doc1', 'doc2']}}

------------------------------ Captured log call -------------------------------
ERROR    ADK.agent_data.tools.save_metadata_to_faiss_tool:save_metadata_to_faiss_tool.py:91 Failed to generate or received invalid embedding for doc_id: doc1. Response: <AsyncMock name='get_openai_embedding()' id='5968263728'>
ERROR    ADK.agent_data.tools.save_metadata_to_faiss_tool:save_metadata_to_faiss_tool.py:91 Failed to generate or received invalid embedding for doc_id: doc2. Response: <AsyncMock name='get_openai_embedding()' id='5968263728'>
ERROR    ADK.agent_data.tools.save_metadata_to_faiss_tool:save_metadata_to_faiss_tool.py:411 Operational error during FAISS processing for index 'test_openai_error_index': EmbeddingGenerationError - No embeddings could be successfully processed. First detailed error for doc_id 'doc1': No result from embedding function
Traceback (most recent call last):
  File "/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/tools/save_metadata_to_faiss_tool.py", line 264, in save_metadata_to_faiss
    raise EmbeddingGenerationError(error_msg)
ADK.agent_data.tools.save_metadata_to_faiss_tool.EmbeddingGenerationError: No embeddings could be successfully processed. First detailed error for doc_id 'doc1': No result from embedding function
___________ TestSaveMetadataToFaiss.test_save_gcs_upload_faiss_fails ___________

self = <ADK.agent_data.tests.tools.test_save_metadata_to_faiss.TestSaveMetadataToFaiss object at 0x1639c8a90>
MockFaissIndexFlatL2 = <MagicMock name='dump.patch()' id='6041214576'>
mock_get_embedding = <AsyncMock name='get_openai_embedding' id='6041232640'>
mock_faiss_write_index = <MagicMock name='IndexFlatL2' id='6041223808'>
mock_pickle_dump = <MagicMock name='write_index' id='6041041456'>
mocker = <MagicMock name='dump' id='6041027920'>
request = <FixtureRequest for <Coroutine test_save_gcs_upload_faiss_fails>>

    @patch(f"{SAVE_TOOL_MODULE_PATH}.openai_client", MagicMock())
    @patch(f"{SAVE_TOOL_MODULE_PATH}.pickle.dump")
    @patch(f"{SAVE_TOOL_MODULE_PATH}.faiss.write_index")
    @patch(f"{SAVE_TOOL_MODULE_PATH}.faiss.IndexFlatL2")
    @patch(f"{SAVE_TOOL_MODULE_PATH}.get_openai_embedding", new_callable=AsyncMock)
    @patch(f"{SAVE_TOOL_MODULE_PATH}.faiss.IndexFlatL2")
    async def test_save_gcs_upload_faiss_fails(self,
                                               MockFaissIndexFlatL2,
                                               mock_get_embedding,
                                               mock_faiss_write_index,
                                               mock_pickle_dump,
                                               mocker, request):
        # Mock dependencies using mocker
        mock_firestore_constructor = mocker.patch(FIRESTORE_CLIENT_PATH)
        mock_upload_with_retry_local = mocker.patch(UPLOAD_WITH_RETRY_PATH)
        # Define a side_effect function that will raise an error for any upload attempt
        def gcs_upload_failure_side_effect(*args, **kwargs):
            raise google_exceptions.NotFound("Mocked GCS upload failed")
        mock_upload_with_retry_local.side_effect = gcs_upload_failure_side_effect
        mock_storage_client = mocker.patch(STORAGE_CLIENT_SAVE_PATH)

        from ADK.agent_data.tools.save_metadata_to_faiss_tool import save_metadata_to_faiss
        # Setup mock instances
        mock_fs_instance = mock_firestore_constructor.return_value
        mock_collection_ref = mock_fs_instance.collection.return_value
        mock_doc_ref = mock_collection_ref.document.return_value

        mock_actual_storage_client_instance = MagicMock(spec=storage.Client)
        mock_storage_client.return_value = mock_actual_storage_client_instance
        mock_bucket = MagicMock()
        mock_blob = MagicMock()
        mock_actual_storage_client_instance.bucket = MagicMock()
        mock_actual_storage_client_instance.bucket.return_value = mock_bucket
        mock_bucket.blob.return_value = mock_blob

        MockFaissIndexFlatL2 = mocker.patch(f"{SAVE_TOOL_MODULE_PATH}.faiss.IndexFlatL2")
        mock_index_instance = MockFaissIndexFlatL2.return_value
        mock_index_instance.ntotal = 1 # One document is processed
        mock_get_embedding.return_value = {"embedding": np.array([0.1]*10, dtype=np.float32), "total_tokens": 5, "status": "success"}

        def upload_side_effect(blob, source_file_name):
            if source_file_name.endswith(".faiss"):
                raise google_exceptions.NotFound("Mocked GCS FAISS upload failed")
            elif source_file_name.endswith(".meta"):
                return None
            raise ValueError(f"Unexpected upload call: {source_file_name}")

        mock_upload_with_retry_local.side_effect = upload_side_effect

        input_data = {
            "index_name": "test_index_gcs_upload_fails",
            "metadata_dict": {
                "doc1": {"text": "Hello world"},
            },
            "text_field_to_embed": "text",
            "dimension": 10
        }
        result = await save_metadata_to_faiss(
            index_name=input_data["index_name"],
            metadata_dict=input_data["metadata_dict"],
            text_field_to_embed=input_data["text_field_to_embed"],
            dimension=input_data["dimension"]
        )
        print(f"\nResult dictionary for {request.node.name}: {result}\n")
        assert result is not None
        assert isinstance(result, dict)
        assert result.get("status") == "error"
        # The tool structure for GCS error returns error in "message" and "meta.error_type"
        assert "message" in result
>       assert "Mocked GCS FAISS upload failed" in result["message"]
E       assert 'Mocked GCS FAISS upload failed' in "The configured universe domain (googleapis.com) does not match the universe domain found in the credentials (<MagicMock name='mock.universe_domain' id='6040916400'>). If you haven't configured the universe domain explicitly, `googleapis.com` is the default."

ADK/agent_data/tests/tools/test_save_metadata_to_faiss.py:480: AssertionError
----------------------------- Captured stdout call -----------------------------

Result dictionary for test_save_gcs_upload_faiss_fails: {'status': 'error', 'error': "The configured universe domain (googleapis.com) does not match the universe domain found in the credentials (<MagicMock name='mock.universe_domain' id='6040916400'>). If you haven't configured the universe domain explicitly, `googleapis.com` is the default.", 'message': "The configured universe domain (googleapis.com) does not match the universe domain found in the credentials (<MagicMock name='mock.universe_domain' id='6040916400'>). If you haven't configured the universe domain explicitly, `googleapis.com` is the default.", 'meta': {'error_type': 'ValueError', 'index_name': 'test_index_gcs_upload_fails', 'duration_seconds': 0.0019, 'original_docs_count': 1, 'embedded_docs_count': 1, 'failed_doc_ids': []}}

------------------------------ Captured log call -------------------------------
ERROR    ADK.agent_data.tools.save_metadata_to_faiss_tool:save_metadata_to_faiss_tool.py:411 Operational error during FAISS processing for index 'test_index_gcs_upload_fails': ValueError - The configured universe domain (googleapis.com) does not match the universe domain found in the credentials (<MagicMock name='mock.universe_domain' id='6040916400'>). If you haven't configured the universe domain explicitly, `googleapis.com` is the default.
Traceback (most recent call last):
  File "/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/tools/save_metadata_to_faiss_tool.py", line 291, in save_metadata_to_faiss
    storage_client = storage.Client(project=FIRESTORE_PROJECT_ID) # Use consistent project ID
  File "/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/setup/venv/lib/python3.10/site-packages/google/cloud/storage/client.py", line 238, in __init__
    raise ValueError(
ValueError: The configured universe domain (googleapis.com) does not match the universe domain found in the credentials (<MagicMock name='mock.universe_domain' id='6040916400'>). If you haven't configured the universe domain explicitly, `googleapis.com` is the default.
___________ TestSaveMetadataToFaiss.test_save_gcs_upload_meta_fails ____________

self = <ADK.agent_data.tests.tools.test_save_metadata_to_faiss.TestSaveMetadataToFaiss object at 0x1639c8e50>
mock_get_embedding = <MagicMock name='IndexFlatL2' id='6009839696'>
mock_faiss_write_index = <AsyncMock name='get_openai_embedding' id='6009846272'>
mock_pickle_dump = <MagicMock name='IndexFlatL2' id='5995636352'>
mocker = <MagicMock name='write_index' id='5995633136'>
request = <MagicMock name='dump' id='5991830416'>

    @patch(f"{SAVE_TOOL_MODULE_PATH}.openai_client", MagicMock())
    @patch(f"{SAVE_TOOL_MODULE_PATH}.pickle.dump")
    @patch(f"{SAVE_TOOL_MODULE_PATH}.faiss.write_index")
    @patch(f"{SAVE_TOOL_MODULE_PATH}.faiss.IndexFlatL2")
    @patch(f"{SAVE_TOOL_MODULE_PATH}.get_openai_embedding", new_callable=AsyncMock)
    @patch(f"{SAVE_TOOL_MODULE_PATH}.faiss.IndexFlatL2")
    async def test_save_gcs_upload_meta_fails(self,
                                              mock_get_embedding,
                                              mock_faiss_write_index,
                                              mock_pickle_dump,
                                              mocker, request):
        # Mock dependencies using mocker
        mock_firestore_constructor = mocker.patch(FIRESTORE_CLIENT_PATH)
        mock_upload_with_retry_local = mocker.patch(UPLOAD_WITH_RETRY_PATH)
        mock_storage_client = mocker.patch(STORAGE_CLIENT_SAVE_PATH)

        from ADK.agent_data.tools.save_metadata_to_faiss_tool import save_metadata_to_faiss
        # Setup mock instances
        mock_fs_instance = mock_firestore_constructor.return_value
        mock_collection_ref = mock_fs_instance.collection.return_value
        mock_doc_ref = mock_collection_ref.document.return_value

        mock_actual_storage_client_instance = MagicMock(spec=storage.Client)
        mock_storage_client.return_value = mock_actual_storage_client_instance
        mock_bucket = MagicMock()
        mock_blob = MagicMock()
        mock_actual_storage_client_instance.bucket = MagicMock()
        mock_actual_storage_client_instance.bucket.return_value = mock_bucket
        mock_bucket.blob.return_value = mock_blob

        MockFaissIndexFlatL2 = mocker.patch(f"{SAVE_TOOL_MODULE_PATH}.faiss.IndexFlatL2")
        mock_index_instance = MockFaissIndexFlatL2.return_value
        mock_index_instance.ntotal = 1 # One document is processed
        mock_get_embedding.return_value = {"embedding": np.array([0.1]*10, dtype=np.float32), "total_tokens": 5, "status": "success"}

        def upload_side_effect_meta_fail(blob, source_file_name):
            if source_file_name.endswith(".meta"):
                raise google_exceptions.NotFound("Mocked GCS META upload failed")
            elif source_file_name.endswith(".faiss"):
                return None
            raise ValueError(f"Unexpected upload call: {source_file_name}")

        mock_upload_with_retry_local.side_effect = upload_side_effect_meta_fail

        input_data = {
            "index_name": "test_index_gcs_meta_upload_fails",
            "metadata_dict": {
                "doc1": {"text": "Hello world for meta fail"},
            },
            "text_field_to_embed": "text",
            "dimension": 10
        }
        result = await save_metadata_to_faiss(
            index_name=input_data["index_name"],
            metadata_dict=input_data["metadata_dict"],
            text_field_to_embed=input_data["text_field_to_embed"],
            dimension=input_data["dimension"]
        )
        print(f"\nResult dictionary for {request.node.name}: {result}\n")
        assert result is not None
        assert result.get("status") == "error"
        assert "message" in result
>       assert "Mocked GCS META upload failed" in result["message"]
E       assert 'Mocked GCS META upload failed' in "No embeddings could be successfully processed. First detailed error for doc_id 'doc1': No result from embedding function"

ADK/agent_data/tests/tools/test_save_metadata_to_faiss.py:552: AssertionError
----------------------------- Captured stdout call -----------------------------

Result dictionary for <MagicMock name='dump.node.name' id='5969567872'>: {'status': 'error', 'error': "No embeddings could be successfully processed. First detailed error for doc_id 'doc1': No result from embedding function", 'message': "No embeddings could be successfully processed. First detailed error for doc_id 'doc1': No result from embedding function", 'meta': {'error_type': 'EmbeddingGenerationError', 'index_name': 'test_index_gcs_meta_upload_fails', 'duration_seconds': 0.0011, 'embedding_generation_errors': {'doc1': 'No result from embedding function'}, 'original_docs_count': 1, 'embedded_docs_count': 0, 'failed_doc_ids': ['doc1']}}

------------------------------ Captured log call -------------------------------
ERROR    ADK.agent_data.tools.save_metadata_to_faiss_tool:save_metadata_to_faiss_tool.py:91 Failed to generate or received invalid embedding for doc_id: doc1. Response: <AsyncMock name='get_openai_embedding()' id='5970358576'>
ERROR    ADK.agent_data.tools.save_metadata_to_faiss_tool:save_metadata_to_faiss_tool.py:411 Operational error during FAISS processing for index 'test_index_gcs_meta_upload_fails': EmbeddingGenerationError - No embeddings could be successfully processed. First detailed error for doc_id 'doc1': No result from embedding function
Traceback (most recent call last):
  File "/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/tools/save_metadata_to_faiss_tool.py", line 264, in save_metadata_to_faiss
    raise EmbeddingGenerationError(error_msg)
ADK.agent_data.tools.save_metadata_to_faiss_tool.EmbeddingGenerationError: No embeddings could be successfully processed. First detailed error for doc_id 'doc1': No result from embedding function
___________ TestSaveMetadataToFaiss.test_save_firestore_update_fails ___________

self = <ADK.agent_data.tests.tools.test_save_metadata_to_faiss.TestSaveMetadataToFaiss object at 0x1639c8fa0>
MockFaissIndexFlatL2 = <MagicMock name='IndexFlatL2' id='5995981952'>
mock_get_embedding = <AsyncMock name='get_openai_embedding' id='5995984400'>
mock_faiss_write_index = <MagicMock name='IndexFlatL2' id='5972047424'>
mock_pickle_dump = <MagicMock name='write_index' id='5972046656'>
mocker = <MagicMock name='dump' id='6010194720'>
request = <FixtureRequest for <Coroutine test_save_firestore_update_fails>>

    @patch(f"{SAVE_TOOL_MODULE_PATH}.openai_client", MagicMock())
    @patch(f"{SAVE_TOOL_MODULE_PATH}.pickle.dump")
    @patch(f"{SAVE_TOOL_MODULE_PATH}.faiss.write_index")
    @patch(f"{SAVE_TOOL_MODULE_PATH}.faiss.IndexFlatL2")
    @patch(f"{SAVE_TOOL_MODULE_PATH}.get_openai_embedding", new_callable=AsyncMock)
    @patch(f"{SAVE_TOOL_MODULE_PATH}.faiss.IndexFlatL2")
    async def test_save_firestore_update_fails(self,
                                               MockFaissIndexFlatL2,
                                               mock_get_embedding,
                                               mock_faiss_write_index,
                                               mock_pickle_dump,
                                               mocker, request):
        # Mock dependencies
        mock_firestore_constructor = mocker.patch(FIRESTORE_CLIENT_PATH)
        mock_upload_with_retry_local = mocker.patch(UPLOAD_WITH_RETRY_PATH, return_value=MagicMock()) # GCS succeeds
        mock_storage_client = mocker.patch(STORAGE_CLIENT_SAVE_PATH)

        from ADK.agent_data.tools.save_metadata_to_faiss_tool import save_metadata_to_faiss

        # Setup mock instances
        mock_fs_instance = mock_firestore_constructor.return_value
        mock_collection_ref = mock_fs_instance.collection.return_value
        mock_doc_ref = mock_collection_ref.document.return_value
        # Simulate Firestore .set() failure
        mock_doc_ref.set.side_effect = api_core_exceptions.Aborted("Mocked Firestore update failed")

        mock_actual_storage_client_instance = MagicMock(spec=storage.Client)
        mock_storage_client.return_value = mock_actual_storage_client_instance
        mock_bucket = MagicMock()
        mock_blob = MagicMock()
        mock_actual_storage_client_instance.bucket = MagicMock()
        mock_actual_storage_client_instance.bucket.return_value = mock_bucket
        mock_bucket.blob.return_value = mock_blob

        mock_index_instance = MockFaissIndexFlatL2.return_value
        mock_index_instance.ntotal = 1
        mock_get_embedding.return_value = {"embedding": np.array([0.1]*10, dtype=np.float32), "total_tokens": 5, "status": "success"}

        input_data = {
            "index_name": "test_firestore_fail_index",
            "metadata_dict": {"doc1": {"text": "Firestore test text"}},
            "text_field_to_embed": "text",
            "dimension": 10
        }

        result = await save_metadata_to_faiss(**input_data)

        print(f"\nResult dictionary for {request.node.name}: {result}\n")
        assert result is not None
>       assert result.get("status") == "partial_success" # Corrected status
E       AssertionError: assert 'error' == 'partial_success'
E
E         - partial_success
E         + error

ADK/agent_data/tests/tools/test_save_metadata_to_faiss.py:616: AssertionError
----------------------------- Captured stdout call -----------------------------

Result dictionary for test_save_firestore_update_fails: {'status': 'error', 'error': "The configured universe domain (googleapis.com) does not match the universe domain found in the credentials (<MagicMock name='mock.universe_domain' id='4380946112'>). If you haven't configured the universe domain explicitly, `googleapis.com` is the default.", 'message': "The configured universe domain (googleapis.com) does not match the universe domain found in the credentials (<MagicMock name='mock.universe_domain' id='4380946112'>). If you haven't configured the universe domain explicitly, `googleapis.com` is the default.", 'meta': {'error_type': 'ValueError', 'index_name': 'test_firestore_fail_index', 'duration_seconds': 0.001, 'original_docs_count': 1, 'embedded_docs_count': 1, 'failed_doc_ids': []}}

------------------------------ Captured log call -------------------------------
ERROR    ADK.agent_data.tools.save_metadata_to_faiss_tool:save_metadata_to_faiss_tool.py:411 Operational error during FAISS processing for index 'test_firestore_fail_index': ValueError - The configured universe domain (googleapis.com) does not match the universe domain found in the credentials (<MagicMock name='mock.universe_domain' id='4380946112'>). If you haven't configured the universe domain explicitly, `googleapis.com` is the default.
Traceback (most recent call last):
  File "/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/tools/save_metadata_to_faiss_tool.py", line 291, in save_metadata_to_faiss
    storage_client = storage.Client(project=FIRESTORE_PROJECT_ID) # Use consistent project ID
  File "/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/setup/venv/lib/python3.10/site-packages/google/cloud/storage/client.py", line 238, in __init__
    raise ValueError(
ValueError: The configured universe domain (googleapis.com) does not match the universe domain found in the credentials (<MagicMock name='mock.universe_domain' id='4380946112'>). If you haven't configured the universe domain explicitly, `googleapis.com` is the default.
__________ TestSaveMetadataToFaiss.test_save_temp_file_deletion_fails __________

self = <ADK.agent_data.tests.tools.test_save_metadata_to_faiss.TestSaveMetadataToFaiss object at 0x1639c96c0>
mock_os_remove = <MagicMock name='Client' id='5971618752'>
mock_os_path_exists = <MagicMock name='upload_with_retry' id='5993268848'>
mock_pickle_dump = <MagicMock name='Client' id='6010250208'>
MockFaissIndexFlatL2 = <AsyncMock name='get_openai_embedding' id='6010253424'>
mock_faiss_write_index = <MagicMock name='write_index' id='5991979408'>
mock_get_embedding = <MagicMock name='IndexFlatL2' id='6010726112'>
mock_firestore_constructor = <MagicMock name='dump' id='6010730720'>
mock_upload_with_retry = <MagicMock name='exists' id='5992937936'>
mock_storage_client_constructor = <MagicMock name='remove' id='5992930736'>
mocker = <pytest_mock.plugin.MockerFixture object at 0x163d9ccd0>
request = <FixtureRequest for <Coroutine test_save_temp_file_deletion_fails>>

    @patch(f"{SAVE_TOOL_MODULE_PATH}.os.remove")
    @patch(f"{SAVE_TOOL_MODULE_PATH}.os.path.exists", return_value=True)
    @patch(f"{SAVE_TOOL_MODULE_PATH}.openai_client", MagicMock())
    @patch(f"{SAVE_TOOL_MODULE_PATH}.pickle.dump")
    @patch(f"{SAVE_TOOL_MODULE_PATH}.faiss.IndexFlatL2")
    @patch(f"{SAVE_TOOL_MODULE_PATH}.faiss.write_index")
    @patch(f"{SAVE_TOOL_MODULE_PATH}.get_openai_embedding", new_callable=AsyncMock)
    @patch(FIRESTORE_CLIENT_PATH)
    @patch(UPLOAD_WITH_RETRY_PATH)
    @patch(STORAGE_CLIENT_SAVE_PATH)
    async def test_save_temp_file_deletion_fails(self,
                                                 mock_os_remove,
                                                 mock_os_path_exists,
                                                 mock_pickle_dump,
                                                 MockFaissIndexFlatL2,
                                                 mock_faiss_write_index,
                                                 mock_get_embedding,
                                                 mock_firestore_constructor,
                                                 mock_upload_with_retry,
                                                 mock_storage_client_constructor,
                                                 mocker, request):
        """Test that the tool logs a warning but otherwise succeeds if temp file deletion fails."""
        from ADK.agent_data.tools.save_metadata_to_faiss_tool import save_metadata_to_faiss

        mock_fs_instance = mock_firestore_constructor.return_value
        mock_doc_ref = mock_fs_instance.collection.return_value.document.return_value
        mock_storage_client_instance = mock_storage_client_constructor.return_value
        mock_bucket_instance = mock_storage_client_instance.bucket.return_value
        mock_blob_instance = mock_bucket_instance.blob.return_value
        mock_upload_with_retry.return_value = MagicMock()

        mock_index_instance = MockFaissIndexFlatL2.return_value
        mock_index_instance.ntotal = 1
        mock_get_embedding.return_value = {"embedding": np.array([0.1]*10, dtype=np.float32), "total_tokens": 5, "status": "success"}

        mock_os_remove.side_effect = OSError("Mocked os.remove failed")
        mock_log_warning = mocker.patch(f"{SAVE_TOOL_MODULE_PATH}.logging.warning")

        input_data = {
            "index_name": "test_temp_delete_fail_index",
            "metadata_dict": {"doc1": {"text": "Successful run text"}},
            "text_field_to_embed": "text",
            "dimension": 10
        }

        result = await save_metadata_to_faiss(**input_data)

        print(f"\nResult dictionary for {request.node.name}: {result}\n")
        assert result is not None
>       assert result.get("status") == "success"
E       AssertionError: assert 'error' == 'success'
E
E         - success
E         + error

ADK/agent_data/tests/tools/test_save_metadata_to_faiss.py:847: AssertionError
----------------------------- Captured stdout call -----------------------------

Result dictionary for test_save_temp_file_deletion_fails: {'status': 'error', 'error': "No embeddings could be successfully processed. First detailed error for doc_id 'doc1': No result from embedding function", 'message': "No embeddings could be successfully processed. First detailed error for doc_id 'doc1': No result from embedding function", 'meta': {'error_type': 'EmbeddingGenerationError', 'index_name': 'test_temp_delete_fail_index', 'duration_seconds': 0.0012, 'embedding_generation_errors': {'doc1': 'No result from embedding function'}, 'original_docs_count': 1, 'embedded_docs_count': 0, 'failed_doc_ids': ['doc1']}}

------------------------------ Captured log call -------------------------------
ERROR    ADK.agent_data.tools.save_metadata_to_faiss_tool:save_metadata_to_faiss_tool.py:91 Failed to generate or received invalid embedding for doc_id: doc1. Response: <AsyncMock name='get_openai_embedding()' id='5972408880'>
ERROR    ADK.agent_data.tools.save_metadata_to_faiss_tool:save_metadata_to_faiss_tool.py:411 Operational error during FAISS processing for index 'test_temp_delete_fail_index': EmbeddingGenerationError - No embeddings could be successfully processed. First detailed error for doc_id 'doc1': No result from embedding function
Traceback (most recent call last):
  File "/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/tools/save_metadata_to_faiss_tool.py", line 264, in save_metadata_to_faiss
    raise EmbeddingGenerationError(error_msg)
ADK.agent_data.tools.save_metadata_to_faiss_tool.EmbeddingGenerationError: No embeddings could be successfully processed. First detailed error for doc_id 'doc1': No result from embedding function
___________ TestSaveMetadataToFaiss.test_save_openai_embedding_error ___________

self = <MagicMock name='dump' id='5972310912'>, args = ()
kwargs = {'agent_context': None, 'text_to_embed': 'Text that will succeed'}
expected = call(agent_context=None, text_to_embed='Text that will succeed')
cause = None, actual = []
expected_string = "dump(agent_context=None, text_to_embed='Text that will succeed')"

    def assert_any_call(self, /, *args, **kwargs):
        """assert the mock has been called with the specified arguments.

        The assert passes if the mock has *ever* been called, unlike
        `assert_called_with` and `assert_called_once_with` that only pass if
        the call is the most recent one."""
        expected = self._call_matcher(_Call((args, kwargs), two=True))
        cause = expected if isinstance(expected, Exception) else None
        actual = [self._call_matcher(c) for c in self.call_args_list]
        if cause or expected not in _AnyComparer(actual):
            expected_string = self._format_mock_call_signature(args, kwargs)
>           raise AssertionError(
                '%s call not found' % expected_string
            ) from cause
E           AssertionError: dump(agent_context=None, text_to_embed='Text that will succeed') call not found

/opt/homebrew/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python3.10/unittest/mock.py:1000: AssertionError

During handling of the above exception, another exception occurred:

self = <test_save_metadata_to_faiss_prompt20_backup.TestSaveMetadataToFaiss object at 0x163b22830>
MockFaissIndexFlatL2 = <MagicMock name='IndexFlatL2' id='6041721952'>
mock_pickle_dump = <AsyncMock name='get_openai_embedding' id='6041477872'>
mock_faiss_write_index = <MagicMock name='write_index' id='6041483584'>
mock_get_embedding = <MagicMock name='dump' id='5972310912'>
mocker = <pytest_mock.plugin.MockerFixture object at 0x1682459c0>
request = <FixtureRequest for <Coroutine test_save_openai_embedding_error>>

    @patch(f"{SAVE_TOOL_MODULE_PATH}.openai_client", MagicMock())
    @patch(f"{SAVE_TOOL_MODULE_PATH}.pickle.dump")
    @patch(f"{SAVE_TOOL_MODULE_PATH}.faiss.write_index")
    @patch(f"{SAVE_TOOL_MODULE_PATH}.get_openai_embedding", new_callable=AsyncMock)
    @patch(f"{SAVE_TOOL_MODULE_PATH}.faiss.IndexFlatL2")
    async def test_save_openai_embedding_error(self,
                                               MockFaissIndexFlatL2,
                                               mock_pickle_dump,
                                               mock_faiss_write_index,
                                               mock_get_embedding,
                                               mocker, request):
        # Mock dependencies using mocker
        mock_firestore_constructor = mocker.patch(FIRESTORE_CLIENT_PATH)
        mock_upload_with_retry_local = mocker.patch(UPLOAD_WITH_RETRY_PATH, return_value=MagicMock())
        mock_storage_client = mocker.patch(STORAGE_CLIENT_SAVE_PATH)

        from ADK.agent_data.tools.save_metadata_to_faiss_tool import save_metadata_to_faiss, EmbeddingGenerationError
        # Setup mock instances
        mock_fs_instance = mock_firestore_constructor.return_value
        mock_collection_ref = mock_fs_instance.collection.return_value
        mock_doc_ref = mock_collection_ref.document.return_value

        mock_actual_storage_client_instance = MagicMock(spec=storage.Client)
        mock_storage_client.return_value = mock_actual_storage_client_instance
        mock_bucket = MagicMock()
        mock_blob = MagicMock()
        mock_actual_storage_client_instance.bucket = MagicMock()
        mock_actual_storage_client_instance.bucket.return_value = mock_bucket
        mock_bucket.blob.return_value = mock_blob

        mock_index_instance = MockFaissIndexFlatL2.return_value
        mock_index_instance.ntotal = 1 # For one successful embedding
        mock_get_embedding.side_effect = [
            {"embedding": np.array([0.1]*10, dtype=np.float32), "total_tokens": 5, "status": "success"}, # doc1 succeeds
            EmbeddingGenerationError("Simulated embedding error for doc2") # doc2 fails
        ]

        input_metadata = {
            "doc1": {"text": "Text that will succeed"},
            "doc2": {"text": "Text that will cause embedding error"}
        }
        input_data = {
            "index_name": "test_openai_error_index",
            "metadata_dict": input_metadata,
            "text_field_to_embed": "text",
            "dimension": 10
        }
        result = await save_metadata_to_faiss(
            index_name=input_data["index_name"],
            metadata_dict=input_data["metadata_dict"],
            text_field_to_embed=input_data["text_field_to_embed"],
            dimension=input_data["dimension"]
        )
        print(f"\nResult dictionary for {request.node.name}: {result}\n")
        assert result is not None
        assert isinstance(result, dict)
        assert result.get("status") == "error"
        assert "No embeddings could be successfully processed" in result.get("message", "")
        assert result.get("meta", {}).get("embedded_docs_count") == 0
        assert result.get("index_name") is None
        assert result.get("meta", {}).get("index_name") == input_data["index_name"]
        assert "embedding_generation_errors" in result["meta"]
        assert set(result.get("meta", {}).get("failed_doc_ids", [])) == {"doc1", "doc2"}
        assert result.get("meta", {}).get("error_type") == "EmbeddingGenerationError"

>       mock_get_embedding.assert_any_call(agent_context=None, text_to_embed="Text that will succeed")
E       AssertionError: dump(agent_context=None, text_to_embed='Text that will succeed') call not found

test_pass_56/test_save_metadata_to_faiss_prompt20_backup.py:392: AssertionError
----------------------------- Captured stdout call -----------------------------

Result dictionary for test_save_openai_embedding_error: {'status': 'error', 'error': "No embeddings could be successfully processed. First detailed error for doc_id 'doc1': No result from embedding function", 'message': "No embeddings could be successfully processed. First detailed error for doc_id 'doc1': No result from embedding function", 'meta': {'error_type': 'EmbeddingGenerationError', 'index_name': 'test_openai_error_index', 'duration_seconds': 0.001, 'embedding_generation_errors': {'doc1': 'No result from embedding function', 'doc2': 'No result from embedding function'}, 'original_docs_count': 2, 'embedded_docs_count': 0, 'failed_doc_ids': ['doc1', 'doc2']}}

------------------------------ Captured log call -------------------------------
ERROR    ADK.agent_data.tools.save_metadata_to_faiss_tool:save_metadata_to_faiss_tool.py:91 Failed to generate or received invalid embedding for doc_id: doc1. Response: <AsyncMock name='get_openai_embedding()' id='6041870752'>
ERROR    ADK.agent_data.tools.save_metadata_to_faiss_tool:save_metadata_to_faiss_tool.py:91 Failed to generate or received invalid embedding for doc_id: doc2. Response: <AsyncMock name='get_openai_embedding()' id='6041870752'>
ERROR    ADK.agent_data.tools.save_metadata_to_faiss_tool:save_metadata_to_faiss_tool.py:411 Operational error during FAISS processing for index 'test_openai_error_index': EmbeddingGenerationError - No embeddings could be successfully processed. First detailed error for doc_id 'doc1': No result from embedding function
Traceback (most recent call last):
  File "/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/tools/save_metadata_to_faiss_tool.py", line 264, in save_metadata_to_faiss
    raise EmbeddingGenerationError(error_msg)
ADK.agent_data.tools.save_metadata_to_faiss_tool.EmbeddingGenerationError: No embeddings could be successfully processed. First detailed error for doc_id 'doc1': No result from embedding function
_________ TestSaveMetadataToFaiss.test_save_malformed_embedding_result _________

self = <test_save_metadata_to_faiss_prompt20_backup.TestSaveMetadataToFaiss object at 0x163b30220>
mock_storage_client_constructor = <MagicMock name='Client' id='5968105616'>
mock_upload_with_retry = <MagicMock name='dump' id='5969695056'>
mock_firestore_constructor = <MagicMock name='write_index' id='5969699520'>
mock_get_embedding = <MagicMock name='IndexFlatL2' id='5971419168'>
mock_faiss_write_index = <AsyncMock name='get_openai_embedding' id='5971416240'>
MockFaissIndexFlatL2 = <MagicMock name='write_index' id='6010195440'>
mock_pickle_dump = <MagicMock name='dump' id='6040720896'>
mocker = <pytest_mock.plugin.MockerFixture object at 0x163b51ae0>
request = <FixtureRequest for <Coroutine test_save_malformed_embedding_result>>

    @patch(f"{SAVE_TOOL_MODULE_PATH}.openai_client", MagicMock())
    @patch(f"{SAVE_TOOL_MODULE_PATH}.pickle.dump")
    @patch(f"{SAVE_TOOL_MODULE_PATH}.faiss.write_index")
    @patch(f"{SAVE_TOOL_MODULE_PATH}.get_openai_embedding", new_callable=AsyncMock)
    @patch(f"{SAVE_TOOL_MODULE_PATH}.faiss.IndexFlatL2")
    @patch(f"{SAVE_TOOL_MODULE_PATH}.faiss.write_index")
    @patch(f"{SAVE_TOOL_MODULE_PATH}.pickle.dump")
    @patch(FIRESTORE_CLIENT_PATH)
    async def test_save_malformed_embedding_result(self,
                                                 mock_storage_client_constructor,
                                                 mock_upload_with_retry,
                                                 mock_firestore_constructor,
                                                 mock_get_embedding,
                                                 mock_faiss_write_index,
                                                 MockFaissIndexFlatL2,
                                                 mock_pickle_dump,
                                                 mocker, request):
        """Test handling of malformed (but not exception-raising) embedding results."""
        from ADK.agent_data.tools.save_metadata_to_faiss_tool import save_metadata_to_faiss

        mock_fs_instance = mock_firestore_constructor.return_value
        mock_doc_ref = mock_fs_instance.collection.return_value.document.return_value
        mock_storage_client_instance = mock_storage_client_constructor.return_value
        mock_bucket_instance = mock_storage_client_instance.bucket.return_value
        mock_blob_instance = mock_bucket_instance.blob.return_value
        mock_upload_with_retry.return_value = MagicMock()

        mock_index_instance = MockFaissIndexFlatL2.return_value

        mock_get_embedding.side_effect = [
            {"embedding": np.array([0.1]*10, dtype=np.float32), "total_tokens": 5, "status": "success"}, # doc1
            {"status": "success", "total_tokens": 0}, # doc2 - malformed (missing 'embedding')
            "This is not a dictionary" # doc3 - malformed (not a dict)
        ]

        input_metadata = {
            "doc1": {"text": "Valid text for doc1"},
            "doc2": {"text": "Text for doc2 (malformed: missing embedding key)"},
            "doc3": {"text": "Text for doc3 (malformed: not a dict)"}
        }
        input_data = {
            "index_name": "test_malformed_embed_index",
            "metadata_dict": input_metadata,
            "text_field_to_embed": "text",
            "dimension": 10
        }

        result = await save_metadata_to_faiss(**input_data)

        print(f"\nResult dictionary for {request.node.name}: {result}\n")
        assert result is not None
>       assert result.get("status") == "success"
E       AssertionError: assert 'error' == 'success'
E
E         - success
E         + error

test_pass_56/test_save_metadata_to_faiss_prompt20_backup.py:879: AssertionError
----------------------------- Captured stdout call -----------------------------

Result dictionary for test_save_malformed_embedding_result: {'status': 'error', 'error': "No embeddings could be successfully processed. First detailed error for doc_id 'doc1': No result from embedding function", 'message': "No embeddings could be successfully processed. First detailed error for doc_id 'doc1': No result from embedding function", 'meta': {'error_type': 'EmbeddingGenerationError', 'index_name': 'test_malformed_embed_index', 'duration_seconds': 0.001, 'embedding_generation_errors': {'doc1': 'No result from embedding function', 'doc2': 'No result from embedding function', 'doc3': 'No result from embedding function'}, 'original_docs_count': 3, 'embedded_docs_count': 0, 'failed_doc_ids': ['doc1', 'doc2', 'doc3']}}

------------------------------ Captured log call -------------------------------
ERROR    ADK.agent_data.tools.save_metadata_to_faiss_tool:save_metadata_to_faiss_tool.py:91 Failed to generate or received invalid embedding for doc_id: doc1. Response: <AsyncMock name='get_openai_embedding()' id='4380936368'>
ERROR    ADK.agent_data.tools.save_metadata_to_faiss_tool:save_metadata_to_faiss_tool.py:91 Failed to generate or received invalid embedding for doc_id: doc2. Response: <AsyncMock name='get_openai_embedding()' id='4380936368'>
ERROR    ADK.agent_data.tools.save_metadata_to_faiss_tool:save_metadata_to_faiss_tool.py:91 Failed to generate or received invalid embedding for doc_id: doc3. Response: <AsyncMock name='get_openai_embedding()' id='4380936368'>
ERROR    ADK.agent_data.tools.save_metadata_to_faiss_tool:save_metadata_to_faiss_tool.py:411 Operational error during FAISS processing for index 'test_malformed_embed_index': EmbeddingGenerationError - No embeddings could be successfully processed. First detailed error for doc_id 'doc1': No result from embedding function
Traceback (most recent call last):
  File "/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/tools/save_metadata_to_faiss_tool.py", line 264, in save_metadata_to_faiss
    raise EmbeddingGenerationError(error_msg)
ADK.agent_data.tools.save_metadata_to_faiss_tool.EmbeddingGenerationError: No embeddings could be successfully processed. First detailed error for doc_id 'doc1': No result from embedding function
_____ TestSaveMetadataToFaiss.test_save_vector_data_zero_dimension_vector ______

self = <test_save_metadata_to_faiss_prompt20_backup.TestSaveMetadataToFaiss object at 0x163b23400>
MockFaissIndexFlatL2 = <MagicMock name='IndexFlatL2' id='6040456160'>
mocker = <pytest_mock.plugin.MockerFixture object at 0x168111ea0>
request = <FixtureRequest for <Coroutine test_save_vector_data_zero_dimension_vector>>

    @patch(f"{SAVE_TOOL_MODULE_PATH}.faiss.IndexFlatL2")
    async def test_save_vector_data_zero_dimension_vector(self, MockFaissIndexFlatL2, mocker, request):
        """Test error when an item in vector_data is an empty list (zero dimension)."""
        from ADK.agent_data.tools.save_metadata_to_faiss_tool import save_metadata_to_faiss

        mocker.patch(FIRESTORE_CLIENT_PATH)
        mocker.patch(f"{SAVE_TOOL_MODULE_PATH}.OPENAI_AVAILABLE", True)
        mocker.patch(f"{SAVE_TOOL_MODULE_PATH}.openai_client", MagicMock())

        input_data = {
            "index_name": "test_vector_zero_dim",
            "metadata_dict": {"doc1": {"description": "Document 1"}},
            "vector_data": [[]], # Zero-dimension vector
            "dimension": 10 # Overall dimension expected
        }
        result = await save_metadata_to_faiss(**input_data)

        print(f"\nResult dictionary for {request.node.name}: {result}\n")
        assert result is not None
        assert result.get("status") == "error"
>       assert "Vector data contains an empty vector or vector with zero dimension for doc_id doc1." in result.get("message", "")
E       AssertionError: assert 'Vector data contains an empty vector or vector with zero dimension for doc_id doc1.' in 'Provided dimension 10 does not match vector dimension 0.'
E        +  where 'Provided dimension 10 does not match vector dimension 0.' = <built-in method get of dict object at 0x1680977c0>('message', '')
E        +    where <built-in method get of dict object at 0x1680977c0> = {'error': 'Provided dimension 10 does not match vector dimension 0.', 'message': 'Provided dimension 10 does not match vector dimension 0.', 'meta': {'duration_seconds': 0.0002, 'embedded_docs_count': 0, 'error_type': 'InvalidVectorDataError', 'failed_doc_ids': ['doc1'], ...}, 'status': 'error'}.get

test_pass_56/test_save_metadata_to_faiss_prompt20_backup.py:976: AssertionError
----------------------------- Captured stdout call -----------------------------

Result dictionary for test_save_vector_data_zero_dimension_vector: {'status': 'error', 'error': 'Provided dimension 10 does not match vector dimension 0.', 'message': 'Provided dimension 10 does not match vector dimension 0.', 'meta': {'error_type': 'InvalidVectorDataError', 'index_name': 'test_vector_zero_dim', 'duration_seconds': 0.0002, 'original_docs_count': 1, 'embedded_docs_count': 0, 'failed_doc_ids': ['doc1']}}

------------------------------ Captured log call -------------------------------
ERROR    ADK.agent_data.tools.save_metadata_to_faiss_tool:save_metadata_to_faiss_tool.py:411 Operational error during FAISS processing for index 'test_vector_zero_dim': InvalidVectorDataError - Provided dimension 10 does not match vector dimension 0.
Traceback (most recent call last):
  File "/Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/tools/save_metadata_to_faiss_tool.py", line 204, in save_metadata_to_faiss
    raise InvalidVectorDataError(f"Provided dimension {dimension} does not match vector dimension {current_dim}.")
ADK.agent_data.tools.save_metadata_to_faiss_tool.InvalidVectorDataError: Provided dimension 10 does not match vector dimension 0.
=============================== warnings summary ===============================
<frozen importlib._bootstrap>:241
  <frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute

<frozen importlib._bootstrap>:241
  <frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute

<frozen importlib._bootstrap>:241
  <frozen importlib._bootstrap>:241: DeprecationWarning: builtin type swigvarlink has no __module__ attribute

ADK/agent_data/tests/tools/test_save_metadata_to_faiss.py::TestSaveMetadataToFaiss::test_save_openai_embedding_error
test_pass_56/test_save_metadata_to_faiss_prompt20_backup.py::TestSaveMetadataToFaiss::test_save_openai_embedding_error
test_pass_56/test_save_metadata_to_faiss_prompt20_backup.py::TestSaveMetadataToFaiss::test_save_malformed_embedding_result
  /Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/tools/save_metadata_to_faiss_tool.py:85: RuntimeWarning: coroutine 'AsyncMockMixin._execute_mock_call' was never awaited
    embedding_value = embedding_response.get("embedding") # Extract embedding value
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

ADK/agent_data/tests/tools/test_save_metadata_to_faiss.py::TestSaveMetadataToFaiss::test_save_openai_embedding_error
ADK/agent_data/tests/tools/test_save_metadata_to_faiss.py::TestSaveMetadataToFaiss::test_save_gcs_upload_meta_fails
ADK/agent_data/tests/tools/test_save_metadata_to_faiss.py::TestSaveMetadataToFaiss::test_save_temp_file_deletion_fails
test_pass_56/test_save_metadata_to_faiss_prompt20_backup.py::TestSaveMetadataToFaiss::test_save_openai_embedding_error
test_pass_56/test_save_metadata_to_faiss_prompt20_backup.py::TestSaveMetadataToFaiss::test_save_malformed_embedding_result
  /Users/nmhuyen/Documents/Manual Deploy/mpc_back_end_for_agents/ADK/agent_data/tools/save_metadata_to_faiss_tool.py:236: RuntimeWarning: coroutine 'AsyncMockMixin._execute_mock_call' was never awaited
    embedding_results_map = await _generate_embeddings_batch(texts_to_embed_with_ids, None) # agent_context=None for now
  Enable tracemalloc to get traceback where the object was allocated.
  See https://docs.pytest.org/en/stable/how-to/capture-warnings.html#resource-warnings for more info.

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
FAILED ADK/agent_data/test_pass_53/test_mcp_agent_all_tools.py::TestMCPStdioAllTools::test_core_tools_and_errors
FAILED ADK/agent_data/test_pass_53/test_mcp_agent_batch_large.py::TestMCPAgentLargeBatchDirect::test_large_batch_processing_direct
FAILED ADK/agent_data/test_pass_53/test_mcp_agent_batch_tools.py::TestMCPBatchToolsStdio::test_batch_mixed_success_failure
FAILED ADK/agent_data/test_pass_53/test_mcp_agent_batch_tools.py::TestMCPBatchToolsStdio::test_batch_successful
FAILED ADK/agent_data/test_pass_53/test_mcp_agent_batch_tools.py::TestMCPBatchToolsStdio::test_query_metadata
FAILED ADK/agent_data/test_pass_53/test_mcp_agent_batch_tools.py::TestMCPBatchToolsStdio::test_save_document
FAILED ADK/agent_data/test_pass_53/test_mcp_agent_batch_tools.py::TestMCPBatchToolsStdio::test_semantic_search_local
FAILED ADK/agent_data/test_pass_53/test_mcp_agent_batch_tools.py::TestMCPBatchToolsStdio::test_update_metadata
FAILED ADK/agent_data/test_pass_53/test_mcp_agent_batch_tools.py::TestMCPBatchToolsStdio::test_update_metadata_fail
FAILED ADK/agent_data/test_pass_53/test_mcp_agent_batch_tools.py::TestMCPBatchToolsStdio::test_vectorize_document
FAILED ADK/agent_data/test_pass_53/test_mcp_registered_tools.py::TestMCPStdioGetRegisteredTools::test_get_registered_tools_mcp_stdio
FAILED ADK/agent_data/test_pass_53/test_mcp_timeout.py::test_mcp_timeout_handling
FAILED ADK/agent_data/test_pass_53/test_mcp_timeout.py::test_log_output_verification
FAILED ADK/agent_data/test_pass_53/test_query_metadata_from_faiss.py::TestQueryMetadataFaiss::test_query_firestore_missing_gcs_faiss_path
FAILED ADK/agent_data/tests/test_mcp_agent_all_tools.py::TestMCPStdioAllTools::test_core_tools_and_errors
FAILED ADK/agent_data/tests/test_mcp_agent_batch_large.py::TestMCPAgentLargeBatchDirect::test_large_batch_processing_direct
FAILED ADK/agent_data/tests/test_mcp_agent_batch_tools.py::TestMCPBatchToolsStdio::test_batch_mixed_success_failure
FAILED ADK/agent_data/tests/test_mcp_agent_batch_tools.py::TestMCPBatchToolsStdio::test_batch_successful
FAILED ADK/agent_data/tests/test_mcp_agent_batch_tools.py::TestMCPBatchToolsStdio::test_query_metadata
FAILED ADK/agent_data/tests/test_mcp_agent_batch_tools.py::TestMCPBatchToolsStdio::test_save_document
FAILED ADK/agent_data/tests/test_mcp_agent_batch_tools.py::TestMCPBatchToolsStdio::test_semantic_search_local
FAILED ADK/agent_data/tests/test_mcp_agent_batch_tools.py::TestMCPBatchToolsStdio::test_update_metadata
FAILED ADK/agent_data/tests/test_mcp_agent_batch_tools.py::TestMCPBatchToolsStdio::test_update_metadata_fail
FAILED ADK/agent_data/tests/test_mcp_agent_batch_tools.py::TestMCPBatchToolsStdio::test_vectorize_document
FAILED ADK/agent_data/tests/test_mcp_registered_tools.py::TestMCPStdioGetRegisteredTools::test_get_registered_tools_mcp_stdio
FAILED ADK/agent_data/tests/test_mcp_timeout.py::test_mcp_timeout_handling - ...
FAILED ADK/agent_data/tests/test_mcp_timeout.py::test_log_output_verification
FAILED ADK/agent_data/tests/tools/test_save_metadata_to_faiss.py::TestSaveMetadataToFaiss::test_save_openai_embedding_error
FAILED ADK/agent_data/tests/tools/test_save_metadata_to_faiss.py::TestSaveMetadataToFaiss::test_save_gcs_upload_faiss_fails
FAILED ADK/agent_data/tests/tools/test_save_metadata_to_faiss.py::TestSaveMetadataToFaiss::test_save_gcs_upload_meta_fails
FAILED ADK/agent_data/tests/tools/test_save_metadata_to_faiss.py::TestSaveMetadataToFaiss::test_save_firestore_update_fails
FAILED ADK/agent_data/tests/tools/test_save_metadata_to_faiss.py::TestSaveMetadataToFaiss::test_save_temp_file_deletion_fails
FAILED test_pass_56/test_save_metadata_to_faiss_prompt20_backup.py::TestSaveMetadataToFaiss::test_save_openai_embedding_error
FAILED test_pass_56/test_save_metadata_to_faiss_prompt20_backup.py::TestSaveMetadataToFaiss::test_save_malformed_embedding_result
FAILED test_pass_56/test_save_metadata_to_faiss_prompt20_backup.py::TestSaveMetadataToFaiss::test_save_vector_data_zero_dimension_vector
ERROR ADK/agent_data/test_pass_53/test_query_metadata_from_faiss.py::TestQueryMetadataFaiss::test_query_embedding_error
ERROR ADK/agent_data/tests/tools/test_save_metadata_to_faiss.py::TestSaveMetadataToFaiss::test_save_malformed_embedding_result
ERROR ADK/agent_data/tests/tools/test_save_metadata_to_faiss.py::TestSaveMetadataToFaiss::test_save_firestore_client_init_fails
ERROR test_pass_56/test_save_metadata_to_faiss_prompt20_backup.py::TestSaveMetadataToFaiss::test_save_firestore_update_fails
ERROR test_pass_56/test_save_metadata_to_faiss_prompt20_backup.py::TestSaveMetadataToFaiss::test_save_empty_metadata_dict
ERROR test_pass_56/test_save_metadata_to_faiss_prompt20_backup.py::TestSaveMetadataToFaiss::test_save_text_field_missing_in_metadata
ERROR test_pass_56/test_save_metadata_to_faiss_prompt20_backup.py::TestSaveMetadataToFaiss::test_save_firestore_client_init_fails
ERROR test_pass_56/test_save_metadata_to_faiss_prompt20_backup.py::TestSaveMetadataToFaiss::test_save_temp_file_deletion_fails
======= 35 failed, 96 passed, 1 skipped, 11 warnings, 8 errors in 24.58s =======
